{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0\n",
        "!pip install fastai==2.6.3 --no-deps\n",
        "!pip install self_supervised"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T10:09:30.682145Z",
          "iopub.execute_input": "2022-09-19T10:09:30.682992Z",
          "iopub.status.idle": "2022-09-19T10:09:56.259961Z",
          "shell.execute_reply.started": "2022-09-19T10:09:30.682879Z",
          "shell.execute_reply": "2022-09-19T10:09:56.258704Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4uQHG8cag8n",
        "outputId": "1c6d1b2c-ae9a-4700-c54d-59fe7869969c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 10.0 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.12.0\n",
            "  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 72.4 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.11.0\n",
            "  Downloading torchaudio-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 78.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (2022.6.15)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.12.1+cu113\n",
            "    Uninstalling torchaudio-0.12.1+cu113:\n",
            "      Successfully uninstalled torchaudio-0.12.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.11.0 torchaudio-0.11.0 torchvision-0.12.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastai==2.6.3\n",
            "  Downloading fastai-2.6.3-py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 8.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: fastai\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 2.7.9\n",
            "    Uninstalling fastai-2.7.9:\n",
            "      Successfully uninstalled fastai-2.7.9\n",
            "Successfully installed fastai-2.6.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting self_supervised\n",
            "  Downloading self_supervised-1.0.4-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 606 kB/s \n",
            "\u001b[?25hCollecting timm>=0.4.5\n",
            "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[K     |████████████████████████████████| 509 kB 13.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fastai>=2.2.7 in /usr/local/lib/python3.7/dist-packages (from self_supervised) (2.6.3)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from self_supervised) (21.1.3)\n",
            "Collecting kornia>=0.5.0\n",
            "  Downloading kornia-0.6.7-py2.py3-none-any.whl (565 kB)\n",
            "\u001b[K     |████████████████████████████████| 565 kB 65.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from self_supervised) (21.3)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (7.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.3.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (6.0)\n",
            "Collecting fastcore<1.5,>=1.3.27\n",
            "  Downloading fastcore-1.4.5-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (0.12.0)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.0.3)\n",
            "Requirement already satisfied: torch<1.12,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (2.23.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.7.3)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (0.0.7)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (3.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.6.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.7)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.10.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.21.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.10)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (8.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.11.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.4.4)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.1.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.8)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (57.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.9.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4->fastai>=2.2.7->self_supervised) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->self_supervised) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4->fastai>=2.2.7->self_supervised) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai>=2.2.7->self_supervised) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4->fastai>=2.2.7->self_supervised) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4->fastai>=2.2.7->self_supervised) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai>=2.2.7->self_supervised) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai>=2.2.7->self_supervised) (2022.2.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (3.1.0)\n",
            "Installing collected packages: fastcore, timm, kornia, self-supervised\n",
            "  Attempting uninstall: fastcore\n",
            "    Found existing installation: fastcore 1.5.25\n",
            "    Uninstalling fastcore-1.5.25:\n",
            "      Successfully uninstalled fastcore-1.5.25\n",
            "Successfully installed fastcore-1.4.5 kornia-0.6.7 self-supervised-1.0.4 timm-0.6.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T10:09:56.265726Z",
          "iopub.execute_input": "2022-09-19T10:09:56.268002Z",
          "iopub.status.idle": "2022-09-19T10:09:56.283515Z",
          "shell.execute_reply.started": "2022-09-19T10:09:56.267961Z",
          "shell.execute_reply": "2022-09-19T10:09:56.282655Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BOv4kkJDag8r",
        "outputId": "f26b20c9-06d5-44f4-d24d-d285c2bf42d1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fastai\n",
        "import self_supervised\n",
        "import torch\n",
        "assert(fastai.__version__ == '2.6.3') #Check that version is 2.6.3"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T10:09:56.287267Z",
          "iopub.execute_input": "2022-09-19T10:09:56.289770Z",
          "iopub.status.idle": "2022-09-19T10:09:58.060276Z",
          "shell.execute_reply.started": "2022-09-19T10:09:56.289734Z",
          "shell.execute_reply": "2022-09-19T10:09:58.059312Z"
        },
        "trusted": true,
        "id": "Pk01WY_Dag8s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "self_supervised.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ogKtiCl4_K4E",
        "outputId": "1d21c9db-15a2-4bfa-9611-fe1146e10bc9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.vision.all import *\n",
        "from self_supervised.augmentations import *\n",
        "from self_supervised.layers import *\n",
        "import inspect\n",
        "import warnings\n",
        "import random\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#from Base_Stein.SVGD_classes import *"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T10:09:58.062775Z",
          "iopub.execute_input": "2022-09-19T10:09:58.063396Z",
          "iopub.status.idle": "2022-09-19T10:10:00.022015Z",
          "shell.execute_reply.started": "2022-09-19T10:09:58.063358Z",
          "shell.execute_reply": "2022-09-19T10:10:00.020853Z"
        },
        "trusted": true,
        "id": "AOjr_YCLag8t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Like most other SSL algorithms BT's model consists of an encoder and projector (MLP) layer.\n",
        "#Definition is straightforward:\n",
        "#https://colab.research.google.com/github/KeremTurgutlu/self_supervised/blob/master/nbs/14%20-%20barlow_twins.ipynb#scrollTo=1M6QcUChcvpz\n",
        "class BarlowTwinsModel(Module):\n",
        "    \"\"\"An encoder followed by a projector\n",
        "    \"\"\"\n",
        "    def __init__(self,encoder,projector):self.encoder,self.projector = encoder,projector\n",
        "        \n",
        "    def forward(self,x): return self.projector(self.encoder(x))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T10:10:00.023769Z",
          "iopub.execute_input": "2022-09-19T10:10:00.024956Z",
          "iopub.status.idle": "2022-09-19T10:10:00.030911Z",
          "shell.execute_reply.started": "2022-09-19T10:10:00.024915Z",
          "shell.execute_reply": "2022-09-19T10:10:00.029896Z"
        },
        "trusted": true,
        "id": "XTSdKC6bag8t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HOWEVER instead of directly using the above, by passing both an encoder and a projector, create_barlow_twins_model\n",
        "#function can be used by minimally passing a predefined encoder and the expected input channels.\n",
        "\n",
        "#In the paper it's mentioned that MLP layer consists of 3 layers... following function will create a 3 layer\n",
        "#MLP projector with batchnorm and ReLU by default. Alternatively, you can change bn and nlayers. \n",
        "\n",
        "#Questions: Why torch.no_grad() when doing this?\n",
        "def create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n",
        "    \"Create Barlow Twins model\"\n",
        "    n_in  = in_channels(encoder)\n",
        "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
        "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n",
        "    apply_init(projector)\n",
        "    return BarlowTwinsModel(encoder, projector)\n",
        "\n",
        "#Similar to above. Simple API to make the BT model:"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T10:10:00.033384Z",
          "iopub.execute_input": "2022-09-19T10:10:00.034084Z",
          "iopub.status.idle": "2022-09-19T10:10:00.042621Z",
          "shell.execute_reply.started": "2022-09-19T10:10:00.034047Z",
          "shell.execute_reply": "2022-09-19T10:10:00.041644Z"
        },
        "trusted": true,
        "id": "ZL3EE07Pag8u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BarlowTwins Callback\n",
        "#The following parameters can be passed:\n",
        "# - aug_pipelines\n",
        "# Imb lambda is the weight for redundancy reduction term in the loss function\n",
        "\n",
        "@delegates(get_multi_aug_pipelines)\n",
        "def get_barlow_twins_aug_pipelines(size,**kwargs): return get_multi_aug_pipelines(n=2,size=size,**kwargs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T10:10:00.045846Z",
          "iopub.execute_input": "2022-09-19T10:10:00.046104Z",
          "iopub.status.idle": "2022-09-19T10:10:00.054986Z",
          "shell.execute_reply.started": "2022-09-19T10:10:00.046079Z",
          "shell.execute_reply": "2022-09-19T10:10:00.053993Z"
        },
        "trusted": true,
        "id": "DFjGL-COag8v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Uniform random number between a and b\n",
        "def Unif(a,b):\n",
        "    return (b-a)*torch.rand(1).item()+a"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T10:10:00.056334Z",
          "iopub.execute_input": "2022-09-19T10:10:00.056788Z",
          "iopub.status.idle": "2022-09-19T10:10:00.063326Z",
          "shell.execute_reply.started": "2022-09-19T10:10:00.056752Z",
          "shell.execute_reply": "2022-09-19T10:10:00.062415Z"
        },
        "trusted": true,
        "id": "xx4KsywAag8v"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_polynomial(A):\n",
        "    \n",
        "    #B=torch.normal(mean=0, std=0.025, size=(1, 1)).item() #First Horner term (and is coefficient of x^4)\n",
        "    \n",
        "    #Btem = torch.normal(mean=0,std=0.05, size=(1, 1)).item() #Sample coefficient of x^3\n",
        "    #B = Btem + B*A #Third Horner term\n",
        "    \n",
        "    B = torch.normal(mean=0,std=0.5, size=(1, 1)).item() #Sample coefficient of x^2\n",
        "    \n",
        "#     Btem = torch.normal(mean=0,std=0.5, size=(1, 1)).item() #Sample coefficient of x^2\n",
        "#     B = Btem + B*A #Third Horner term\n",
        "    \n",
        "    Btem = random.choice([-1,1])*torch.normal(mean=0,std=1, size=(1, 1)).item() #Sample coefficient of x\n",
        "    #Btem=1\n",
        "    B = Btem + B*A #Fourth Horner term\n",
        "    \n",
        "    Btem = torch.normal(mean=0,std=1, size=(1, 1)).item() #Sample coefficient of x^0\n",
        "    B = Btem + B*A #Fifth Horner term\n",
        "    \n",
        "    \n",
        "    return B"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T06:20:31.062887Z",
          "iopub.execute_input": "2022-09-19T06:20:31.063346Z",
          "iopub.status.idle": "2022-09-19T06:20:31.076330Z",
          "shell.execute_reply.started": "2022-09-19T06:20:31.063306Z",
          "shell.execute_reply": "2022-09-19T06:20:31.075215Z"
        },
        "trusted": true,
        "id": "3v0NL3f1ag8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_polynomial_bestsofar(A):\n",
        "    \n",
        "    \n",
        "    B=torch.normal(mean=0, std=0.025, size=(1, 1)).item() #First Horner term (and is coefficient of x^4)\n",
        "    \n",
        "    Btem = torch.normal(mean=0,std=0.05, size=(1, 1)).item() #Sample coefficient of x^3\n",
        "    B = Btem + B*A #Third Horner term\n",
        "    \n",
        "    Btem = torch.normal(mean=0,std=0.5, size=(1, 1)).item() #Sample coefficient of x^2\n",
        "    B = Btem + B*A #Third Horner term\n",
        "    \n",
        "    Btem = random.choice([-1,1])*torch.normal(mean=1,std=2, size=(1, 1)).item() #Sample coefficient of x\n",
        "    B = Btem + B*A #Fourth Horner term\n",
        "    \n",
        "    Btem = torch.normal(mean=0,std=1, size=(1, 1)).item() #Sample coefficient of x^0\n",
        "    B = Btem + B*A #Fifth Horner term\n",
        "    \n",
        "    \n",
        "    return B"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T06:20:34.011877Z",
          "iopub.execute_input": "2022-09-19T06:20:34.012326Z",
          "iopub.status.idle": "2022-09-19T06:20:34.023926Z",
          "shell.execute_reply.started": "2022-09-19T06:20:34.012284Z",
          "shell.execute_reply": "2022-09-19T06:20:34.022841Z"
        },
        "trusted": true,
        "id": "VdWjbdMdag8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def low_deg(A):\n",
        "    power=Unif(1,1.25)\n",
        "    coeff2 = torch.normal(mean=0, std=1, size=(1, 1)).item() #degree 2 term\n",
        "    coeff1 = torch.normal(mean=1, std=1, size=(1, 1)).item() #degree 2 term\n",
        "    \n",
        "#     coeff2 = torch.normal(mean=0, std=1, size=(1, 1)).item() #degree 2 term\n",
        "#     coeff1 = torch.normal(mean=1, std=0.7, size=(1, 1)).item() #degree 2 term\n",
        "\n",
        "#     coeff2 = torch.normal(mean=0, std=1, size=(1, 1)).item() #degree 2 term\n",
        "#     coeff1 = torch.normal(mean=0, std=1, size=(1, 1)).item() #degree 2 term\n",
        "\n",
        "    \n",
        "    B = (coeff1*A + coeff2*torch.abs(A).pow(power))\n",
        "    \n",
        "    #B = (1/power)*torch.abs(A).pow(power)\n",
        "    return B"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T06:20:36.918012Z",
          "iopub.execute_input": "2022-09-19T06:20:36.918461Z",
          "iopub.status.idle": "2022-09-19T06:20:36.931102Z",
          "shell.execute_reply.started": "2022-09-19T06:20:36.918422Z",
          "shell.execute_reply": "2022-09-19T06:20:36.930199Z"
        },
        "trusted": true,
        "id": "Z_nknWDoag8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_quintic(A):\n",
        "\n",
        "    \n",
        "    B=torch.normal(mean=0, std=0.125, size=(1, 1)).item() #First Horner term (and is coefficient of x^4)\n",
        "    \n",
        "    \n",
        "    Btem=torch.normal(mean=0, std=0.25, size=(1, 1)).item()#Sample coefficient of x^3\n",
        "    B = Btem + B*A #Second Horner term\n",
        "    \n",
        "    Btem = torch.normal(mean=0,std=0.5, size=(1, 1)).item() #Sample coefficient of x^2\n",
        "    B = Btem + B*A #Third Horner term\n",
        "    \n",
        "    Btem = random.choice([-1,1])*torch.normal(mean=1,std=2, size=(1, 1)).item() #Sample coefficient of x\n",
        "    B = Btem + B*A #Fourth Horner term\n",
        "    \n",
        "    Btem = torch.normal(mean=0,std=1, size=(1, 1)).item() #Sample coefficient of x^0\n",
        "    B = Btem + B*A #Fifth Horner term\n",
        "    \n",
        "    \n",
        "    return B"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T06:20:39.813382Z",
          "iopub.execute_input": "2022-09-19T06:20:39.814036Z",
          "iopub.status.idle": "2022-09-19T06:20:39.824153Z",
          "shell.execute_reply.started": "2022-09-19T06:20:39.813992Z",
          "shell.execute_reply": "2022-09-19T06:20:39.823117Z"
        },
        "trusted": true,
        "id": "DtX22j1Vag8y"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def random_sinusoid(x,std=0.5):\n",
        "    \n",
        "    t=torch.normal(mean=1,std=std,size=(1,1)).item()\n",
        "    s=torch.normal(mean=1,std=std,size=(1,1)).item()\n",
        "    \n",
        "    u=torch.normal(mean=0,std=std,size=(1,1)).item()\n",
        "    v=torch.normal(mean=0,std=std,size=(1,1)).item()\n",
        "    \n",
        "    a=torch.normal(mean=0,std=2,size=(1,1)).item()\n",
        "    b=torch.normal(mean=0,std=2,size=(1,1)).item()\n",
        "    \n",
        "    return a*torch.sin(t*x+s) + b*torch.cos(u*x + v)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T06:20:42.732783Z",
          "iopub.execute_input": "2022-09-19T06:20:42.733240Z",
          "iopub.status.idle": "2022-09-19T06:20:42.748687Z",
          "shell.execute_reply.started": "2022-09-19T06:20:42.733199Z",
          "shell.execute_reply": "2022-09-19T06:20:42.747440Z"
        },
        "trusted": true,
        "id": "l6gBw66Iag8y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def poly_sinusoid(x):\n",
        "    \n",
        "    return (x) + 0.2*random_sinusoid(x,std=(0.5))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T06:20:45.433727Z",
          "iopub.execute_input": "2022-09-19T06:20:45.434179Z",
          "iopub.status.idle": "2022-09-19T06:20:45.442958Z",
          "shell.execute_reply.started": "2022-09-19T06:20:45.434132Z",
          "shell.execute_reply": "2022-09-19T06:20:45.441837Z"
        },
        "trusted": true,
        "id": "UnodDXFoag8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Exp_sample(scale=3.,loc=1.8):\n",
        "    \n",
        "    Expo = torch.distributions.exponential.Exponential\n",
        "    E = Expo(torch.tensor([scale]))\n",
        "#     if random.random()<0.5:\n",
        "#         return 1\n",
        "#     else:\n",
        "    return loc+E.sample().item()"
      ],
      "metadata": {
        "id": "li3BQ1ukX0XB"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#export\n",
        "class BarlowTwins(Callback):\n",
        "    order,run_valid = 9,True\n",
        "    def __init__(self, aug_pipelines, lmb=5e-3, print_augs=False):\n",
        "        assert_aug_pipelines(aug_pipelines)\n",
        "        self.aug1, self.aug2 = aug_pipelines\n",
        "        if print_augs: print(self.aug1), print(self.aug2)\n",
        "        store_attr('lmb')\n",
        "        \n",
        "    def before_fit(self): \n",
        "        self.learn.loss_func = self.lf\n",
        "        nf = self.learn.model.projector[-1].out_features\n",
        "        self.I = torch.eye(nf).to(self.dls.device)\n",
        "            \n",
        "    def before_batch(self):\n",
        "        xi,xj = self.aug1(self.x), self.aug2(self.x)\n",
        "        self.learn.xb = (torch.cat([xi, xj]),)\n",
        "        \n",
        "        #Uncomment to run standard BT\n",
        "    def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
        "        bs,nf = pred.size(0)//2,pred.size(1)\n",
        "\n",
        "        z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
        "\n",
        "        z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
        "        z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
        "        \n",
        "        C = (z1norm.T @ z2norm) / bs \n",
        "        cdiff = (C - self.I)**2\n",
        "        loss = (cdiff*self.I + cdiff*(1-self.I)*self.lmb).sum() \n",
        "        return loss\n",
        "\n",
        "\n",
        "    # def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
        "    #     bs,nf = pred.size(0)//2,pred.size(1)\n",
        "\n",
        "    #     #All standard, from BT\n",
        "    #     z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
        "    #     z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
        "    #     z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
        "        \n",
        "        \n",
        "    #     C = (z1norm.T @ z2norm) / bs \n",
        "    #     cdiff = (C - self.I)**2\n",
        "\n",
        "    #     #{'Pr': 0.45546983480453496, 'dist': 'Exp', 'loc': 0.49445648193359376, 'scale': 2.5828861594200134, 'percent_correct': 0.8277832269668579} #Best so far\n",
        "\n",
        "    \n",
        "    #     #polyprob=0.1\n",
        "    #     #polyprob=0.45546\n",
        "    #     #polyprob=1.0\n",
        "    #     polyprob=1.0\n",
        "    #     eps=1e-8\n",
        "    #     temrand = random.random()\n",
        "    #     if temrand < polyprob: #With some probability we want off diag terms to be (quadratic) say.\n",
        "\n",
        "    #         #p=Exp_sample(loc=0.494456,scale=2.582)\n",
        "            \n",
        "    #         # p=Unif(1,2.5)\n",
        "\n",
        "    #         # p=Unif(1.8,2.5)\n",
        "\n",
        "    #         # if p<1:\n",
        "    #         #     z1norm=z1norm+eps #jitter for numerical reasons\n",
        "    #         # z1norm_2 = (1/p)*torch.abs(z1norm).pow(p)\n",
        "\n",
        "    #         z1norm_2 = random_quintic(z1norm)\n",
        "    #         z2norm_2 = z2norm\n",
        "                \n",
        "    #         C_2 = (z1norm_2.T @ z2norm_2) / bs\n",
        "            \n",
        "    #         cdiff_2 = (C_2)**2 #don't need to subtract I as only looking at off diag terms\n",
        "            \n",
        "    #     else:\n",
        "    #         cdiff_2 = cdiff\n",
        "            \n",
        "    #     l2 = cdiff_2*(1-self.I)*self.lmb #Is either the standard term - or not.\n",
        "\n",
        "    #     loss = (cdiff*self.I + l2).sum() \n",
        "    #     return loss\n",
        "\n",
        "\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def show(self, n=1):\n",
        "        bs = self.learn.x.size(0)//2\n",
        "        x1,x2  = self.learn.x[:bs], self.learn.x[bs:] \n",
        "        idxs = np.random.choice(range(bs),n,False)\n",
        "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n",
        "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n",
        "        images = []\n",
        "        for i in range(n): images += [x1[i],x2[i]] \n",
        "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T13:36:19.050256Z",
          "iopub.execute_input": "2022-09-19T13:36:19.050642Z",
          "iopub.status.idle": "2022-09-19T13:36:19.086273Z",
          "shell.execute_reply.started": "2022-09-19T13:36:19.050609Z",
          "shell.execute_reply": "2022-09-19T13:36:19.085253Z"
        },
        "trusted": true,
        "id": "a2Exs2s3ag8z"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Debugging cell - delete later (similar to cell below)\n",
        "ps=500\n",
        "hs=500\n",
        "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
        "model = create_barlow_twins_model(fastai_encoder, hidden_size=hs,projection_size=ps)# projection_size=1024)\n",
        "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
        "#values for these which is tantamount to doing nothing\n",
        "#So if we choose resize_scale=(1,1) then the images look the same.\n",
        "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
        "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=True)\n",
        "#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "learn.fit(300) #300                                        "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T13:36:26.314293Z",
          "iopub.execute_input": "2022-09-19T13:36:26.316743Z",
          "iopub.status.idle": "2022-09-19T14:02:55.046511Z",
          "shell.execute_reply.started": "2022-09-19T13:36:26.316709Z",
          "shell.execute_reply": "2022-09-19T14:02:55.045453Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vbS1WtLiag80",
        "outputId": "d7a7c34a-e3d3-4cc7-ff4f-4b9032e8971a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='120' class='' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      40.00% [120/300 18:04&lt;27:06]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>134.577148</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>94.195786</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>71.588013</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>56.849567</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>46.172470</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>39.083176</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>36.435322</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>31.723850</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>27.590014</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>24.694378</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>23.167044</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>23.119009</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>21.563004</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>21.350143</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>20.003679</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>18.537327</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>19.175867</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>19.323706</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>18.186831</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>16.283428</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>14.802546</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>14.195197</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>13.618475</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>12.992090</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>12.484274</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>13.338120</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>13.865638</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>13.266565</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>12.490574</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>11.981826</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>11.691928</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>11.311548</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>11.670488</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>11.005073</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>10.582679</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>10.860136</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>12.751156</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>12.224681</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>11.217517</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>10.450280</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>9.632960</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>8.921717</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>8.967664</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>8.951754</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>9.796128</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>9.651993</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>9.974607</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>10.234957</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>9.542793</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>9.296852</td>\n",
              "      <td>None</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>9.406761</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>9.372168</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>9.974705</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>9.033435</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>8.318203</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>8.262234</td>\n",
              "      <td>None</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>7.921675</td>\n",
              "      <td>None</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>7.573665</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>7.657456</td>\n",
              "      <td>None</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>7.716532</td>\n",
              "      <td>None</td>\n",
              "      <td>00:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>7.535991</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>9.000353</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>9.335954</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>9.076466</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>8.306412</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>8.015912</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>7.492574</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>7.170702</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>7.198977</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>7.284149</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>7.346414</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>7.938229</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>8.582387</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>8.582989</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>7.866939</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>7.210721</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>6.670464</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>6.594156</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>6.496301</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>6.442724</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>6.170250</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>6.268777</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>6.799898</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>6.744437</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>6.431894</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>6.525100</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>6.757092</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>7.127003</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>7.259691</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>6.798035</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>6.212998</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>6.322475</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>5.981707</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>6.134582</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>7.424767</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>7.509560</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>6.910154</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>6.257625</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>5.906508</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>5.723353</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.885214</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>5.858675</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>5.785604</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>5.880152</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>5.859746</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>5.987700</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>5.781299</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>5.621196</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>5.526329</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>5.759119</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>5.912308</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>5.724793</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>5.348570</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>5.117934</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>5.311962</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>5.403394</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>5.230000</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>5.053563</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>4.890280</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>5.044390</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "      <progress value='10' class='' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      31.25% [10/32 00:03&lt;00:07 5.4720]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=42):\n",
        "    \"\"\"\"\n",
        "    Seed everything.\n",
        "    \"\"\"   \n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def tune_set(items0,tune_s=1000):\n",
        "    \n",
        "  \n",
        "    items0=items0.shuffle()\n",
        "    d = {'0':0,'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0,'8':0,'9':0}\n",
        "    ITEMS=[]\n",
        "    for i in items0:\n",
        "        s=str(i).split('/training/')[1][0]\n",
        "        if d[s] is 0 or d[s] is 1:\n",
        "            ITEMS.append(i)\n",
        "            d[s]+=1\n",
        "    #items0=ITEMS\n",
        "\n",
        "    for i in items0:\n",
        "        if i not in ITEMS:\n",
        "            ITEMS.append(i)\n",
        "            \n",
        "    split = IndexSplitter(list(range(20)))\n",
        "\n",
        "    tds_tune = Datasets(ITEMS, [PILImageBW.create, [parent_label, Categorize()]], splits=split(ITEMS)) #Or do we want this?\n",
        "    dls_tune = tds_tune.dataloaders(bs=20, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "    \n",
        "    return dls_tune\n",
        "\n",
        " "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-15T11:19:32.561864Z",
          "iopub.execute_input": "2022-09-15T11:19:32.562468Z",
          "iopub.status.idle": "2022-09-15T11:19:59.557414Z",
          "shell.execute_reply.started": "2022-09-15T11:19:32.562421Z",
          "shell.execute_reply": "2022-09-15T11:19:59.555967Z"
        },
        "trusted": true,
        "id": "Y5FN3Safag80"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the dataloader and set batch size \n",
        "ts=16384 #training set size - most everything\n",
        "bs=512\n",
        "device='cuda'\n",
        "path = untar_data(URLs.MNIST)\n",
        "\n",
        "items = get_image_files(path/'training') #i.e. NOT testing!!!\n",
        "items.sort() \n",
        "\n",
        "seed_everything(seed=42)\n",
        "# import random\n",
        "# random.seed(20)\n",
        "items=items.shuffle()\n",
        "\n",
        "items1 = items[0:ts] #train on these guys\n",
        "split = RandomSplitter(valid_pct=0.0) #randomly split training set into training and validation\n",
        "#tds = Datasets(items,splits=split(items)) #Do we want this?\n",
        "tds = Datasets(items1, [PILImageBW.create, [parent_label, Categorize()]], splits=split(items1)) #Or do we want this?\n",
        "dls = tds.dataloaders(bs=bs, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "\n",
        "tune_s=2000 #we choose 20 guys (randomly) out of 1000 to tune on\n",
        "items0 = items[ts:ts+tune_s] #for fine tuning - just choose 2000 guys to extract 20 for fine tuning \n",
        "dls_tune=tune_set(items0,tune_s=tune_s)\n",
        "\n",
        "\n",
        "#NB: Uncomment and compare in colab and kaggle\n",
        "# for x,y in dls_tune.train:\n",
        "#   print(x.mean())\n",
        "#   input()\n",
        "#   break\n",
        "\n",
        "# for x,y in dls_tune.train:\n",
        "#   print(x.mean())\n",
        "#   input()\n",
        "#   break\n",
        "\n",
        "\n",
        "#Evaluate linear classifier on this guy\n",
        "items2 = items[ts+tune_s:]\n",
        "split = RandomSplitter(valid_pct=0.0) #randomly split training set into training and validation\n",
        "tds_test = Datasets(items2, [PILImageBW.create, [parent_label, Categorize()]], splits=split(items2)) #Or do we want this?\n",
        "dls_test = tds_test.dataloaders(bs=578, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "\n",
        "print(len(dls_test.train_ds))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T13:35:19.936529Z",
          "iopub.execute_input": "2022-09-19T13:35:19.936907Z",
          "iopub.status.idle": "2022-09-19T13:35:26.313182Z",
          "shell.execute_reply.started": "2022-09-19T13:35:19.936872Z",
          "shell.execute_reply": "2022-09-19T13:35:26.312223Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv4RE3O0ag81",
        "outputId": "50155b7a-8ecd-4b0d-dfd7-624672176f77"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "41616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A \"reasonable\" composite augmentation: initially copy pasted BT. We run this cell a few times to check it makes sense\n",
        "#Also define encoder and model\n",
        "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
        "model = create_barlow_twins_model(fastai_encoder, hidden_size=10,projection_size=10)# projection_size=1024)\n",
        "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
        "#values for these which is tantamount to doing nothing\n",
        "#So if we choose resize_scale=(1,1) then the images look the same.\n",
        "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
        "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=False)\n",
        "#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "\n",
        "#dls.valid.bs = len(dls.valid_ds) #Set the validation dataloader batch size to be the length of the validation dataset\n",
        "\n",
        "b = dls.one_batch()\n",
        "learn._split(b)\n",
        "learn('before_batch')\n",
        "axes = learn.barlow_twins.show(n=2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T10:12:13.514342Z",
          "iopub.execute_input": "2022-09-19T10:12:13.515047Z",
          "iopub.status.idle": "2022-09-19T10:12:22.096586Z",
          "shell.execute_reply.started": "2022-09-19T10:12:13.515008Z",
          "shell.execute_reply": "2022-09-19T10:12:22.095608Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "AKbw2pxMag82",
        "outputId": "cdbe46f9-7745-4a12-ff63-0cd4fedf8ff5"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple linear classifier\n",
        "class LinearClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self,zdim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(zdim,10) #As 10 classes for mnist\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = cast(self.fc1(x),Tensor) #so we have to use cross entropy loss. cast is because using old version fastai \n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T10:51:53.056515Z",
          "iopub.execute_input": "2022-09-19T10:51:53.058819Z",
          "iopub.status.idle": "2022-09-19T10:51:53.067915Z",
          "shell.execute_reply.started": "2022-09-19T10:51:53.058774Z",
          "shell.execute_reply": "2022-09-19T10:51:53.067001Z"
        },
        "trusted": true,
        "id": "Kujpa-Lvag82"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NB: Will give same random 20-tune set (for fixed random seed), only if the cell\n",
        "#\"#Get the dataloader and set batch size\" is the same. Perhaps later we can make this cell a function of that one. \n",
        "#Functions to train and evaluate head\n",
        "fastai_encoder.eval()\n",
        "\n",
        "def train_head(seed=10): #The seed choses a different (20) samples for training the head. 2 of each class\n",
        "\n",
        "    seed_everything(seed=seed)\n",
        "    dls_tune=tune_set(items0,tune_s=tune_s) #different random tune set each time (but random seed same for consistency)\n",
        "  \n",
        "    zdim=1024 #see above\n",
        "    head = LinearClassifier(zdim=zdim)\n",
        "    device='cuda'\n",
        "    head.to(device)\n",
        "    optimizer = torch.optim.Adam(head.parameters())\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    #EPOCHS=100\n",
        "\n",
        "    for epoch in range(200):\n",
        "        for x,y in dls_tune.valid:\n",
        "            #break \n",
        "            #b = dls.train.one_batch() #Seems need dls[0] or dls.train for training ... dls[1] is validation see here https://docs.fast.ai/data.core.html#DataLoaders.__getitem__\n",
        "            #x,y = b[0],b[1]\n",
        "\n",
        "            loss = criterion(head(fastai_encoder(x)),y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            #print(loss)\n",
        "\n",
        "    return head\n",
        "\n",
        "\n",
        "def eval_head(head):\n",
        "\n",
        "    N=len(dls_test.train)*bs #close to len(dls_test.train_ds) but not quite...\n",
        "    num_correct=0\n",
        "    for x,y in dls_test.train:\n",
        "\n",
        "        ypred = head(fastai_encoder(x))\n",
        "        correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n",
        "        num_correct += correct.sum()\n",
        "    \n",
        "    return num_correct/N\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-19T14:06:59.108059Z",
          "iopub.execute_input": "2022-09-19T14:06:59.108549Z",
          "iopub.status.idle": "2022-09-19T14:07:17.815701Z",
          "shell.execute_reply.started": "2022-09-19T14:06:59.108504Z",
          "shell.execute_reply": "2022-09-19T14:07:17.814530Z"
        },
        "trusted": true,
        "id": "QJTzqSefag83"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed=10\n",
        "performance_dict={}\n",
        "for num in range(3):\n",
        "\n",
        "  head=train_head(seed=seed+num)\n",
        "  pct_correct = eval_head(head)\n",
        "\n",
        "  performance_dict[f'seed_{seed}'] = pct_correct \n",
        "\n",
        "performance_dict\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "VULhbDWawO_J",
        "outputId": "c2a5dc4e-89fd-4374-8843-ce586ba2de85"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorImageBW(0.1299, device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-409250035adc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mpct_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-7020a4e29990>\u001b[0m in \u001b[0;36mtrain_head\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdls_tune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m         )\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "performance_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hw0VjYxRx1va",
        "outputId": "6f9ccceb-8e7b-48bf-e624-987e88ead647"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'seed_0': TensorCategory(0.3205), 'seed_1': TensorCategory(0.2461)}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "COooHESjoNGb"
      }
    }
  ]
}