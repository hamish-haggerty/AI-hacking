{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We did some tests with pytest. Now we want to experiment with writing some tests with fastai/fastcore and also \n",
    "#with using the nbdev procedure etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8jFsEXz_61O",
    "outputId": "7a884ca7-e8fc-422c-d878-0c87e7a13488"
   },
   "outputs": [],
   "source": [
    "# !pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0\n",
    "# !pip install fastai==2.6.3 --no-deps\n",
    "# !pip install self_supervised\n",
    "\n",
    "# !pip install pytest\n",
    "# !pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pk01WY_Dag8s"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamishhaggerty/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fastai\n",
    "import self_supervised\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():device='cuda'\n",
    "else:device='cpu'\n",
    "    \n",
    "assert(fastai.__version__ == '2.6.3') #Check that version is 2.6.3\n",
    "from fastai.vision.all import *\n",
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *\n",
    "import inspect\n",
    "import warnings\n",
    "import random\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AOjr_YCLag8t"
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *\n",
    "import inspect\n",
    "import warnings\n",
    "import random\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import ipytest\n",
    "# ipytest.autoconfig()\n",
    "# import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test([1,2],[1,2], operator.eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.getLogger('PIL').setLevel(logging.WARNING)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "XTSdKC6bag8t"
   },
   "outputs": [],
   "source": [
    "#Like most other SSL algorithms BT's model consists of an encoder and projector (MLP) layer.\n",
    "#Definition is straightforward:\n",
    "#https://colab.research.google.com/github/KeremTurgutlu/self_supervised/blob/master/nbs/14%20-%20barlow_twins.ipynb#scrollTo=1M6QcUChcvpz\n",
    "class BarlowTwinsModel(Module):\n",
    "    \"\"\"An encoder followed by a projector\n",
    "    \"\"\"\n",
    "    def __init__(self,encoder,projector):self.encoder,self.projector = encoder,projector\n",
    "        \n",
    "    def forward(self,x): return self.projector(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZL3EE07Pag8u"
   },
   "outputs": [],
   "source": [
    "#HOWEVER instead of directly using the above, by passing both an encoder and a projector, create_barlow_twins_model\n",
    "#function can be used by minimally passing a predefined encoder and the expected input channels.\n",
    "\n",
    "#In the paper it's mentioned that MLP layer consists of 3 layers... following function will create a 3 layer\n",
    "#MLP projector with batchnorm and ReLU by default. Alternatively, you can change bn and nlayers. \n",
    "\n",
    "#Questions: Why torch.no_grad() when doing this?\n",
    "def create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n",
    "    \"Create Barlow Twins model\"\n",
    "    n_in  = in_channels(encoder)\n",
    "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
    "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n",
    "    apply_init(projector)\n",
    "    return BarlowTwinsModel(encoder, projector)\n",
    "\n",
    "#Similar to above. Simple API to make the BT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DFjGL-COag8v"
   },
   "outputs": [],
   "source": [
    "#BarlowTwins Callback\n",
    "#The following parameters can be passed:\n",
    "# - aug_pipelines\n",
    "# Imb lambda is the weight for redundancy reduction term in the loss function\n",
    "\n",
    "@delegates(get_multi_aug_pipelines)\n",
    "def get_barlow_twins_aug_pipelines(size,**kwargs): return get_multi_aug_pipelines(n=2,size=size,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xx4KsywAag8v"
   },
   "outputs": [],
   "source": [
    "#Uniform random number between a and b\n",
    "def Unif(a,b):\n",
    "    return (b-a)*torch.rand(1).item()+a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "l6gBw66Iag8y"
   },
   "outputs": [],
   "source": [
    "def random_sinusoid(x,std=0.15):\n",
    "    \n",
    "    t=torch.normal(mean=0,std=std,size=(1,1)).item()\n",
    "    s=torch.normal(mean=0,std=std,size=(1,1)).item()\n",
    "    \n",
    "    u=torch.normal(mean=0,std=1,size=(1,1)).item()\n",
    "    v=torch.normal(mean=0,std=1,size=(1,1)).item()\n",
    "    \n",
    "    a=torch.normal(mean=0,std=1,size=(1,1)).item()\n",
    "    b=torch.normal(mean=0,std=1,size=(1,1)).item()\n",
    "    \n",
    "    return torch.sin(t*math.pi*x+u) + torch.cos(s*math.pi*x + v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sinusoid_new(x,std=0.1,seed=0):\n",
    "\n",
    "    seed_everything(seed=seed)\n",
    "    \n",
    "    t=(std) * torch.randn(1,500).to(device)\n",
    "    #s=(std) * torch.randn(1,500).to(device)\n",
    "    \n",
    "    #u=torch.randn(1,500).to(device)\n",
    "    #v=torch.randn(1,500).to(device)\n",
    "\n",
    "    return torch.sin(t*x[:,]*math.pi)# + u) + torch.cos(s*x[:,]*math.pi + v)\n",
    "\n",
    "    #return torch.sin(t*math.pi*x+u) + torch.cos(s*math.pi*x + v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "li3BQ1ukX0XB"
   },
   "outputs": [],
   "source": [
    "def Exp_sample(scale=3.,loc=1.8):\n",
    "    \n",
    "    Expo = torch.distributions.exponential.Exponential\n",
    "    E = Expo(torch.tensor([scale]))\n",
    "#     if random.random()<0.5:\n",
    "#         return 1\n",
    "#     else:\n",
    "    return loc+E.sample().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MDYVO2u1U4We"
   },
   "outputs": [],
   "source": [
    "def p_norm(x,p):\n",
    "\n",
    "    eps=1e-8\n",
    "    if p<1:\n",
    "        x=x+eps\n",
    "    return (1/p)*torch.abs(x).pow(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Max_Corr(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #self.fc1 = nn.Linear(500,1)\n",
    "        #self.fc2 = nn.Linear(500,1)\n",
    "        self.t = nn.Parameter(0.1*torch.randn(1,500),requires_grad=True)\n",
    "        self.s = nn.Parameter(0.1*torch.randn(1,500),requires_grad=True)\n",
    "        \n",
    "    def forward(self,x,y):\n",
    "        \n",
    "        return (torch.sin(self.t*x[:,]*math.pi),torch.sin(self.s*y[:,]*math.pi))\n",
    "        \n",
    "        \n",
    "        #return (torch.sin(cast(self.fc1(x[:,])*math.pi,Tensor)),torch.sin(cast(self.fc2(y[:,])*math.pi,Tensor)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "a2Exs2s3ag8z"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class BarlowTwins(Callback):\n",
    "    order,run_valid = 9,True\n",
    "    def __init__(self, aug_pipelines, lmb=5e-3, print_augs=False):\n",
    "        assert_aug_pipelines(aug_pipelines)\n",
    "        self.aug1, self.aug2 = aug_pipelines\n",
    "        if print_augs: print(self.aug1), print(self.aug2)\n",
    "        store_attr('lmb')\n",
    "        self.index=-1\n",
    "        \n",
    "    def before_fit(self): \n",
    "        self.learn.loss_func = self.lf\n",
    "        nf = self.learn.model.projector[-1].out_features\n",
    "        self.I = torch.eye(nf).to(self.dls.device)\n",
    "\n",
    "    def update_seed(self):\n",
    "\n",
    "        #if self.index%8 == 0: #every `indexmod` index update the seed (best we have found so far)\n",
    "\n",
    "        self.seed = np.random.randint(0,10000)\n",
    "\n",
    "    def before_epoch(self):\n",
    "        self.index=-1\n",
    "            \n",
    "    def before_batch(self):\n",
    "                \n",
    "        xi,xj = self.aug1(self.x), self.aug2(self.x)\n",
    "        self.learn.xb = (torch.cat([xi, xj]),)\n",
    "\n",
    "        self.index=self.index+1\n",
    "\n",
    "        self.update_seed()\n",
    "\n",
    "        \n",
    "        #Uncomment to run standard BT\n",
    "    # def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
    "    #     bs,nf = pred.size(0)//2,pred.size(1)\n",
    "\n",
    "    #     z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "\n",
    "    #     z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "    #     z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "        \n",
    "    #     C = (z1norm.T @ z2norm) / bs \n",
    "    #     cdiff = (C - self.I)**2\n",
    "    #     loss = (cdiff*self.I + cdiff*(1-self.I)*self.lmb).sum() \n",
    "    #     return loss\n",
    "\n",
    "\n",
    "    def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
    "        bs,nf = pred.size(0)//2,pred.size(1)\n",
    "\n",
    "        #All standard, from BT\n",
    "        z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "        z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "        z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "        \n",
    "        max_corr=inner_step(z1norm,z2norm,I=self.I)\n",
    "        z1norm_2,z2norm_2 = max_corr(z1norm,z2norm)\n",
    "        \n",
    "        Ctem = (z1norm_2.T @ z2norm_2) / bs\n",
    "        cdiff_2 = Ctem.pow(2)\n",
    "\n",
    "        \n",
    "        C = (z1norm.T @ z2norm) / bs \n",
    "        cdiff = (C - self.I)**2\n",
    "\n",
    "\n",
    "\n",
    "        #{'Pr': 0.45546983480453496, 'dist': 'Exp', 'loc': 0.49445648193359376, 'scale': 2.5828861594200134, 'percent_correct': 0.8277832269668579} #Best so far\n",
    "\n",
    "        #self.update_seed()\n",
    "\n",
    "        #input('hi')\n",
    "\n",
    "        #polyprob=0.1\n",
    "        #polyprob=0.45546\n",
    "        #polyprob=1.0\n",
    "#         polyprob=0.0\n",
    "#         temrand = random.random()\n",
    "#         if temrand < polyprob: #With some probability we want off diag terms to be (quadratic) say.\n",
    "\n",
    "#             K=10\n",
    "#             cdiff_2=0\n",
    "#             for i in range(K):\n",
    "#             #p=Exp_sample(loc=1.5,scale=2.0)\n",
    "#             # #p=Unif(1.0,2.5)\n",
    "#                 z1norm_2 = random_sinusoid_new(z1norm,std=0.1,seed=self.seed+i)\n",
    "#                 z2norm_2 = random_sinusoid_new(z2norm,std=0.1,seed=2*self.seed+i)\n",
    "#                 C_2 = (z1norm_2.T @ z2norm_2) / bs\n",
    "#                 #cdiff_2 = (C_2)**2 #don't need to subtract I as only looking at off diag terms\n",
    "#                 cdiff_2 = cdiff_2 + (C_2)**2\n",
    "            \n",
    "#             cdiff_2=(1/K)*cdiff_2\n",
    "\n",
    "#             #symmetrize loss - so copy paste above block but swap place of 1 and 2\n",
    "#             #p=Exp_sample(loc=1.5,scale=2.0)\n",
    "#             #p=Unif(1.0,2.5)\n",
    "            \n",
    "#             # cdiff_2_sym=0\n",
    "#             # for i in range(K):\n",
    "#             #     z1norm_2 = random_sinusoid_new(z1norm,std=0.1,seed=4*self.seed+i)\n",
    "#             #     z2norm_2 = random_sinusoid_new(z2norm,std=0.1,seed=8*self.seed+i)\n",
    "\n",
    "#             #     C_2_sym = (z1norm_2.T @ z2norm_2) / bs\n",
    "#             #     cdiff_2_sym = cdiff_2_sym + C_2_sym**2\n",
    "            \n",
    "#             # cdiff_2_sym=(1/K)*cdiff_2_sym\n",
    "\n",
    "#             cdiff_2 = cdiff_2 #+ 0.5*cdiff_2_sym #Symmetrized random loss\n",
    "\n",
    "#             cdiff_2 = 0.2*cdiff +  0.8*cdiff_2 #convex comb of BT loss with random loss -> assumes polyprob=1.0\n",
    "            \n",
    "#         else:\n",
    "#             cdiff_2 = cdiff\n",
    "            \n",
    "            \n",
    "            \n",
    "        l2 = cdiff_2*(1-self.I)*self.lmb #Is either the standard term - or not.\n",
    "\n",
    "        loss = (cdiff*self.I + l2).sum() \n",
    "        return loss\n",
    "\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def show(self, n=1):\n",
    "        bs = self.learn.x.size(0)//2\n",
    "        x1,x2  = self.learn.x[:bs], self.learn.x[bs:] \n",
    "        idxs = np.random.choice(range(bs),n,False)\n",
    "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n",
    "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n",
    "        images = []\n",
    "        for i in range(n): images += [x1[i],x2[i]] \n",
    "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vbS1WtLiag80",
    "outputId": "9ea77dd3-d3df-4261-c22e-dcd436cd9e78"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m create_barlow_twins_model(fastai_encoder, hidden_size\u001b[38;5;241m=\u001b[39mhs,projection_size\u001b[38;5;241m=\u001b[39mps)\u001b[38;5;66;03m# projection_size=1024)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m aug_pipelines \u001b[38;5;241m=\u001b[39m get_barlow_twins_aug_pipelines(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m28\u001b[39m, rotate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,flip_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,resize_scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.7\u001b[39m,\u001b[38;5;241m1\u001b[39m), jitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, bw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,blur\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,blur_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,blur_s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cuda\u001b[38;5;241m=\u001b[39m(device\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m learn \u001b[38;5;241m=\u001b[39m Learner(\u001b[43mdls\u001b[49m, model, cbs\u001b[38;5;241m=\u001b[39m[BarlowTwins(aug_pipelines, print_augs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)])\n\u001b[1;32m      8\u001b[0m learn\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dls' is not defined"
     ]
    }
   ],
   "source": [
    "#Debugging cell - delete later (similar to cell below)\n",
    "ps=500\n",
    "hs=500\n",
    "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
    "model = create_barlow_twins_model(fastai_encoder, hidden_size=hs,projection_size=ps)# projection_size=1024)\n",
    "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=(device=='cuda'))\n",
    "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
    "learn.fit(10) #300                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Y5FN3Safag80"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"\"\n",
    "    Seed everything.\n",
    "    \"\"\"   \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def tune_set(items0,bs_tune=20):\n",
    "    items0=items0.shuffle()\n",
    "    d = {'0':0,'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0,'8':0,'9':0}\n",
    "    ITEMS=[]\n",
    "    for i in items0:\n",
    "        s=str(i).split('/training/')[1][0]\n",
    "        if d[s] is 0 or d[s] is 1:\n",
    "            ITEMS.append(i)\n",
    "            d[s]+=1\n",
    "    #items0=ITEMS\n",
    "\n",
    "    for i in items0:\n",
    "        if i not in ITEMS:\n",
    "            ITEMS.append(i)\n",
    "            \n",
    "    split = IndexSplitter(list(range(bs_tune)))\n",
    "\n",
    "    tds_tune = Datasets(ITEMS, [PILImageBW.create, [parent_label, Categorize()]], splits=split(ITEMS)) #Or do we want this?\n",
    "    dls_tune = tds_tune.dataloaders(bs=bs_tune,num_workers=0, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
    "    \n",
    "    return dls_tune\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xv4RE3O0ag81",
    "outputId": "c37cb7e8-0b16-4a16-c8f7-5756906fb01a"
   },
   "outputs": [],
   "source": [
    "#TODO: Do this in a slicker way. Write more tests \n",
    "#Get the dataloader and set batch size \n",
    "ts=16384 #training set size\n",
    "bs=512 #training batch size\n",
    "\n",
    "tune_s=2000 #we choose (say) 20 guys (randomly) out of theses 2000 to tune on\n",
    "bs_tune=20 #With MNIST this is also the size of the tune set\n",
    "\n",
    "bs_test=578 #Batch size for test set\n",
    "\n",
    "#Get the data\n",
    "path = untar_data(URLs.MNIST)\n",
    "items = get_image_files(path/'training') #i.e. NOT testing!!!\n",
    "items.sort() \n",
    "\n",
    "#Set random seed when defining/extracting training set, tuning set testing set for reproducibility\n",
    "seed=42\n",
    "seed_everything(seed=seed)\n",
    "items=items.shuffle()\n",
    "\n",
    "#Training set (for BT)\n",
    "items1 = items[0:ts] #train BT on these guys\n",
    "split = RandomSplitter(valid_pct=0.0)\n",
    "#tds = Datasets(items,splits=split(items)) #Do we want this?\n",
    "tds = Datasets(items1, [PILImageBW.create, [parent_label, Categorize()]], splits=split(items1)) #Or do we want this?\n",
    "dls = tds.dataloaders(bs=bs,num_workers=0, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
    "\n",
    "#We use items0 to extract tuning set, via the function `tune_set` above\n",
    "items0 = items[ts:ts+tune_s] #for fine tuning - just choose 2000 guys to extract 20 for fine tuning \n",
    "\n",
    "#Evaluate linear classifier on this guy\n",
    "test_bs=578\n",
    "items2 = items[ts+tune_s:] #test on the remaining set\n",
    "split = RandomSplitter(valid_pct=0.0) #randomly split training set into training and validation\n",
    "tds_test = Datasets(items2, [PILImageBW.create, [parent_label, Categorize()]], splits=split(items2)) #Or do we want this?\n",
    "dls_test = tds_test.dataloaders(bs=bs_test,num_workers=0, after_item=[ToTensor(), IntToFloatTensor()], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%ipytest -qq\n",
    "# #Test dataloaders\n",
    "\n",
    "# labeller = using_attr(RegexLabeller(pat = r'(\\d+).png$'), 'name')\n",
    "\n",
    "# def verify_DatasetShape(dls_obj,batch_size,ds_settype='train'):\n",
    "#     \"\"\"\"Helper function to verify shape of a dls object given the batch size; ds_settype is either `train` or \n",
    "#         `valid`. The idea is we want the batch_size to divide the length of the dlsobj.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     tem = len(getattr(dls_obj,ds_settype)) #length of dlsobj.train or dlsobj.valid depending on settpe\n",
    "#     return tem*batch_size == len(getattr(dls_obj,ds_settype+'_ds'))\n",
    "\n",
    "# def verify_first_item(items,expected):\n",
    "#     \"\"\"Helper function to verify first element of items is as expected, given random seed of 42\n",
    "#     \"\"\"\n",
    "#     if seed==42:\n",
    "#         return labeller(items[0]) == expected\n",
    "#     else:\n",
    "#         logging.debug('The seed is not 42 - is this ok?')\n",
    "#         return False\n",
    "\n",
    "# dls_tune=tune_set(items0,bs_tune=bs_tune) #used for generic tests below\n",
    "\n",
    "# def test_first_item():\n",
    "#     \"\"\"\"Verify that the first item of each dls_obj is as expected\n",
    "#     \"\"\"\n",
    "    \n",
    "#     assert verify_first_item(items1,'19825')\n",
    "    \n",
    "#     assert verify_first_item(items0,'40684')\n",
    "    \n",
    "#     assert verify_first_item(items2,'43064')\n",
    "    \n",
    "# def test_shape_dlsobjects():\n",
    "#     \"\"\"\"Test the shape of each dlsobj\n",
    "#     \"\"\"\n",
    "    \n",
    "#     assert verify_DatasetShape(dls,batch_size=bs,ds_settype='train')\n",
    "    \n",
    "#     assert verify_DatasetShape(dls_tune,batch_size=bs_tune,ds_settype='valid')\n",
    "    \n",
    "#     assert verify_DatasetShape(dls_test,batch_size=bs_test,ds_settype='train')\n",
    "    \n",
    "# def test_length_dlsobjects():\n",
    "#     \"\"\"\"Test the length of each dlsobj that we use\n",
    "#     \"\"\"\n",
    "    \n",
    "#     assert len(dls.train_ds) == ts and len(dls_tune.valid_ds) == bs_tune and len(dls_test.train_ds)==41616\n",
    "    \n",
    "# def test1_tune_set():\n",
    "#     \"\"\"Check whether the function `tune_set` gives us the expected values\"\"\"\n",
    "    \n",
    "#     tune_seed=10\n",
    "#     expected = {10:0.12255,11:0.153564,12:0.12781,13:0.129523,14:0.13019}\n",
    "#     for i in range(5):\n",
    "#         seed=tune_seed+i\n",
    "#         seed_everything(seed=seed)\n",
    "#         dls_tune=tune_set(items0,bs_tune=20)\n",
    "#         x_mean=0\n",
    "#         for x,y in dls_tune.valid:\n",
    "#             x_mean += x.mean()\n",
    "\n",
    "#         assert abs(x_mean-expected[seed])<0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "AKbw2pxMag82",
    "outputId": "07e65765-a9ae-42b5-9e8a-c05f4dab18b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#A \"reasonable\" composite augmentation: initially copy pasted BT. We run this cell a few times to check it makes sense\n",
    "#Also define encoder and model\n",
    "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
    "model = create_barlow_twins_model(fastai_encoder, hidden_size=10,projection_size=10)# projection_size=1024)\n",
    "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
    "#values for these which is tantamount to doing nothing\n",
    "#So if we choose resize_scale=(1,1) then the images look the same.\n",
    "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
    "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=False)\n",
    "#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
    "\n",
    "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
    "\n",
    "#dls.valid.bs = len(dls.valid_ds) #Set the validation dataloader batch size to be the length of the validation dataset\n",
    "\n",
    "b = dls.one_batch()\n",
    "learn._split(b)\n",
    "learn('before_batch')\n",
    "axes = learn.barlow_twins.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Kujpa-Lvag82"
   },
   "outputs": [],
   "source": [
    "#Simple linear classifier\n",
    "class LinearClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,zdim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(zdim,10) #As 10 classes for mnist\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = cast(self.fc1(x),Tensor) #so we have to use cross entropy loss. cast is because using old version fastai \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turnoffgrad_model(fastai_encoder):\n",
    "    for p in fastai_encoder.parameters():\n",
    "        p.requires_grad=False\n",
    "        \n",
    "    return fastai_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "QJTzqSefag83"
   },
   "outputs": [],
   "source": [
    "#NB: Will give same random 20-tune set (for fixed random seed), only if the cell\n",
    "#\"#Get the dataloader and set batch size\" is the same. Perhaps later we can make this cell a function of that one. \n",
    "#Functions to train and evaluate head\n",
    "fastai_encoder.eval()\n",
    "encoder_nograd = turnoffgrad_model(fastai_encoder) \n",
    "def train_head(encoder_nograd,tune_seed=10,bs_tune=20): #The seed choses a different (20) samples for training the head. 2 of each class\n",
    "    \"\"\"Train head on a tune_set, chosen through given tune_seed for reproducibility if needed\n",
    "    \"\"\"\n",
    "    seed_everything(seed=tune_seed) #Set the random seed, so that we choose a `fixed` tune set (i.e. as a function\n",
    "                                    # of the tune_seed)\n",
    "    dls_tune=tune_set(items0,bs_tune=bs_tune) #different random tune set each time (but as a function of tune_seed)\n",
    " \n",
    "    N=len(dls_tune.valid)*bs_tune \n",
    "    assert N == len(dls_tune.valid_ds) #Check that the tune set (valid) is divided by the batch size\n",
    "    assert len(dls_tune.valid_ds) == bs_tune\n",
    "\n",
    "    zdim=1024 #see above\n",
    "    head = LinearClassifier(zdim=zdim)\n",
    "    head.to(device)\n",
    "    optimizer = torch.optim.Adam(head.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(200):\n",
    "        #for x,y in dls_tune.valid: #Slows massively on colab but not on kaggle. Weird. \n",
    "        x,y=dls_tune.valid.one_batch() #Same every time since dataset only has length=batch size = 20.\n",
    "                                        #Will need to fix this for CIFAR10 etc\n",
    "\n",
    "        loss = criterion(head(encoder_nograd(x)),y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return head\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_head(head):\n",
    "    \"\"\"Evaluate the (typically trained) head on on the test set\n",
    "    \"\"\"\n",
    "    N=len(dls_test.train)*bs_test \n",
    "    assert N == len(dls_test.train_ds)\n",
    "\n",
    "    num_correct=0\n",
    "    for x,y in dls_test.train:\n",
    "    #for i in range(3):\n",
    "\n",
    "        ypred = head(encoder_nograd(x))\n",
    "        correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n",
    "        num_correct += correct.sum()\n",
    "    \n",
    "    return num_correct/N\n",
    "\n",
    "def eval_encoder(encoder_nograd,tune_seed=10):\n",
    "    \"\"\"\"Evaluate the encoder, which means to train and evaluate the head - basically wrap functions train_head\n",
    "        and eval_head\n",
    "    \"\"\"\n",
    "    head=train_head(encoder_nograd,tune_seed=tune_seed)\n",
    "    pct_correct = eval_head(head)\n",
    "    return pct_correct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "VULhbDWawO_J",
    "outputId": "130dedb3-22ab-4efa-a972-5f889a8501be"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tune_seed=10\n",
    "performance_dict={}\n",
    "for num in range(1):\n",
    "    \n",
    "    pct_correct = eval_encoder(encoder_nograd,tune_seed=tune_seed+num)\n",
    "    performance_dict[f'seed_{num}'] = pct_correct \n",
    "\n",
    "print(torch.mean(tensor(list(performance_dict.values()))))\n",
    "performance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test([1,2],[1,2], operator.eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
