{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a141c829",
   "metadata": {},
   "source": [
    "Ok, what's the plan? Remember (0) - loosely speaking, to get a good setup where we are training SSL models on \n",
    "datasets, and tuning them on classification problems. This is really the (0th!) step to then compare my own methods.\n",
    "\n",
    "We have already jumped ahead a bit and thought of some new ideas, and kind-of-started implementing them. However, \n",
    "the implementations are currently not so good. And of course, we want to learn more about best practises and so on.\n",
    "\n",
    "So yeah. We can't foresee everything at this point - need to just get started in some sense. But still with self \n",
    "reflection.\n",
    "\n",
    "Ok, so the plan for the next few hours (and will keep coming back to this): Implement BT / copy paste a BT implementation. This includes image augmentations and so on. Train model (in SSL way) then fine tune in \n",
    "classification way. Want to to do this in a \"minimal\" way first and then build up. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05afb14",
   "metadata": {},
   "source": [
    "Ok. I feel like I'm not progressing as fast as I could on this stuff at the moment. Hitting my head against the wall a bit. Going to set a timer for 1 hour. Then reassess progress in here. If progress is minimal, then perhaps it is time to take a step back a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bfc833",
   "metadata": {},
   "source": [
    "Ok so in BT.pynb and SBT.pynb we have a base to build on, namely: minimal working set up for BT and SBT on a dataset\n",
    "that is tiny mnist. (This includes plausible augmentation pipeline etc) However this dataset is \"too easy\" - a simply linear classifier (logistic regression) can get\n",
    "97% + accuracy. Similarly, training a linear head on encoder(data) where encoder is random (hasn't been trained)\n",
    "can also get 97% accuracy. So the next step is to do the following:\n",
    "\n",
    "        - Train BT on a harder dataset, say MNIST, and clean things up further. Should be possible to get better performance training a linear head on encoder(data) (for a TRAINED encoder) v.s. just a linear head, or vs a linear head training on a freshly initialized encoder. i.e. we want to get barlow twins working as a baseline. Then we can compare to SBT and other variants. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c1f676",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
