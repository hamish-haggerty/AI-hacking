{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[],"include_colab_link":true},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/hamish-haggerty/AI-hacking/blob/master/SSL/BT_MNIST_new3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"#Install\n!pip install self_supervised\n\n!pip install pytest\n!pip install ipytest","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X8jFsEXz_61O","outputId":"1f39715f-9abd-46cf-cfd8-750bf6e1c416","execution":{"iopub.status.busy":"2022-10-19T04:23:20.796415Z","iopub.execute_input":"2022-10-19T04:23:20.796901Z","iopub.status.idle":"2022-10-19T04:24:21.717201Z","shell.execute_reply.started":"2022-10-19T04:23:20.796819Z","shell.execute_reply":"2022-10-19T04:24:21.715919Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting self_supervised\n  Downloading self_supervised-1.0.4-py3-none-any.whl (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m170.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting timm>=0.4.5\n  Downloading timm-0.6.11-py3-none-any.whl (548 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.7/548.7 kB\u001b[0m \u001b[31m989.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from self_supervised) (22.1.2)\nRequirement already satisfied: kornia>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from self_supervised) (0.5.8)\nRequirement already satisfied: fastai>=2.2.7 in /opt/conda/lib/python3.7/site-packages (from self_supervised) (2.7.9)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from self_supervised) (21.3)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (1.7.3)\nRequirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (1.0.3)\nRequirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (0.12.0)\nRequirement already satisfied: fastcore<1.6,>=1.4.5 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (1.5.26)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (1.3.5)\nRequirement already satisfied: spacy<4 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (3.3.1)\nRequirement already satisfied: torch<1.14,>=1.7 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (1.11.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (2.28.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (3.5.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (1.0.2)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (6.0)\nRequirement already satisfied: fastdownload<2,>=0.0.5 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (0.0.7)\nRequirement already satisfied: pillow>6.0.0 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (9.1.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from timm>=0.4.5->self_supervised) (0.8.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->self_supervised) (3.0.9)\nRequirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.4.2)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.8.2)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.8)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.4.4)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.6)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.64.0)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.8)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.1.2)\nRequirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.10.1)\nCollecting typing-extensions<4.2.0,>=3.7.4\n  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.21.6)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.3)\nRequirement already satisfied: thinc<8.1.0,>=8.0.14 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (8.0.17)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.7.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.7)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.3.0)\nRequirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.6.2)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.10)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (59.8.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fastai>=2.2.7->self_supervised) (2022.6.15.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fastai>=2.2.7->self_supervised) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fastai>=2.2.7->self_supervised) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->fastai>=2.2.7->self_supervised) (2.1.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm>=0.4.5->self_supervised) (4.12.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm>=0.4.5->self_supervised) (3.7.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai>=2.2.7->self_supervised) (1.4.3)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai>=2.2.7->self_supervised) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai>=2.2.7->self_supervised) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai>=2.2.7->self_supervised) (4.33.3)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->fastai>=2.2.7->self_supervised) (2022.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (1.0.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<4->fastai>=2.2.7->self_supervised) (3.8.0)\nRequirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<4->fastai>=2.2.7->self_supervised) (5.2.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->fastai>=2.2.7->self_supervised) (1.15.0)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<4->fastai>=2.2.7->self_supervised) (8.0.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<4->fastai>=2.2.7->self_supervised) (2.1.1)\nInstalling collected packages: typing-extensions, timm, self_supervised\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.3.0\n    Uninstalling typing_extensions-4.3.0:\n      Successfully uninstalled typing_extensions-4.3.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ntensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible.\ntensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible.\ntensorflow 2.6.4 requires tensorboard<2.7,>=2.6.0, but you have tensorboard 2.10.0 which is incompatible.\ntensorflow 2.6.4 requires typing-extensions<3.11,>=3.7, but you have typing-extensions 4.1.1 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nflax 0.6.0 requires rich~=11.1, but you have rich 12.1.0 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.12.0 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\nallennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\naiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.72 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed self_supervised-1.0.4 timm-0.6.11 typing-extensions-4.1.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: pytest in /opt/conda/lib/python3.7/site-packages (7.1.3)\nRequirement already satisfied: py>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from pytest) (1.11.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from pytest) (21.3)\nRequirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest) (2.0.1)\nRequirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest) (1.0.0)\nRequirement already satisfied: iniconfig in /opt/conda/lib/python3.7/site-packages (from pytest) (1.1.1)\nRequirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest) (4.12.0)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest) (21.4.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest) (3.8.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->pytest) (3.0.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting ipytest\n  Downloading ipytest-0.12.0-py3-none-any.whl (15 kB)\nRequirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from ipytest) (7.33.0)\nRequirement already satisfied: pytest>=5.4 in /opt/conda/lib/python3.7/site-packages (from ipytest) (7.1.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from ipytest) (21.3)\nRequirement already satisfied: tomli>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest>=5.4->ipytest) (2.0.1)\nRequirement already satisfied: importlib-metadata>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest>=5.4->ipytest) (4.12.0)\nRequirement already satisfied: pluggy<2.0,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest>=5.4->ipytest) (1.0.0)\nRequirement already satisfied: py>=1.8.2 in /opt/conda/lib/python3.7/site-packages (from pytest>=5.4->ipytest) (1.11.0)\nRequirement already satisfied: iniconfig in /opt/conda/lib/python3.7/site-packages (from pytest>=5.4->ipytest) (1.1.1)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.7/site-packages (from pytest>=5.4->ipytest) (21.4.0)\nRequirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython->ipytest) (59.8.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython->ipytest) (4.8.0)\nRequirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->ipytest) (5.3.0)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->ipytest) (0.7.5)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython->ipytest) (0.18.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython->ipytest) (0.1.3)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->ipytest) (3.0.30)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->ipytest) (2.12.0)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->ipytest) (0.2.0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->ipytest) (5.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->ipytest) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (4.1.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython->ipytest) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython->ipytest) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipytest) (0.2.5)\nInstalling collected packages: ipytest\nSuccessfully installed ipytest-0.12.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"#import \nimport fastai\nimport self_supervised\nimport torch\nif torch.cuda.is_available():device='cuda'\nelse:device='cpu'\nfrom fastai.vision.all import *\nfrom self_supervised.augmentations import *\nfrom self_supervised.layers import *\nfrom torchvision import transforms\nimport inspect\nimport warnings\nimport random\nimport math\nwarnings.filterwarnings(\"ignore\")\nimport ipytest\nipytest.autoconfig()\nimport pytest","metadata":{"id":"Pk01WY_Dag8s","execution":{"iopub.status.busy":"2022-10-19T04:27:40.286779Z","iopub.execute_input":"2022-10-19T04:27:40.287420Z","iopub.status.idle":"2022-10-19T04:27:44.916936Z","shell.execute_reply.started":"2022-10-19T04:27:40.287371Z","shell.execute_reply":"2022-10-19T04:27:44.914712Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Base functions / classes we need to train a BT / RBT model.\nclass BarlowTwinsModel(Module):\n    \"\"\"An encoder followed by a projector\n    \"\"\"\n    def __init__(self,encoder,projector):self.encoder,self.projector = encoder,projector\n        \n    def forward(self,x): \n        \n        return self.projector(self.encoder(x))\n\n#In the paper it's mentioned that MLP layer consists of 3 layers... following function will create a 3 layer\n#MLP projector with batchnorm and ReLU by default. Alternatively, you can change bn and nlayers. \n\n#Questions: Why torch.no_grad() when doing this?\ndef create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n    \"Create Barlow Twins model\"\n    n_in  = in_channels(encoder)\n    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n    apply_init(projector)\n    return BarlowTwinsModel(encoder, projector)\n\n@delegates(get_multi_aug_pipelines)\ndef get_barlow_twins_aug_pipelines(size,**kwargs): return get_multi_aug_pipelines(n=2,size=size,**kwargs)\n\ndef random_sinusoid(x,std=0.1,seed=0):\n    \n    seed_everything(seed=seed)    \n    t=(std) * torch.randn(1,500).to(device)\n    s=(std) * torch.randn(1,500).to(device)\n    \n    u=torch.randn(1,500).to(device)\n    v=torch.randn(1,500).to(device)\n\n    a=(0.2) * torch.randn(1,500).to(device)\n    b=(0.2) * torch.randn(1,500).to(device)\n\n    return a*torch.sin(t*x[:,]*math.pi+u) + b*torch.cos(s*x[:,]*math.pi+v)\n\ndef C_z1z2(z1norm,z1norm_2,z2norm,z2norm_2,bs,indep=True):\n    \n    if indep == False:\n        Ctem1 =  (z1norm.T @ z2norm_2) / bs\n        Ctem2 = (z1norm_2.T @ z2norm) / bs\n        cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2))\n        \n    elif indep == True:\n        cdiff_2 =  (z1norm_2.T @ z2norm_2) / bs\n        \n\n    return cdiff_2\n\n\nclass Cdiff_Rand:\n    \n    def __init__(self,seed,bs,std=0.1,K=2):\n        self.seed=seed\n        self.std=std\n        self.K=K\n        self.bs=bs\n\n    def __call__(self,z1norm,z2norm):\n        \n        cdiff_rand=0\n        for i in range(self.K):\n\n            z1norm_2,z2norm_2 = random_sinusoid(z1norm,std=self.std,seed=self.seed+i), random_sinusoid(z2norm,std=self.std,seed=2*self.seed+i)\n            cdiff_rand = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)\n\n        cdiff_rand=(1/self.K)*cdiff_rand\n    \n        return cdiff_rand\n\nclass Max_Corr(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(ps,ps)\n        self.fc2 = nn.Linear(ps,ps)\n\n        self.fc3 = nn.Linear(ps,ps)\n        self.fc4 = nn.Linear(ps,ps)\n\n        self.sigmoid = nn.Sigmoid()\n\n        self.relu = nn.ReLU()\n    def forward(self,x,y):\n\n        x=self.sigmoid(self.fc1(x)) #when (sigmoid,relu) GREAT results, with (sigmoid,sigmoid) TERRIBLE. Currently testing (relu,relu)\n        x=self.fc2(x)\n       \n        y=self.relu(self.fc3(y)) #originally had relu and got really good results. If we can't reproduce those results, possible reasons:\n                                    #results were due to chance; or having relu on one branch (and sigmoid on the other) helps via breaking\n                                      #the symmetry! Other idea: set fc1=fc3, fc2=fc4. \n        y=self.fc4(y)\n\n        return x,y\n\nclass Cdiff_Sup:\n    \n    def __init__(self,I,inner_steps,bs):\n        \n        self.I=I\n        self.inner_steps=inner_steps\n        self.bs=bs\n        self.max_corr = Max_Corr()\n        if device == 'cuda':\n            self.max_corr.cuda()\n        \n    def inner_step(self,z1norm,z2norm):\n    \n        max_corr=self.max_corr\n        I=self.I\n        bs=self.bs\n        inner_steps=self.inner_steps\n\n        z1norm=z1norm.detach()\n        z2norm=z2norm.detach()\n\n        # z1norm=z1norm[:,0]\n        # z2norm=z2norm[:,0]\n\n        max_corr = Max_Corr()\n        max_corr.cuda()\n    \n        # for p in max_corr.parameters():\n        #     p.requires_grad=True]\n\n        optimizer = torch.optim.Adam(list(max_corr.parameters()),lr=0.001)\n        for i in range(inner_steps):\n            z1norm_2,z2norm_2=max_corr(z1norm,z2norm)\n            #z1norm_2 = (z1norm_2 - z1norm_2.mean(0)) / z1norm_2.std(0, unbiased=False)\n        \n            assert (z1norm_2.shape,z2norm_2.shape) == (z1norm.shape,z2norm.shape)\n\n            # Ctem1 =  (z1norm.T @ z2norm_2) / bs\n            # Ctem2 = (z1norm_2.T @ z2norm) / bs\n            # cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2))\n\n            cdiff_2 = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs,indep=False)\n            #cdiff_2=Ctem.pow(2)\n\n            inner_loss=-1*(cdiff_2*(1-I)).mean()\n\n            optimizer.zero_grad()\n            inner_loss.backward()\n            optimizer.step()\n        \n        for p in max_corr.parameters():\n            p.requires_grad=False\n            \n        return max_corr\n    \n    def __call__(self,z1norm,z2norm):\n        \n            max_corr =  self.inner_step(z1norm,z2norm)\n            z1norm_2,z2norm_2 = max_corr(z1norm,z2norm)\n      \n            cdiff_sup = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs,indep=False)\n    \n            return cdiff_sup\n\n\n#export\nclass BarlowTwins(Callback):\n    order,run_valid = 9,True\n    def __init__(self, aug_pipelines, lmb=5e-3, print_augs=False):\n        assert_aug_pipelines(aug_pipelines)\n        self.aug1, self.aug2 = aug_pipelines\n        if print_augs: print(self.aug1), print(self.aug2)\n        store_attr('lmb')\n        self.index=-1\n\n        self.inner_steps=4\n        \n    def before_fit(self): \n        self.learn.loss_func = self.lf\n        nf = self.learn.model.projector[-1].out_features\n        self.I = torch.eye(nf).to(self.dls.device)\n\n    def update_seed(self):\n        \n        indexmod=2\n        if self.index%indexmod == 0: #every `indexmod` index update the seed (best we have found so far)\n            self.seed = np.random.randint(0,10000)\n\n    def before_epoch(self):\n        self.index=-1\n\n        if self.epoch%10==0:\n            self.inner_steps += 1\n            \n    def before_batch(self):\n    \n        xi,xj = self.aug1(TensorImageBW(self.x)), self.aug2(TensorImageBW(self.x))\n        self.learn.xb = (torch.cat([xi, xj]),)\n\n        self.index=self.index+1\n        self.update_seed()\n\n        #Uncomment to run standard BT\n#     def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n#         bs,nf = pred.size(0)//2,pred.size(1)\n\n#         z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n\n#         z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n#         z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n        \n#         C = (z1norm.T @ z2norm) / bs \n#         cdiff = (C - self.I)**2\n#         loss = (cdiff*self.I + cdiff*(1-self.I)*self.lmb).sum() \n#         return loss\n\n\n    def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n        \n        bs,nf = pred.size(0)//2,pred.size(1)\n\n        #All standard, from BT\n        z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n        z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n        z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n        \n        C = (z1norm.T @ z2norm) / bs \n        cdiff = (C - self.I)**2\n\n\n\n        # #Let's change this block to rewritten (should do same thing)\n        # max_corr = inner_step(z1norm,z2norm,I=self.I,inner_steps=5)#,inner_steps=self.inner_steps)\n        # z1norm_2,z2norm_2 = max_corr(z1norm,z2norm)\n        # Ctem1 =  (z1norm.T @ z2norm_2) / bs\n        # Ctem2 = (z1norm_2.T @ z2norm) / bs\n        # #Ctem = (z1norm_2.T @ z2norm_2) / bs\n        # #cdiff_2 = Ctem.pow(2)\n        # cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2)) #+ 0.1*Ctem.pow(2)\n\n\n        CdiffSup = Cdiff_Sup(I=self.I,inner_steps=2,bs=bs)\n        cdiff_2 = CdiffSup(z1norm,z2norm)\n\n        CdiffRand = Cdiff_Rand(seed=self.seed,bs=bs,std=0.2,K=2)\n        cdiff_2_2 = CdiffRand(z1norm,z2norm)\n        cdiff_2 = 0.5*cdiff_2_2 + 0.5*cdiff_2\n        #cdiff_2 = cdiff_2_2\n            \n        l2 = cdiff_2*(1-self.I)*self.lmb #Is either the standard term - or not.\n\n        loss = (cdiff*self.I + l2).sum()\n        torch.cuda.empty_cache()\n        return loss\n\n    @torch.no_grad()\n    def show(self, n=1):\n \n        bs = self.learn.x.size(0)//2\n        x1,x2  = self.learn.x[:bs], self.learn.x[bs:]\n        #x1 = TensorImageBW(x1)\n        #x2 = TensorImageBW(x2)\n        idxs = np.random.choice(range(bs),n,False)\n        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n        images = []\n        for i in range(n): images += [x1[i],x2[i]]\n        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)","metadata":{"id":"zU4GwLruU5AD","execution":{"iopub.status.busy":"2022-10-19T09:07:08.216241Z","iopub.execute_input":"2022-10-19T09:07:08.216670Z","iopub.status.idle":"2022-10-19T09:07:08.286945Z","shell.execute_reply.started":"2022-10-19T09:07:08.216622Z","shell.execute_reply":"2022-10-19T09:07:08.286058Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#Functions / classes we need to evaluate the encoder - i.e. train linear head on frozen rep and evaluate. \nclass LinearClassifier(nn.Module):\n    \n    def __init__(self,zdim):\n        super().__init__()\n        self.fc1 = nn.Linear(zdim,10) #As 10 classes for mnist\n        \n    def forward(self,x):\n        x = cast(self.fc1(x),Tensor) #so we have to use cross entropy loss. cast is because using old version fastai \n        return x\n\ndef turnoffgrad_model(fastai_encoder):\n    for p in fastai_encoder.parameters():\n        p.requires_grad=False\n        \n    return fastai_encoder\n\ndef train_head(encoder_nograd,tune_seed=10,bs_tune=20): #The seed choses a different (20) samples for training the head. 2 of each class\n    \"\"\"Train head on a tune_set, chosen through given tune_seed for reproducibility if needed\n    \"\"\"\n                                    # of the tune_seed)\n    \n    dls_tune=tune_set(items0,seed=tune_seed,bs_tune=bs_tune) #different random tune set each time (but as a function of tune_seed)\n \n    N=len(dls_tune.valid)*bs_tune \n    assert N == len(dls_tune.valid_ds) #Check that the tune set (valid) is divided by the batch size\n    assert len(dls_tune.valid_ds) == bs_tune\n    \n    zdim=1024 \n    head = LinearClassifier(zdim=zdim)\n    head.to(device)\n    optimizer = torch.optim.Adam(head.parameters())\n    criterion = nn.CrossEntropyLoss()\n    for epoch in range(200):\n        #for x,y in dls_tune.valid: #Slows massively on colab but not on kaggle. Weird. \n        x,y=dls_tune.valid.one_batch() #Same every time since dataset only has length=batch size = 20.\n                                        #Will need to fix this for CIFAR10 etc\n\n        loss = criterion(head(encoder_nograd(x)),y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    return head\n\n@torch.no_grad()\ndef eval_head(head):\n    \"\"\"Evaluate the (typically trained) head on on the test set\n    \"\"\"\n    N=len(dls_test.train)*bs_test\n    assert N == len(dls_test.train_ds)\n\n    num_correct=0\n    for x,y in dls_test.train:\n\n        ypred = head(encoder_nograd(x))\n        correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n        num_correct += correct.sum()\n    \n    return num_correct/N\n\ndef eval_encoder(encoder_nograd,tune_seed=10):\n    \"\"\"\"Evaluate the encoder, which means to train and evaluate the head - basically wrap functions train_head\n        and eval_head\n    \"\"\"\n    head=train_head(encoder_nograd,tune_seed=tune_seed)\n    pct_correct = eval_head(head)\n    return pct_correct\n    ","metadata":{"id":"G7pPw34mzPG7","execution":{"iopub.status.busy":"2022-10-19T04:27:51.599653Z","iopub.execute_input":"2022-10-19T04:27:51.600232Z","iopub.status.idle":"2022-10-19T04:27:51.628225Z","shell.execute_reply.started":"2022-10-19T04:27:51.600197Z","shell.execute_reply":"2022-10-19T04:27:51.627015Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#In this cell we get the data for MNIST (including some helpful functions we can potentially use\n#to get CIFAR10 data etc)\ndef seed_everything(seed=42):\n    \"\"\"\"\n    Seed everything.\n    \"\"\"   \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\ndef tune_set(items0,seed=None,bs_tune=20):\n    \n    seed_everything(seed=seed)\n    \n    items0=items0.shuffle()\n    d = {'0':0,'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0,'8':0,'9':0}\n    ITEMS=[]\n    for i in items0:\n        s=str(i).split('/training/')[1][0]\n        if d[s] is 0 or d[s] is 1:\n            ITEMS.append(i)\n            d[s]+=1\n    #items0=ITEMS\n\n    for i in items0:\n        if i not in ITEMS:\n            ITEMS.append(i)\n            \n    split = IndexSplitter(list(range(bs_tune)))\n\n    tds_tune = Datasets(ITEMS, [PILImageBW.create, [parent_label, Categorize()]], splits=split(ITEMS)) #Or do we want this?\n    dls_tune = tds_tune.dataloaders(bs=bs_tune,num_workers=6, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n    \n    return dls_tune\n\n\ndef shuffle_items(items,seed):\n    \"\"\"Helper function to sort a list according to given random seed\n    \"\"\"\n    items.sort()\n    \n    if seed !=None:\n        seed_everything(seed=seed)\n        items=items.shuffle()\n    \n    return items\n\nclass BT_Data:\n    \n    def __init__(self,items,seed=42,ts=16384,bs=512,tune_s=2000,bs_tune=20,bs_test=578):\n        \n        self.ts=ts\n        self.bs=bs\n        self.tune_s=tune_s\n        self.bs_tune=bs_tune\n        self.bs_test=bs_test\n        \n        self._seed=seed\n        self.seed=seed\n        items=shuffle_items(items,seed)\n        self.items=items\n\n    @property\n    def seed(self):\n        return self._seed\n    \n    @seed.setter #When we update the seed, we update the datasets (so items and dls objects) accordingly\n    def seed(self,val):\n        self._seed=val\n        self.items = shuffle_items(items,val)\n        self.build_items_i()\n        self.build_dls()\n        \n    def build_items_i(self):\n        self.items1 = self.items[0:ts] #train BT on these guys\n        self.items0 = self.items[self.ts:self.ts+self.tune_s] #for fine tuning - just choose 2000 guys to extract 20 for fine tuning \n        self.items2 = self.items[self.ts+self.tune_s:] #test on remainder\n        \n    def build_dls(self):\n        \n        split = RandomSplitter(valid_pct=0.0)\n        tds = Datasets(self.items1, [PILImageBW.create, [parent_label, Categorize()]], splits=split(self.items1))\n        self.dls = tds.dataloaders(bs=self.bs,num_workers=6, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n\n        #Evaluate linear classifier on this guy\n        split = RandomSplitter(valid_pct=0.0) #randomly split training set into training and validation\n        tds_test = Datasets(self.items2, [PILImageBW.create, [parent_label, Categorize()]], splits=split(self.items2)) #Or do we want this?\n        self.dls_test = tds_test.dataloaders(bs=self.bs_test,num_workers=6, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n\npath = untar_data(URLs.MNIST)\nitems = get_image_files(path/'training') #i.e. NOT testing!!!\nitems.sort()\n\nseed=420\ntune_seed=100\nts=16384\nbs=512\ntune_s=2000\nbs_tune=20\nbs_test=578\n\nbt_data = BT_Data(items=items,seed=seed,ts=ts,bs=bs,tune_s=tune_s,bs_tune=bs_tune,bs_test=bs_test)\n\nitems1=bt_data.items1\nitems0=bt_data.items0\nitems2=bt_data.items2\n\ndls=bt_data.dls\ndls_test=bt_data.dls_test\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"YuBW-lc5s73g","outputId":"75fd9858-da9e-4212-9f28-af432fd7db3b","execution":{"iopub.status.busy":"2022-10-19T05:53:28.992299Z","iopub.execute_input":"2022-10-19T05:53:28.992906Z","iopub.status.idle":"2022-10-19T05:53:34.732522Z","shell.execute_reply.started":"2022-10-19T05:53:28.992863Z","shell.execute_reply":"2022-10-19T05:53:34.731228Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#In this cell we train BT / RBT and then evaluate\n\n#Train BT / RBT\nps=500\nhs=500\nfastai_encoder = create_fastai_encoder(xresnet18(),pretrained=False,n_in=1)\nmodel = create_barlow_twins_model(fastai_encoder, hidden_size=hs,projection_size=ps)# projection_size=1024)\naug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=True)\nlearn = Learner(dls,model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\nlearn.fit(200)\n\n#Test performance of encoder\nperformance_dict={}\nfastai_encoder.eval()\nencoder_nograd = turnoffgrad_model(fastai_encoder) \nfor num in range(5):\n     \n    pct_correct = eval_encoder(encoder_nograd,tune_seed=tune_seed+num)\n    performance_dict[f'seed_{num}'] = pct_correct \nprint(torch.mean(tensor(list(performance_dict.values()))))\nperformance_dict     ","metadata":{"id":"D8xzAoBCYoKx","outputId":"dd28ee29-3873-45ca-9ba8-eede32ed5800","colab":{"base_uri":"https://localhost:8080/","height":1000},"execution":{"iopub.status.busy":"2022-10-19T09:07:16.846747Z","iopub.execute_input":"2022-10-19T09:07:16.847453Z","iopub.status.idle":"2022-10-19T10:17:51.887682Z","shell.execute_reply.started":"2022-10-19T09:07:16.847415Z","shell.execute_reply":"2022-10-19T10:17:51.886470Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\nPipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>66.579887</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>47.536304</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>31.669376</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>23.851734</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>20.805325</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>17.274506</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>14.388918</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>12.299798</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>10.686383</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>9.082285</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>9.108425</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>9.027209</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>7.726142</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>6.895893</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>6.495681</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>5.708495</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>5.203595</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>5.005143</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>4.476947</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>4.312864</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>6.916646</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>6.678496</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>6.171301</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>5.641273</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>4.853310</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>4.304868</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>4.018959</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>3.645188</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>3.311947</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>3.387067</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>3.220933</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>3.012254</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>3.181002</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>3.063399</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>2.774419</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>2.665330</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>2.562198</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>2.348659</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>2.544705</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>3.030645</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.879021</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>2.745788</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>2.615041</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>2.337063</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>2.145840</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>2.090650</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>1.955830</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>1.961293</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>2.121016</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>2.265141</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>2.195697</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>2.184096</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>1.969266</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>1.819065</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>1.759766</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>1.718123</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>1.669247</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>1.688867</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>2.202646</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>2.186991</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.114543</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>1.924736</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>1.722057</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>1.579352</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>1.561929</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>1.445114</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>1.403961</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>1.799561</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>1.889829</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>1.862952</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.798757</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>1.597230</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>1.461218</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>1.443034</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>1.367452</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.311245</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>1.327331</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>1.410604</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>1.392026</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>1.485875</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.469533</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>1.348910</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>1.281328</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>1.241572</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>1.150830</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>1.135071</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>1.359346</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>1.459124</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>1.456650</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>1.409341</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.262727</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>1.160664</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>1.179065</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>1.133200</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>1.098279</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>1.084778</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>1.041970</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>0.993258</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>1.071031</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>1.073301</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.035097</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>101</td>\n      <td>1.071717</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>102</td>\n      <td>1.071031</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>103</td>\n      <td>0.995048</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>0.975571</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>1.401603</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>106</td>\n      <td>1.418949</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>107</td>\n      <td>1.336502</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>108</td>\n      <td>1.256466</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>109</td>\n      <td>1.111612</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.021412</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>111</td>\n      <td>1.015684</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>0.948734</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>113</td>\n      <td>0.913231</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>114</td>\n      <td>0.958862</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>1.005792</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>116</td>\n      <td>0.992272</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>117</td>\n      <td>1.035614</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>118</td>\n      <td>0.976922</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>119</td>\n      <td>0.911932</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.896924</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>121</td>\n      <td>0.894366</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>122</td>\n      <td>0.859998</td>\n      <td>None</td>\n      <td>00:22</td>\n    </tr>\n    <tr>\n      <td>123</td>\n      <td>0.899756</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>124</td>\n      <td>1.034786</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.017324</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>1.002888</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>127</td>\n      <td>0.978452</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>0.906286</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>129</td>\n      <td>0.860428</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>0.911945</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>131</td>\n      <td>0.852928</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>132</td>\n      <td>0.822518</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>133</td>\n      <td>0.805465</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>134</td>\n      <td>0.771081</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>0.786633</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>0.843415</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>137</td>\n      <td>0.827319</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>0.793020</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>139</td>\n      <td>0.840111</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.827899</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>141</td>\n      <td>0.793358</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>142</td>\n      <td>0.862816</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>143</td>\n      <td>1.168926</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>144</td>\n      <td>1.102087</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>1.067749</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>146</td>\n      <td>0.964091</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>147</td>\n      <td>0.860383</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>148</td>\n      <td>0.789129</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>149</td>\n      <td>0.784175</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.724471</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>151</td>\n      <td>0.711548</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>152</td>\n      <td>0.700002</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>153</td>\n      <td>0.676929</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>154</td>\n      <td>0.715660</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>0.774279</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>156</td>\n      <td>0.749053</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>157</td>\n      <td>0.729513</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>158</td>\n      <td>0.749397</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>159</td>\n      <td>0.721878</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.688944</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>161</td>\n      <td>0.694389</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>162</td>\n      <td>0.672571</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>163</td>\n      <td>0.645640</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>164</td>\n      <td>0.710859</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>0.771482</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>166</td>\n      <td>0.740551</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>167</td>\n      <td>0.765052</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>168</td>\n      <td>0.746459</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>169</td>\n      <td>0.675223</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>0.653107</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>171</td>\n      <td>0.644805</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>172</td>\n      <td>0.629333</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>173</td>\n      <td>0.651518</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>174</td>\n      <td>0.705456</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>0.672781</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>176</td>\n      <td>0.650792</td>\n      <td>None</td>\n      <td>00:22</td>\n    </tr>\n    <tr>\n      <td>177</td>\n      <td>0.673376</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>178</td>\n      <td>0.641000</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>179</td>\n      <td>0.644319</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.672033</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>181</td>\n      <td>0.762388</td>\n      <td>None</td>\n      <td>00:21</td>\n    </tr>\n    <tr>\n      <td>182</td>\n      <td>0.746413</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>183</td>\n      <td>0.765378</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>184</td>\n      <td>0.702088</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>0.642970</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>186</td>\n      <td>0.628551</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>187</td>\n      <td>0.619086</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>188</td>\n      <td>0.578736</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>189</td>\n      <td>0.596461</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>0.610585</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>191</td>\n      <td>0.622526</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>192</td>\n      <td>0.657007</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>193</td>\n      <td>0.727957</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>194</td>\n      <td>0.672820</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>0.623470</td>\n      <td>None</td>\n      <td>00:20</td>\n    </tr>\n    <tr>\n      <td>196</td>\n      <td>0.626543</td>\n      <td>None</td>\n      <td>00:18</td>\n    </tr>\n    <tr>\n      <td>197</td>\n      <td>0.585820</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>198</td>\n      <td>0.582660</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n    <tr>\n      <td>199</td>\n      <td>0.619785</td>\n      <td>None</td>\n      <td>00:19</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}},{"name":"stdout","text":"tensor(0.8343)\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"{'seed_0': TensorCategory(0.8144),\n 'seed_1': TensorCategory(0.8173),\n 'seed_2': TensorCategory(0.8706),\n 'seed_3': TensorCategory(0.8423),\n 'seed_4': TensorCategory(0.8267)}"},"metadata":{}}]},{"cell_type":"code","source":"#So, we are applying the random functions `as in the theorem` and getting good results.\n#E\n#This is extremely close to commit cbd0512... where we had  #tensor(0.8379). See below\n\n#Inner steps = 5\n\nMBT_42_10_run1 = {'seed_0': TensorCategory(0.8091),\n 'seed_1': TensorCategory(0.7975),\n 'seed_2': TensorCategory(0.9041),\n 'seed_3': TensorCategory(0.8573),\n 'seed_4': TensorCategory(0.8456)} #tensor(0.8427)\nprint(torch.mean(tensor(list(MBT_42_10_run1.values()))))#tensor(0.8427)\n\nMBT_420_100_run1 = {'seed_0': TensorCategory(0.7947),\n 'seed_1': TensorCategory(0.7877),\n 'seed_2': TensorCategory(0.8431),\n 'seed_3': TensorCategory(0.8303),\n 'seed_4': TensorCategory(0.7821)}\nprint(torch.mean(tensor(list(MBT_420_100_run1.values())))) #tensor(0.8126)\n\n#Inner steps = 2: Better!\nMBT_420_100_run2 = {'seed_0': TensorCategory(0.8144),\n 'seed_1': TensorCategory(0.8173),\n 'seed_2': TensorCategory(0.8706),\n 'seed_3': TensorCategory(0.8423),\n 'seed_4': TensorCategory(0.8267)}\nprint(torch.mean(tensor(list(MBT_420_100_run1.values())))) #tensor(0.8343)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-10-19T07:19:35.505111Z","iopub.execute_input":"2022-10-19T07:19:35.505489Z","iopub.status.idle":"2022-10-19T07:19:35.520746Z","shell.execute_reply.started":"2022-10-19T07:19:35.505446Z","shell.execute_reply":"2022-10-19T07:19:35.519464Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"tensor(0.8427)\ntensor(0.8076)\n","output_type":"stream"}]},{"cell_type":"code","source":"(0.8427+0.8076)/2","metadata":{"execution":{"iopub.status.busy":"2022-10-19T07:20:02.602346Z","iopub.execute_input":"2022-10-19T07:20:02.602721Z","iopub.status.idle":"2022-10-19T07:20:02.609983Z","shell.execute_reply.started":"2022-10-19T07:20:02.602683Z","shell.execute_reply":"2022-10-19T07:20:02.608896Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0.82515"},"metadata":{}}]},{"cell_type":"code","source":"#Old results from cbd0512 for comparison\nMBT_42_10_run1 = {'seed_0': TensorCategory(0.8238),\n 'seed_1': TensorCategory(0.7834),\n 'seed_2': TensorCategory(0.8999),\n 'seed_3': TensorCategory(0.8601),\n 'seed_4': TensorCategory(0.8221)}\n\nprint(torch.mean(tensor(list(MBT_42_10_run1.values()))))#tensor(0.8379)\n\nMBT_420_100_run1={'seed_0': TensorCategory(0.8016),\n 'seed_1': TensorCategory(0.7849),\n 'seed_2': TensorCategory(0.8416),\n 'seed_3': TensorCategory(0.8364),\n 'seed_4': TensorCategory(0.7986)}\nprint(torch.mean(tensor(list(MBT_420_100_run1.values())))) #tensor(0.8126)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-19T07:19:40.082283Z","iopub.execute_input":"2022-10-19T07:19:40.083064Z","iopub.status.idle":"2022-10-19T07:19:40.094492Z","shell.execute_reply.started":"2022-10-19T07:19:40.083020Z","shell.execute_reply":"2022-10-19T07:19:40.093272Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"tensor(0.8379)\ntensor(0.8126)\n","output_type":"stream"}]},{"cell_type":"code","source":"(0.8379+0.8126)/2","metadata":{"execution":{"iopub.status.busy":"2022-10-19T07:20:17.165233Z","iopub.execute_input":"2022-10-19T07:20:17.165604Z","iopub.status.idle":"2022-10-19T07:20:17.172347Z","shell.execute_reply.started":"2022-10-19T07:20:17.165574Z","shell.execute_reply":"2022-10-19T07:20:17.171100Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"0.82525"},"metadata":{}}]},{"cell_type":"markdown","source":"Comments","metadata":{"id":"pbepNL0JxTmI"}},{"cell_type":"markdown","source":"Below we have some tests and example of augmentations used:","metadata":{"id":"Go-6_SkNxZDa"}},{"cell_type":"code","source":"%%ipytest -qq\n#Tests\n\nlabeller = using_attr(RegexLabeller(pat = r'(\\d+).png$'), 'name')\nconvert_tensor = transforms.ToTensor()\nExpected_first_item = {42:{'items1':'19825','items0':'40684','items2':'43064'},420:{'items1':'44942','items0':'23821','items2':'908'}}\nExpected_first_dls = {42:{'dls':0.085169,'dls_test':0.099924},420:{'dls':0.183678,'dls_test':0.162825}}\nExpected_first_dls_tune = {(42,55):0.093707,(420,55):0.1513355}\n\ndef build_BT_Data(items,seed,tune_seed):\n    \"\"\" Helper function to get dictionary of data we need for testing purposes\n    \"\"\"\n    ts=16384\n    bs=512\n    tune_s=2000\n    bs_tune=20\n    bs_test=578\n    \n    k=dict(seed=seed,ts=ts,bs=bs,tune_s=tune_s,bs_tune=bs_tune,bs_test=bs_test)\n    bt_dataset = BT_Data(items=items,**k)\n\n    items = bt_dataset.items\n    items1 = bt_dataset.items1\n    items0 = bt_dataset.items0\n    items2 = bt_dataset.items2\n\n    dls = bt_dataset.dls\n    dls_test = bt_dataset.dls_test\n\n    dls_tune=tune_set(items0=items0,seed=seed,bs_tune=bs_tune)\n    \n    return dict(seed=seed,tune_seed=tune_seed,items=items,items1=items1,items0=items0,items2=items2,dls=dls,dls_test=dls_test,dls_tune=dls_tune)\n\nbt_data_42 = build_BT_Data(items=items,seed=42,tune_seed=10)\nbt_data_420 = build_BT_Data(items=items,seed=420,tune_seed=100)\n\ndef verify_DatasetShape(dls_obj,batch_size,ds_settype='train'):\n    \"\"\"\"Helper function to verify shape of a dls object given the batch size; ds_settype is either `train` or \n        `valid`. The idea is we want the batch_size to divide the length of the dlsobj.\n    \"\"\"\n    \n    tem = len(getattr(dls_obj,ds_settype)) #length of dlsobj.train or dlsobj.valid depending on settpe\n    return tem*batch_size == len(getattr(dls_obj,ds_settype+'_ds'))\n\ndef verify_first_item(items,expected):\n    \"\"\"Helper function to verify first element of items is as expected, given random seed of 42\n    \"\"\"\n        \n    return labeller(items[0]) == expected\n        \ndef verify_first_dls(dls_obj,expected,ds_settype='train'):\n    \"\"\"Helper function to verify first element of the given dls object is as expected, given random seed of 42.\n        Note that ds_settype is either `train` or `valid\n    \"\"\"\n    \n    #All we are doing here is getting the first tensor in, for example e.g. dls_obj.train_ds and computing\n    #the mean of all the elements. If random seed is same, then it should give the same results\n    z=convert_tensor(next(iter(getattr(dls_obj,ds_settype+'_ds')))[0]).mean().item()\n    \n    #logging.debug(f'with {ds_settype} has: {z}')\n    assert z-expected < 0.0001\n\n@pytest.mark.parametrize('bt_dataset',[bt_data_42,bt_data_420])\nclass Test_shapes:\n    \n    def test_shape_dlsobjects(self,bt_dataset):\n        \"\"\"\"Test the shape of each dlsobj\n        \"\"\"\n    \n        assert verify_DatasetShape(bt_dataset['dls'],batch_size=bs,ds_settype='train')\n\n        assert verify_DatasetShape(bt_dataset['dls_tune'],batch_size=bs_tune,ds_settype='valid')\n\n        assert verify_DatasetShape(bt_dataset['dls_test'],batch_size=bs_test,ds_settype='train')\n    \n    def test_length_dlsobjects(self,bt_dataset):\n        \"\"\"\"Test the length of each dlsobj that we use\n        \"\"\"\n        assert len(bt_dataset['dls'].train_ds) == ts and len(bt_dataset['dls_tune'].valid_ds) == bs_tune and len(bt_dataset['dls_test'].train_ds)==41616\n    \n    \n@pytest.mark.parametrize('bt_dataset',[bt_data_42,bt_data_420])   \nclass Test_first:\n    \n    def test_first_item(self,bt_dataset):\n        \"\"\"\"Verify that the first item of each items is as expected\n        \"\"\"\n\n        seed = bt_dataset['seed']\n\n        assert verify_first_item(bt_dataset['items1'],Expected_first_item[seed]['items1'])\n\n        assert verify_first_item(bt_dataset['items0'],Expected_first_item[seed]['items0'])\n\n        assert verify_first_item(bt_dataset['items2'],Expected_first_item[seed]['items2'])\n\n    def test_first_dlsobj(self,bt_dataset):\n        \"\"\"Verify that the first item of each dlsobj is as expected\n        \"\"\"\n        seed = bt_dataset['seed']\n        dls = bt_dataset['dls']\n        dls_test = bt_dataset['dls_test']\n        items0 = bt_dataset['items0']\n\n        verify_first_dls(dls,ds_settype='train',expected=Expected_first_dls[seed]['dls'])\n        verify_first_dls(dls_test,ds_settype='train',expected=Expected_first_dls[seed]['dls_test'])\n\n        tune_seed=55\n        dls_tune=tune_set(items0,seed=tune_seed,bs_tune=bs_tune)\n\n        verify_first_dls(dls_tune,ds_settype='valid',expected=Expected_first_dls_tune[(seed,tune_seed)])\n\n\n@pytest.mark.parametrize('bt_dataset',[bt_data_42,bt_data_420])\ndef test1_tune_set(bt_dataset):\n    \"\"\"Check whether the function `tune_set` gives us the expected values\"\"\"\n    \n\n    seed=bt_dataset['seed']\n    tune_seed=bt_dataset['tune_seed']\n    items0 = bt_dataset['items0']\n    \n    if tune_seed==10 and seed==42:\n        expected = {10:0.12255,11:0.153564,12:0.12781,13:0.129523,14:0.13019}\n    \n    elif tune_seed==100 and seed==420:\n        expected={100:0.136104,101:0.120989,102:0.1381390,103:0.1380412,104:0.14285138}\n        \n    for i in range(5):\n        #seed_everything(seed=seed)\n        dls_tune=tune_set(items0,seed=tune_seed+i,bs_tune=20)\n        x_mean=0\n        for x,y in dls_tune.valid:\n            x_mean += x.mean()\n\n        assert abs(x_mean-expected[tune_seed+i])<0.0001\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1z3pyrAB8rpN","outputId":"fdbfda5d-9d3d-49b6-90d9-4c0655164b65"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                   [100%]\u001b[0m\n"}]},{"cell_type":"code","source":"#In this cell we display the augmentations used. \nfastai_encoder = create_fastai_encoder(xresnet18(),pretrained=False,n_in=1)\nmodel = create_barlow_twins_model(fastai_encoder, hidden_size=10,projection_size=10)# projection_size=1024)\naug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=False)\nlearn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\nb = dls.one_batch()\nlearn._split(b)\nlearn('before_batch')\naxes = learn.barlow_twins.show(n=2)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":413},"id":"AKbw2pxMag82","outputId":"f902e81a-66b9-4301-f6ad-dc21b95c72e8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":"Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n\nPipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x432 with 4 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}