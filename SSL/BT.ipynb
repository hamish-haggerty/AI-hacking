{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e41dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building / continuing from SL_library_dataloading, we consider BT from here: https://keremturgutlu.github.io/self_supervised/14%20-%20barlow_twins.html\n",
    "#and here https://keremturgutlu.github.io/self_supervised/14%20-%20barlow_twins.html#BarlowTwinsModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514cae59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastai\n",
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a893d809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamishhaggerty/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4abb754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *\n",
    "import inspect\n",
    "\n",
    "#These are imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df738458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.torch_core.Module"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Module #So some fastai thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40bd1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Like most other SSL algorithms BT's model consists of an encoder and projector (MLP) layer.\n",
    "#Definition is straightforward:\n",
    "#https://colab.research.google.com/github/KeremTurgutlu/self_supervised/blob/master/nbs/14%20-%20barlow_twins.ipynb#scrollTo=1M6QcUChcvpz\n",
    "\n",
    "class BarlowTwinsModel(Module):\n",
    "    \"\"\"An encoder followed by a projector\n",
    "    \"\"\"\n",
    "    def __init__(self,encoder,projector):self.encoder,self.projector = encoder,projector\n",
    "        \n",
    "    def forward(self,x): return self.projector(self.encoder(x))\n",
    "    \n",
    "    \n",
    "#Nothing much to this: Just a simple API for the BT model, with inputs encoder and projector. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f629c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOWEVER instead of directly using the above, by passing both an encoder and a projector, create_barlow_twins_model\n",
    "#function can be used by minimally passing a predefined encoder and the expected input channels.\n",
    "\n",
    "#In the paper it's mentioned that MLP layer consists of 3 layers... following function will create a 3 layer\n",
    "#MLP projector with batchnorm and ReLU by default. Alternatively, you can change bn and nlayers. \n",
    "\n",
    "\n",
    "#Questions: Why torch.no_grad() when doing this?\n",
    "def create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n",
    "    \"Create Barlow Twins model\"\n",
    "    n_in  = in_channels(encoder)\n",
    "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
    "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n",
    "    apply_init(projector)\n",
    "    return BarlowTwinsModel(encoder, projector)\n",
    "\n",
    "#Similar to above. Simple API to make the BT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f420c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can use self_supervised.layers module to create an encoder. \n",
    "\n",
    "encoder = create_encoder(\"tf_efficientnet_b0_ns\",n_in=3,pretrained=False,pool_type=PoolingType.CatAvgMax)\n",
    "model = create_barlow_twins_model(encoder,hidden_size=2048,projection_size=128,nlayers=2)\n",
    "out = model(torch.randn((2,3,224,224))); out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a158cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function self_supervised.augmentations.get_multi_aug_pipelines(n, size, rotate=True, jitter=True, bw=True, blur=True, resize_scale=(0.2, 1.0), resize_ratio=(0.75, 1.3333333333333333), rotate_deg=30, jitter_s=0.6, blur_s=(4, 32), same_on_batch=False, flip_p=0.5, rotate_p=0.3, jitter_p=0.3, bw_p=0.3, blur_p=0.3, stats=([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), cuda=False, xtra_tfms=[])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_multi_aug_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37acc15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BarlowTwins Callback\n",
    "#The following parameters can be passed:\n",
    "# - aug_pipelines\n",
    "# Imb lambda is the weight for redundancy reduction term in the loss function\n",
    "\n",
    "@delegates(get_multi_aug_pipelines)\n",
    "def get_barlow_twins_aug_pipelines(size,**kwargs): return get_multi_aug_pipelines(n=2,size=size,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "126b5cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(size, rotate=True, jitter=True, bw=True, blur=True, resize_scale=(0.2, 1.0), resize_ratio=(0.75, 1.3333333333333333), rotate_deg=30, jitter_s=0.6, blur_s=(4, 32), same_on_batch=False, flip_p=0.5, rotate_p=0.3, jitter_p=0.3, bw_p=0.3, blur_p=0.3, stats=([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), cuda=False, xtra_tfms=[])\n"
     ]
    }
   ],
   "source": [
    "aug_pipelines = get_barlow_twins_aug_pipelines(size=28,rotate=False,jitter=False,bw=False,blur=False,stats=None,cuda=False)\n",
    "aug_pipelines\n",
    "\n",
    "print(inspect.signature(get_barlow_twins_aug_pipelines)) #If we comment out @delegates above, then only size is printed\n",
    "                                                         #here; i.e. just prints **kwargs instead of all the actual \n",
    "                                                         #keyword arguments!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61abfd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BarlowTwins(Callback):\n",
    "    order,run_valid = 9,True\n",
    "    def __init__(self, aug_pipelines, lmb=5e-3, print_augs=False):\n",
    "        assert_aug_pipelines(aug_pipelines)\n",
    "        self.aug1, self.aug2 = aug_pipelines\n",
    "        if print_augs: print(self.aug1), print(self.aug2)\n",
    "        store_attr('lmb')\n",
    "        \n",
    "        \n",
    "    def before_fit(self): \n",
    "        self.learn.loss_func = self.lf\n",
    "        nf = self.learn.model.projector[-1].out_features\n",
    "        self.I = torch.eye(nf).to(self.dls.device)\n",
    "                    \n",
    "            \n",
    "    def before_batch(self):\n",
    "        xi,xj = self.aug1(self.x), self.aug2(self.x)\n",
    "        self.learn.xb = (torch.cat([xi, xj]),)\n",
    "        \n",
    "    \n",
    "    #loss function for BT. This is where the action is at: and potentially where I can make my edits...\n",
    "    def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
    "        bs,nf = pred.size(0)//2,pred.size(1)\n",
    "\n",
    "        z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "\n",
    "        z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "        z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "        \n",
    "        C = (z1norm.T @ z2norm) / bs \n",
    "        cdiff = (C - self.I)**2\n",
    "        loss = (cdiff*self.I + cdiff*(1-self.I)*self.lmb).sum() \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def show(self, n=1):\n",
    "        bs = self.learn.x.size(0)//2\n",
    "        x1,x2  = self.learn.x[:bs], self.learn.x[bs:] \n",
    "        idxs = np.random.choice(range(bs),n,False)\n",
    "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n",
    "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n",
    "        images = []\n",
    "        for i in range(n): images += [x1[i],x2[i]] \n",
    "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ae54f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "items = get_image_files(path)\n",
    "tds = Datasets(items, [PILImageBW.create, [parent_label, Categorize()]], splits=GrandparentSplitter()(items))\n",
    "dls = tds.dataloaders(bs=bs, after_item=[ToTensor(), IntToFloatTensor()], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a17ce95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "709"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "76cd6de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVIAAAKaCAYAAABshtGrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAweklEQVR4nO3deXBedfn38dMtTdJm75Y2TZvu1AKltICAPwYZFMQFRVHAHcYRBxlwEJhRQQYRRVwGRXEQZUTUQZgRRgTEFUfoFJFSWro3bZOmTZNma5smTZfnn9/jw8Pnc6cnubLdyfv15yf3fZ+TcJ+LM+fq9f2OOn78eAIA6L3Rg30CAJDtKKQAEEQhBYAgCikABFFIASBobHc/XLlypbT0a2tr5XU1NTWStbe3SzZ6tNbtsWP1FEpKSiSbOXOmPcfKykrJJk2aJFlBQYFk48ePlyztv2IYNWpUn75umMmGX5p/roLesN9t7kgBIIhCCgBBFFIACKKQAkDQqO6aKzU1NfLDQ4cOyes6Ojok6+rqksw1m/Lz8yWbMGGCZK5ZlOm1x44dk+zw4cOSHT16VDL393ANI9ckc9m4ceMkc3+HYYZmE4Yrmk0A0B8opAAQRCEFgCAKKQAEddts6uzslB9GGjSOa8a49x48eNC+v6WlRbLW1lbJ2traJHMNKHc+aZtfbiJr4sSJkuXk5Eg2zBpQNJsGWWNjo2SrVq2S7M9//rNk7hpP68wzz5TsHe94h2SZJhWzAM0mAOgPFFIACKKQAkAQhRQAgrpdRi+yzJzjmjsHDhyQrKGhQbKdO3faz9y8ebNk7kF7Z2enZEeOHJHM/c5lZWWSVVRUSDZnzhzJ3DJ/bgJqmDWb0E9cI2j9+vWSPfroo5L985//lGzlypWpjpHWySefLNm///1vyT75yU9KtnTp0l4fd7Bx9QJAEIUUAIIopAAQRCEFgKBum02OmzpKu2ydawJVV1dL9tprr0nm9opKkiRpbm6WzDWw3LJ++/fvl8ydt2s2uSaSW06wuLhYMjcp5RpQwFvV1dVJ9q1vfUuy3/3ud5LNnj1bsksvvVSySOPTNX/vv/9+ydyk4c9+9rNeH3ewcUcKAEEUUgAIopACQBCFFACCetzhcFMPbmpo3759krkH0W7qYcuWLZK1t7fb83FNJNcwampqSnWc+vp6ySZNmiSZm8JwS+addNJJqT5vhO7thB568cUXJXPN2QsvvFCy97znPZJdfPHFko0ZM6aXZ+ebXDfddJNk7rrPZlypABBEIQWAIAopAARRSAEgqNtmk1syzzV33L5JbmJpzZo1ktXU1Ejm9mdyx810jm55PNe4cQ/VXTPNTWG4qai0E17udZHlCTFyuGXqHnnkEclOOeUUyfq6eem+x9OnT0/1XteYzWbckQJAEIUUAIIopAAQRCEFgKBum01pmyeuOeSWsnMTUK7J4h6KZ3o47Za4y8/Pl8w1h9xydm6ZQNeAmjx5smRuyTy3B5T7/dxxgbdauHBhqtcNxFTc1q1bJXvggQdSvffrX/96H5/N4OKOFACCKKQAEEQhBYAgCikABFFIASCox137tKOMrlvtuuSu6+7W65w6dao9zqxZsyTLzc2VbPv27ZK5f1mwZ88eydxaoRUVFZLNnDlTssLCQski6z1iZBus787atWslu+OOOyRz64yedtppkp111ll9c2JDBHekABBEIQWAIAopAARRSAEgqNtmkxszGztW3+KaO26k061V6BpLbvyyvLzcnqMbrWxoaJDMNZbc6Kc7n5KSEskqKyslcw2ogoICyVzDgBFRDAa3dq9bS/gHP/iBZM8995xkZ599tmS33HKLZK7xnM24IwWAIAopAARRSAEgiEIKAEE9bja5RkleXp5krmnjHjC75pVbT9RNFyWJn0TasWOHZDt37pTMra3qznvatGmSuYkq1yRzv/NArBWJkcNNG7rv9oYNGyRbt26dZH/5y18ke/rppyVbvHixZF/72tckW7FihWTDrbnKFQ0AQRRSAAiikAJAEIUUAIK6bTY5aZtNrjnkmixussJtVLdr1y57Pps2bZJs8+bNkjU1NUnW1dUlmZvImjJlimRu+T/3t3FLEbrjutelnYDK9OB+uD3Qh//uuCaSuwZ+97vfSbZy5UrJFi1aJNm73/1uyT70oQ9Jds4550g2EpaN5I4UAIIopAAQRCEFgCAKKQAEddtscs0K9+DY7c/kJpbcsnVueTvXWHrllVfsObqH6m6yaffu3ZKl3YvJnWNjY6NkrlHV2dkpmWvOub9h2iZepqkv99+ABlR2a2lpkez666+X7O9//7tkbrlLt+zd7bffLpmb5Mu0tOVIxB0pAARRSAEgiEIKAEEUUgAI6vFkk5tOcg0M11hqb2+XrL6+XrK1a9dK9uqrr9rzWbNmjWQ1NTWSuWkpN51UWFgoWXNzs2Rbt26VzE1PuWX03D5OpaWlkhUVFUnm9o8qLi6WLEliE2cYmrZt2yaZuzbcf2e3Z9o999wjmWuauqUt3VJ97rgjocHJFQQAQRRSAAiikAJAEIUUAIJ63Gxy3J4xLnNTPm1tbZK5ps2+ffvssWtrayXr6OiQzDWR3MN314ByD8tdA8pNnbi/g5s4cg0ody5uwqSyslKyJPG/n2sk0GzKHkuWLJHsT3/6k2R33323ZC+88IJkF154Yarjuv2ZLr74Ysmuu+46yVzTdLjhCgKAIAopAARRSAEgiEIKAEGjXDPkTbr94X9fZD7DTT20trZKVl1dLdnLL78sWabJJreMnmvmuH2XXOYmONJObqWd5nLL8rk9m9xkkmsgLV26VLIkSZJly5ZJNm3aNMncEn7BaZRsGGVJ9d0eatLuAeaauK4J+89//lOyZ599VjLXqHLX+MKFCyX73Oc+J9nll18uWZbs7WS/29yRAkAQhRQAgiikABBEIQWAoH5rNrnGy8GDByWrq6uTzC1R55YPSxI/BeWaTa6J5PawcY0Xxz1od40At7eT+1127twpmfsbumbTihUr7Dmef/75ks2bN08yt9Rf8ME/zaYs4ZaXbGhokMxNEK5atUqyBx54QDI3Tffxj39cMjcV5a7RQUazCQD6A4UUAIIopAAQRCEFgKA+WUbPTcG4LCcnRzK3B9H8+fMlKy8vt8c+cuSIZG5ZONeAcg2VtJnjJpa2b98umWu6uYf5rhHgplNcliR+2UL39zpBwxHDmFu+0WVu+Ua3pJ+7Th955BHJfvCDH0jmGsLXXnutZEky9PaB4o4UAIIopAAQRCEFgCAKKQAE9clkk31jymkn1/xwy4K5qaEkSb/fkHs4nbaJ5I7tMjdltWHDBsnckmTr1q2TzDWLZs+eLdlpp50mWZIkyRlnnCFZVVWVZG7yhMkm9Ib7zq5cuVKyz3zmM5K5ZTbddZEkfjnIAcJkEwD0BwopAARRSAEgiEIKAEF9MtnkpG3uuGaRm4DK1BRLO1WVdnrHNcTc5JB7ML5nzx7J6uvrUx2jtLRUMrekn2s2uamTJEmSoqIiycaNGyfZUJsSQfZy39nzzjtPsssuu0yye++9V7LHH3/cHsctuTeYuCMFgCAKKQAEUUgBIIhCCgBB/dZsSivSGMr0fsdNIrkJKrfE3e7duyWrrq6WzO275JbCc0v6TZ48WbLCwkLJ5syZI5nbxylJfLPJHZtm0+By031J4huxaSf5hhJ3nV1wwQWSuWaT279tKMq+/yoAMMRQSAEgiEIKAEEUUgAI6pNl9NI2h9xDdTflk3bZuiRJ//C9ra1NMjd15BpGNTU1kjU2NkrmlhBLu1dUbm6uZG5iaeHChZJVVFRIliR+7x032dQPDYxs6F4NyjJ67lr5xz/+YV/r9i6bMWNGn59TX3LX+KOPPiqZayy5JSfddZYkvpE6QFhGDwD6A4UUAIIopAAQRCEFgCAKKQAEdTsimrZ77jLXjXdd7fb2dskOHTqU6r1JkiSHDx9O9f6GhgbJtm3bJpnr0DturNJ16PPy8iQrKSmRzHXeXdfWjYO6zeuSxP/rAMZBB9cvfvELyb797W/b1z7yyCOS9XXX3q2r29LSIpnb2PG5556T7Pnnn5fMjXm6f0Vz5513Sub+5clQxB0pAARRSAEgiEIKAEEUUgAI6rbZ5Bo8bm3BtE0kl7kRMNfwcQ/Ak8RvTOeaTW50zb0u7birayy5Mc9p06ZJ5sY83Tio2xDPHcOdC4Ym17Spra21r73rrrskO/300yV7+9vfLtlLL72U6nzcho3umnTNIfde1/i86qqrJDvnnHMkW7FihWTZsv5qdpwlAAxhFFIACKKQAkAQhRQAgrpdj7S6ulp+6Jo7rtnkJo5cs8lNHLlJCPeQPpPx48dL5qZ8XAPKnbdbw9NNJ7mpkwULFkg2c+ZMydz6ijk5OZJlyWRSNpzkoKxHumnTJsl+9atf2de6dUrd99M1IFevXp3qfNz1vGjRIslck+uUU06RbOnSpZLNnTtXMrfZo/u+D0GsRwoA/YFCCgBBFFIACKKQAkBQt82m3//+9/JDt2SeW+rKNXzcVFRzc7NkbtIj02STOx/XWEq7CZ3L3ISRW+KusrJSsrKyMsnSTidlSWPJyYYTH5Rmk5Npg7eXX35Zsrq6OsnWrFnTp+fjJqWWL18umWuuumUjhxmaTQDQHyikABBEIQWAIAopAAR122y666675IcTJkyQ17lJHddkSTtd5KaYMk02ufN3S2+56aQpU6ZI5s7bTWG4BlR+fn6q444ANJswXNFsAoD+QCEFgCAKKQAEUUgBIKjbzX72798vmWueuAkcN73jGlVu6aypU6dK5pbq68mx3cSFOx+XDaVpDddcyzQB5V7bk/enkcXTV0Cf4Y4UAIIopAAQRCEFgCAKKQAEdTvZBAA4Me5IASCIQgoAQRRSAAiikAJAEIUUAIIopAAQRCEFgCAKKQAEUUgBIIhCCgBBFFIACKKQAkAQhRQAgiikABBEIQWAIAopAARRSAEgiEIKAEEUUgAIopACQBCFFACCKKQAEEQhBYAgCikABFFIASCIQgoAQRRSAAiikAJA0NgT/Pz4gJwFhptRg30CKfDdRm/Y7zZ3pAAQRCEFgCAKKQAEnegZaa8dP66PoI4cOZIqO3z4sGSdnZ32OJnytxo3bpxkubm5kuXk5KR679ix+qcbNSobHg0C6GvckQJAEIUUAIIopAAQRCEFgKA+aTa5xlJXV5dkhw4dkqylpUWyxsZGyfbu3WuP7d7vTJw4UbKSkhLJSktLU72uoKBAMteock2p0aP5/xd6xzVX77vvPslqa2tTfZ67ds877zzJLrnkEslcs3ak4ooGgCAKKQAEUUgBIIhCCgBB/TbZdOzYMcna29sl27dvn2Tbt2+XbNu2bfY4rjHlpqVcI6iwsFCysrIyyaqqqiSbOXOmZK5R5Zpc48ePl2zMmDGSAW/11FNPSfa9731Psj179vT6GC+99JJkrql76qmnSrZ06VLJXMN1uOGOFACCKKQAEEQhBYAgCikABPXbZJPLXAPKTUC56Y2Ojg57bDct5V7rjt3Q0CCZmwipq6uT7KSTTpJswYIFkpWXl0tWXFwsGc0mpPHyyy9LNmPGjFTZlClTJNu6datk//nPfyS77bbbJFuxYoVkt99+u2SuATXclpzkjhQAgiikABBEIQWAIAopAARlzchBTx5Ou2ZTc3OzZE1NTZK5/aImTZokmWuIuWXF3HJ7LgPSuOaaayQ744wzUr133rx5kv3rX/+S7IknnpDshRdekMxNWR08eFCyZ599VrLh1lzljhQAgiikABBEIQWAIAopAASNchNIb9LtD//7IvMZrhlz4MABydI2gTLt2eSWC9uxY4dkmzdvlmz16tWS1dfXS+aWwnP72rjsnHPOkWzWrFmSTZgwQbIsnv7IhhNP9d0e7ty1m3aSzzWWXMPVLcE3bty4lGc45NjvNnekABBEIQWAIAopAARRSAEgqN8mm9w+La5p4/ZScpM/bj+kJEmSoqIiyVyja/fu3ZLl5eVJ5s7bPRh3mftdhtsEB4YXd61ceeWVkrlGsdvf7Mtf/rJkI+Ea4I4UAIIopAAQRCEFgCAKKQAEUUgBIGhA1yNNu0ne0aNHJXOb3CVJkrS1tUnW2toqmetOuq79zJkzJausrJRs7ty5ks2ePVsy9y8V3L8MyOJxUGQJt07vJz/5ScncBnuO2/zu85//vGSjRw//+7Xh/xsCQD+jkAJAEIUUAIIopAAQ1ONmk2sOdXV1SeYebLvG0L59+yRza4xu27bNns/WrVsle+ONNyTbtGmTZO3t7ZKVlJRIVlxcLJlrSrlN8ty4q2s2Ab3lvsdu/d7HHntMsj/84Q+SuWv3Yx/7mGS33XabZIWFhRnPczjjjhQAgiikABBEIQWAIAopAAT1uOtx7NgxyY4cOSKZmy7atWuXZBs3bpRsy5YtkrkN7ZLEN5vcZ7qmluMmjNzv5zL3t4lkI2EiBJm5abyVK1dK9tprr0m2fv16yZ5++mnJXGPJ+dSnPiXZokWLUr13JOBKBYAgCikABFFIASCIQgoAQX0yYuOWvXPTFo2NjZLV1dWlyty0U5IkycGDByVzG9O55ezSvs41ltzvUl9fL5lbqs9xr3Ob6dGAGjlWr14t2S233CKZm9praWnp03NxjWJ3fieddJJk48eP79NzGYq4KgEgiEIKAEEUUgAIopACQFC/reeWdn8mN0nkHk6XlZXZ47hl72bNmiXZ4cOH7fvfKjc3N1XW1NQk2YYNGyRzSwy6/aemTJkimVuSzDWgxowZI1mSsA9Utnv00Uclc/spueulqqoq1euam5sla2hokOy6666TzO1b9rOf/UyyJUuWSDZhwgTJshl3pAAQRCEFgCAKKQAEUUgBIGiUawC9ifzQTTG55onbM8ZNLO3cuTPVe13TJkn8dJJbks4tSeamr9zv4iab3N8hbZPMNcPcg3u3L5TbPyrT5EimJtQAyIYuV7df/KHgL3/5i2RXXXWVZPPmzZPsxhtvlKy8vFyy5557TrKHH35YMtdcddeFa+r+6le/kuwjH/mIZFmyl5n9bnNHCgBBFFIACKKQAkAQhRQAgnrcbHKNHPeA2TVy2traJHPL4KWdQkoS32xyv5NrIh04cEAyN9Xhlsdze0i5KRHHNaAWLlwo2fLlyyWrrKyUrKCgwB7H/W0GCM2mPuCaOdXV1ZK575ObinOTbq5pWltbK9kLL7wgmZuy+u1vfyuZ89hjj0l2wQUXpHrvIKPZBAD9gUIKAEEUUgAIopACQFCPm03u9a4BlXYayL23J9LuYeTOx007uQaU2y/qxRdflGzt2rWp3uuW5Zs/f75kF154oWRve9vbJMu0xOAg7pVDs6mfuAk/N8EW2dvLHcM1gF3m9nG66KKLJHOTfHfccYdkbgJqkNFsAoD+QCEFgCAKKQAEUUgBIKjbdatccyjtXkzuYbfL3LRF2tdlytOeo2v65OXlpXrvpEmTJHMP/ffv358qcw2jlpYWyVzTDCPHQEyruWO4zO27dNZZZ0nmrouNGzdK9swzz0j24Q9/2J7jUNuPjDtSAAiikAJAEIUUAIIopAAQ1G2zyU3+pJ1iSjux5B5iu4mc6H4u7nzS7j/V2toqWWNjo2Ru6UDXqMrPz5ds4sSJqTL3dxhqD96BE3HXxSDuMRbGHSkABFFIASCIQgoAQRRSAAjqtoPjlsnq6OiQzO3F5Bov7vNcQ6W0tFQyN0WRJL754h5ku2O7ySG37N3rr78umZvMcHs2ufOuqKiQbM6cOZJNmzZNMteoiiyZhsGXaVot2mDtb+46W7NmjWRuWT73nV26dKlk2dJI5QoEgCAKKQAEUUgBIIhCCgBB3T7Ndg+J3bTTvn37JNu1a5dke/fulcxNNpWXl0vmGlBJ4qeg3ENwtxdTXV2dZNXV1ZLt2LEj1XvdZMb06dMlW7RokWSnnnqqZFOmTJHM/b40m7KH+25u2LDBvnb27NmSuebsQHATf665+qMf/UgyN0F40kknSXbeeef18uwGH1cgAARRSAEgiEIKAEEUUgAI6vHohHtY7ppSDQ0Nkm3fvl0y17zaunWrZJkmm1zzxS2Z5/ZJcpNNLnONKjd1MnXqVMkWLlwomZvgmDVrlmRFRUWSueZctkx/wH83zz33XPvau+++W7Krr75asr7+Trjm0D/+8Q/J7rvvPsmeffZZydy1cuedd0q2ZMmStKc45HBHCgBBFFIACKKQAkAQhRQAgrptNrmH2Gn3WMrJyZHMPWh3S/A1NTVJ5ppcSeKnetxx3DJ6rknmppPcNImbOpo/f75k7gG6ayy5ya3c3NxU50ezKbu5ayBJfEPGLfN4/fXXS1ZWVtbr83FL4d18882SZZrIeqv3vOc9kl100UU9P7EhjDtSAAiikAJAEIUUAIIopAAQRCEFgKBRmbrhSZIkhw4dkh+67rdbj9St4blu3TrJamtrJWttbZXMbaaXievaO+5fIBQWFkrmRj/dZnXz5s2TzK2t6o4xzDr02XCSmb/4/XlQc73dcMMN9rX333+/ZJMnT5Zs7ty5krkxZDdy/Oqrr0pWU1Mj2aZNm+w5vtWFF14o2YMPPiiZu6ayZF1d+93OijMHgKGMQgoAQRRSAAiikAJAULfNpqNHj8oP3VqFHR0dkrkxz927d0vmmlJu47zGxkZ7jq755biR1ZKSEslmzJghWWVlZarXuc/Ly8uTzDW5XGMpi9Fs6oFt27bZ/PHHH5fsG9/4hmRuvdzi4mLJXEOzvr5eMlcT3CaObm3UK6+8UrIFCxZIliVNU4dmEwD0BwopAARRSAEgiEIKAEHdNpsS80Devd5NErlN7dx0ktuUrrm5WTI37ZQkvtnkztE9aC8oKJDMNYzcJFJ+fr5krqGVxdNJEdnwCw6ZZpNr4CaJ34jxrrvukmzt2rWSuYbt+vXrJVu0aJFkFRUVkn3sYx+TzK0p6ppSwwzNJgDoDxRSAAiikAJAEIUUAIJ63GxKyzWg3EN1l7lN6VyW6f3O2LG6z5/L3NSRe90IbSKllQ1/iCHTbOqJ7du3p8rcUnivvPKKZKeddppkbnPGs846SzLXwB0BaDYBQH+gkAJAEIUUAIIopAAQ1G/NJoxoNJswXNFsAoD+QCEFgCAKKQAEUUgBIIhCCgBBFFIACKKQAkAQhRQAgiikABCk68P9/7JhQgXoDb7b6DPckQJAEIUUAIIopAAQRCEFgCAKKQAEUUgBIIhCCgBBFFIACKKQAkAQhRQAgiikABBEIQWAIAopAARRSAEgiEIKAEEUUgAIopACQBCFFACCKKQAEEQhBYAgCikABFFIASCIQgoAQRRSAAiikAJAEIUUAIIopAAQRCEFgKCxJ/j58QE5Cww3owb7BFLgu43esN9t7kgBIIhCCgBBFFIACKKQAkAQhRQAgiikABBEIQWAIAopAARRSAEgiEIKAEEUUgAIopACQBCFFACCKKQAEHSiZfSy0vHjukKay44dOybZkSNHUmVHjx5NlY0erf+vGjtW/+xpszFjxkg2alQ2rFoHDF/ckQJAEIUUAIIopAAQRCEFgKABbTalbfi4po17XZKkbw51dXVJdvjwYckOHTqUKuvo6Eh1DNcwKigokKyoqEiyiRMnSpabmyvZuHHjJEsSmlDonYMHD0rmrt3a2lrJ9u7dm+oYkyZNkmzx4sWp3jsUcUcKAEEUUgAIopACQBCFFACCum02ueZJZGrIfV5nZ6dkrpHjXpckSdLe3i6Zaw6597v3trS0SLZ//37J3AN517zKycmRbNq0aZJVVVWlel1ZWZlkbnoqSXyjC8OP+x67a81dkwcOHJDstddeS/XeH//4x5I999xzGc/zzf7nf/5Hsscee0yyqVOnpvq8wcYdKQAEUUgBIIhCCgBBFFIACOq2G7Fnz55UH+ImidJODbmH3a6509raao/d1taW6v3u2K5hlPZ80ja08vLyJKuoqJDMNQdco2rChAmSjR8/XjIMPtegcY3ZTNJO+D3//POSrVq1SjJ3rbgGj7sGHHcNpJ2mW7lypWSf/vSnJXvmmWdSfd5g444UAIIopAAQRCEFgCAKKQAEddtsWrdunWTuYblrLLkH1q5hlLZZlOkBeNoJI9fMcRMh+/btS5W5CSh3Lm7JvGXLlknmltGrrKyULO20GQZfdXW1ZK5Bk8kTTzwhmWvSvPTSS5Kl/Z5kmhjsb+78GhsbB+FM+gZ3pAAQRCEFgCAKKQAEUUgBIIhCCgBB3Xbt3bqEjuvAuW582k3kXCcxU3cxbdfRdfLdvyJoamqSzHXo3evcMdzGdGnXiqQbn91uv/12ydauXZv6/W4jOXdduX99ko1mz5492KfQa9yRAkAQhRQAgiikABBEIQWAoG6bTe7BtuPWI3Vjnq4Z49YvdOtrZlpz0zVz3DqerinV3Nws2ZgxYyRzDTG34Vxubq5kxcXFkk2ePFmySZMmSebGRt3fIe0akBhY9fX1kq1Zs6bPj+PWqC0sLEz1Xre53GWXXSaZa7h+97vfTXUMZ9asWZLdfPPNvf68wcYdKQAEUUgBIIhCCgBBFFIACOq22TRx4kTJ0jY20jZF3OZwPWk2udw1oNzD8pqaGsnchJFrSrnjut9lzpw5krl1RquqqiQrLS1NdVzX+MLgu/rqqyXbuHGjZJkmk9773vemOo6bCFqyZEmq95aUlEi2cOFCye67775Un5fWBz/4QclOPfXUPj3GQOIKBIAgCikABFFIASCIQgoAQd02m5YuXSqZaxilnQYaO1YP56YyXEPFTStl+kw3abVnzx7J3KRVQ0NDqvNx5+2mRFwTaf78+ZLNnDlTMjcV5f4ONJuGpve///2Sue9hpg3xXEPGcZNyZWVlqd7ruOaX24gvLdc0veKKKyTLdI1nA65AAAiikAJAEIUUAIIopAAQ1G2zKe10hGt2HD16VDLXlHIPmF2WaaLKTSK5Jfxc5vZEck2ygwcPSuaWzHMNI5e5BpRbRs81udzfkGX0hqb8/HzJbrjhhoE/kR5yDbHq6upef975558v2dve9rZef95QxB0pAARRSAEgiEIKAEEUUgAI6rbZNGXKFMn6urHhGlXuGK6plCS+OdTV1SVZU1OTZG5Pnbq6Oslcs8lNa7i/l2s2uUkUt2ShWw6QKSb0JdcUjuzF5Hz84x+XzDXishlXJQAEUUgBIIhCCgBBFFIACOq22ZRpn6T+5iaO3NJ4SeKX/HKTGVu2bJFs586dku3atUsy90B+2rRpkk2fPl0ylsfDULZq1SrJfvvb3/bpMc4+++w+/byhiCsVAIIopAAQRCEFgCAKKQAEddtsGixuisntr5QkSdLc3CyZW/Jr27ZtkrkGVFtbm2QlJSWSlZeXSzZjxgzJ3PJ4br8nlsfDYPj1r38tWWtra68/z10XbsnJ4YY7UgAIopACQBCFFACCKKQAEDTozaa0U0xuKbskSZK9e/dK5ppNbmJp3759krml6yoqKiRzjaXKykrJXKOKKSYMhpaWFsn+/ve/S5Zpyco0PvrRj0qWl5fX68/LFly9ABBEIQWAIAopAARRSAEgaNCbTWmnmNyD8iRJkpqaGslcY8llrqnlJpHmzJmTKnPL6BUWFko2dqz+2ZliQn974YUXJHON2bTcxNI111wjmfu+DzfckQJAEIUUAIIopAAQRCEFgKBBfwqcdoqpoaHBvt/tu1RbWytZU1OTZG5PqqqqKslmz54t2bx58yRzjSr3QH4kPHzH4Dp06JBkv/nNbyRze56lde6550q2YMGCXn9eNuOOFACCKKQAEEQhBYAgCikABA1o18NNMXV1dUnm9oxxE0xJ4htL7rXu4fvkyZMlcw0jtw9NaWmpZOzFhKHCTfI9//zzvf48933/0pe+JNlIbaRyRwoAQRRSAAiikAJAEIUUAIIG9MmwaywdOHBAMrcP044dO+xnusmm+vp6yTo6OtKcYmjvJLf/VNqMBhT60tatWyVz031pXXrppZJddNFFvf684YY7UgAIopACQBCFFACCKKQAEDSgzSa3ZN7+/fslq6urk2z37t32M7dv3y6Z29/JTRi5zDWb0r6OhhGGittuu61PP++CCy7o088bbrgjBYAgCikABFFIASCIQgoAQRRSAAgakl17t9FdY2Oj/Uw3TurGQYuKiiTLz8+XLC8vTzK3gd24ceMko2uPwbBx40bJMq3fm0ZhYaFky5cv7/XnjQTckQJAEIUUAIIopAAQRCEFgKABbTYdPXpUssOHD0vmmkVuLdNMn+kaRiUlJZJNmTJFsuLiYslco2qkbvKFoeerX/2qZG5MOi23UeRPfvITyebNmyfZJZdcItmsWbN6fS7ZgjtSAAiikAJAEIUUAIIopAAQNKAdE7eup5sumjFjhmRz5syxn+kaU26CavLkyZJVVlamOs6kSZMkc9NO7vdj2gn97amnnpIsU3M2DffeJ598UrJrr71WsvHjx/f6uNmMO1IACKKQAkAQhRQAgiikABA0oM0mNw3kJomqqqokO378uP1Mt+SXe21ZWZlkbuLCNbqmTp0qmZt2ckvr0YBCtnENo0svvVSyK664QjJ3rYwE3JECQBCFFACCKKQAEEQhBYCgUZmaOP+r2x/2VGdnp2Ruya7W1lbJMi0L5l7rFBQUSJZ2yTy3LJ97ID96NP9f+l/Z0E3r0+/2YJo+fbpke/bsSfVe9z1etmyZZE8//bRk7voZAex3mysfAIIopAAQRCEFgCAKKQAEDehkk5vocQ0aNyGUaXkut5yd+8wJEyZI5ppIOTk5krmJLKaTMFS4fZIeeuihVO91k0i33nqrZCO0sZQad6QAEEQhBYAgCikABFFIASDoRJNNAIAT4I4UAIIopAAQRCEFgCAKKQAEUUgBIIhCCgBBFFIACKKQAkAQhRQAgiikABBEIQWAIAopAARRSAEgiEIKAEEUUgAIopACQBCFFACCKKQAEEQhBYAgCikABFFIASCIQgoAQRRSAAiikAJAEIUUAIIopAAQRCEFgKCxJ/j58QE5Cww3owb7BFLgu43esN9t7kgBIIhCCgBBFFIACKKQAkAQhRQAgiikABBEIQWAIAopAARRSAEgiEIKAEEUUgAIopACQBCFFACCKKQAEHSiZfSGjePHddW0Y8eOSXbkyBHJurq6JDt8+HCqzxszZoxk48aNk2zsWP1PkZOTI9no0fy/DxhquCoBIIhCCgBBFFIACKKQAkBQ1jebXBPp6NGjkrmG0aFDhyRraWmRrKmpKdXrXKOqoKBAsrKyMslKS0slKywslIwGFAZDR0eHZHV1daky14RdtmyZZBMnTuzl2Q0+rkAACKKQAkAQhRQAgiikABA0JJtNaaeQksQ3kdrb2yVzDaPdu3dLtm3bNsl27NghWUNDQ6pznDp1qmQLFy6UbNGiRZK5aSc3KTVq1CjJusuB/8s1Zvfv3y/Z6tWrJXvooYcke/zxxyVzjarnn39esne+852SZUsjNTvOEgCGMAopAARRSAEgiEIKAEGD3mxyDRrXQOrs7LTvb25ulsxNV2zcuFGyzZs3S+YaSzU1NZK5RpVrBC1evFgy9zsXFRVJ5qadJkyYIBmQhmssuevnrrvukuyHP/xhqs9L695775Xs3HPPlSw3N7fXxxhI3JECQBCFFACCKKQAEEQhBYCgAW02uYfTbu8jN1lRX19vP9M1jNasWSOZm1javn27ZK5R1draKtm+ffskcxNZrnHmlsebNm2aZHPmzEl1DCCNtWvXSnbjjTdKtnLlSskyTRb2ljvGgQMHJKPZBAAjBIUUAIIopAAQRCEFgCAKKQAE9UnXPu0GdG7M060TWltbK9nrr79uj/3GG29ItmXLllSf6bqE7ndxXXa30Z37/caNGydZ2o343L9ooGuPt3Id9Z///OeSPfjgg5K5Tr77LrpNHBcsWCDZTTfdJNnNN98s2QUXXCBZcXGxZNmCO1IACKKQAkAQhRQAgiikABDU42ZT2saSe2Dtxiqrq6sle+WVVyRbv369PZ+dO3emOo5r+rgRzJKSEsncBly7du2SzG2IN3nyZMnGjx8vWXSjO4wMrrH0xz/+UbLvf//7krk1ed21W1VVJdmtt94q2VlnnSVZZWWlZF/5ylckcxtUumsgW3BHCgBBFFIACKKQAkAQhRQAgvqk2eQmdVpaWiRzjaVVq1ZJ5qYt3HuTxD8sd2t7zpgxI1XmNqFzE0YTJ05M9V63nqKbEnGZe69rNtGAGp5cY+mvf/2rZPfcc49kbuLPNVff9773SfbOd75TsvPOO08y9313x3Ub7LkmbDbjjhQAgiikABBEIQWAIAopAAR122xKO8XkphTcJnKvvvqqZOvWrZPMNZbccZPET2HMnz9fskWLFknmmk2uceOW4HPTUwcPHpTMTSdNmDBBMjdR5Sag3JQVhie36aKbEnINW3cNXHPNNZJ94hOfkKy8vDzV+bn68PDDD0vmmk0LFy5MdYxswVUJAEEUUgAIopACQBCFFACCejzZ5Jo+bW1tkrmG0Y4dOyRzS885c+fOtfny5cslW7FihWTuAbpbtsvtIeWWBHSNAPc6NwHlskmTJkmWn58vGc2mkeNvf/ubZK6xlJOTI9m1114r2Wc/+1nJ3HRSWu4aePLJJyVz5+f2QctmXJUAEEQhBYAgCikABFFIASCox5NNbmmvzs5OydyUj5sachM9rjF08skn23M855xzJJs1a5ZkbkrINcncFMa2bdskc3s2uWZTaWmpZO53dq9z+0yxZN7w466zJEmS+++/XzL3399NCX3hC1+QzF0Dablz/PnPfy7Zpk2bJFuwYIFky5Yt6/W5DEXckQJAEIUUAIIopAAQRCEFgKAeTza5h91u2sY1VCorKyVzEz2u8ZLp4bRrLLll6lxDzE1Vvfbaa5Jt3rxZsvr6esncPjQuc1Na7u9Fs2lkcA3OJEmS//znP5K5//4PPPCAZG6aKOLAgQOS/frXv0713uuuu06yxYsXh89pKOGOFACCKKQAEEQhBYAgCikABPVJsykvL08ytx9ScXGxZG5Syi2xVVFRYc/HNZbcUn+NjY2Suf2iXLZ9+3Z77LeaOnWqZK6x5H6XgoICydwyfzSbhp/c3Fybu+vqAx/4gGSuEdvX3xO3F9Mbb7yR6r3u93B7mWUz7kgBIIhCCgBBFFIACKKQAkBQt82mtFNMrlEyffp0yVxjyXHLfbmmUqbPdPsubdiwQTI3xeQaS255vKqqKslcY2nRokWSsWQe3izT8na33nqrZO9617sky9Ss6i03afXggw9K5q6LpUuXSvbud7+7T85rKOOOFACCKKQAEEQhBYAgCikABPW42eSmbdzkQl8v45WpUdXS0iLZ1q1bJVu1apVkbjLDLa3nJrJcs8ntK+UmvCZOnCgZU0wjV6YpnyuvvFIy913saw899JBkaaeYPvrRj0rmGs/DDXekABBEIQWAIAopAARRSAEgqMfL6LnJJtdYOn78eKrPc6/r6uqSrK2tzb6/pqZGspUrV0r2+uuvS7Zjxw7JXONszpw5kp1yyimSLViwQDK3J5WbZHF/V4xs7rsT4a61nTt3Sva9731PMrc05cUXXyzZ5ZdfLtlwWzLP4eoFgCAKKQAEUUgBIIhCCgBBPW42OW4Cx2XuYfeRI0cka29vl6yurs4e++WXX5bMLY+3adOmVOeYtrF06qmnSub2bHJLnI2Eh+8Yeg4ePCjZnXfeKdnu3bslKyoqkuyb3/ymZG7ibyTgjhQAgiikABBEIQWAIAopAAT1SbMpLTcd0dHRIVl9fb1kq1evtp/pmk1r166VzO0vM3/+fMncHksrVqyQzC2P5/aVYi8mDAbX2H3qqackc0vmOW6vqCVLlkg2Ur/b3JECQBCFFACCKKQAEEQhBYCgfms2pV0er7m5WbItW7ZI5pbBS5IkWb9+vWRugqO8vFyyxYsXS3b66adLVllZKVlhYaFk7LuEocJNB37/+9/v9ed98YtflMx930cq7kgBIIhCCgBBFFIACKKQAkAQhRQAgga0a9/Z2SnZ3r17Jdu8ebNkbo3EJPEd+mnTpkm2bNkyyc4880zJ3IhoaWmpZG7DPzaww2A4duyYZHfffbdk69atS/V5d9xxh2TLly/v+YmNIFz5ABBEIQWAIAopAARRSAEgaEBnvFyzqa2tTbL9+/dLlmnUcvLkyZItWLBAMvew/OSTT5bMbWCXl5cnmdvAjnFQDAY3Jv3LX/5SMrcmr9uc8VOf+pRk7hrA/8MdKQAEUUgBIIhCCgBBFFIACBrQZpNrxriH3a7h4zbOS5IkqaiokMxNJ5122mmSTZ8+XTIaSxjKjhw5ItlPf/pTyWpqaiRzGzFef/31krnrAt3jjhQAgiikABBEIQWAIAopAASNcsvdvUm3P+yOW9qrpaVFsrq6Osnc0noHDhywx3FNn1mzZknmHqAXFBRI5pbHo7HUY9nwB+v1d3swuWUj586dK1ljY6NkV1xxhWTf+c53JHPLUOK/7HebO1IACKKQAkAQhRQAgiikABA0oM0mt4yXWzLPvS7Tebp9koqKiiTLz8+XjMZSv8mGP2JWNptaW1slKy8vl8w1jP72t79J5hqz6BbNJgDoDxRSAAiikAJAEIUUAIL6bRk917RxjSHX8OmJtJ/JUngYDtwyj26y6fLLL5eMxlL/4Y4UAIIopAAQRCEFgCAKKQAEnWiyCQBwAtyRAkAQhRQAgiikABBEIQWAIAopAARRSAEg6P8AC/97lLktYd0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x864 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's build a reasonable composite augmentation: initially copy pasted from aug_pipe_exploration\n",
    "\n",
    "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
    "model = create_barlow_twins_model(fastai_encoder, hidden_size=1024,projection_size=10)# projection_size=1024)\n",
    "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
    "#values for these which is tantamount to doing nothing\n",
    "\n",
    "#So if we choose resize_scale=(1,1) then the images look the same.\n",
    "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
    "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=False)\n",
    "\n",
    "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
    "b = dls.one_batch()\n",
    "learn._split(b)\n",
    "learn('before_batch')\n",
    "axes = learn.barlow_twins.show(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8250ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.recorder.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d893cfa3",
   "metadata": {},
   "source": [
    "So, in the above we train Barlow Twins on `MNIST_TINY`. \n",
    "\n",
    "Next step: Figure out what exactly we are training on. Then we want to \"fine tune\" on validation set and examine \n",
    "performance. \n",
    "\n",
    "Ok, for now let's just train as usual. We WILL improve this later. Iterate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50e24aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eaba7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, train a classifier on the embedding!\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,zdim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(zdim,2) #As only 2 classes in TINY_MNIST\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = cast(self.fc1(x),Tensor) #so we have to use cross entropy loss. cast is because using old version fastai \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53c5de70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataLoaders.__getitem__ of <fastai.data.core.DataLoaders object at 0x7fea38bb1270>>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=128\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "items = get_image_files(path)\n",
    "tds = Datasets(items, [PILImageBW.create, [parent_label, Categorize()]], splits=GrandparentSplitter()(items))\n",
    "dls = tds.dataloaders(bs=bs,num_workers=0, after_item=[ToTensor(), IntToFloatTensor()], device='cpu')\n",
    "\n",
    "# print(len(list(iter(dls.valid))))\n",
    "# print(len(list(iter(dls.train))))\n",
    "\n",
    "dls.__getitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2f624468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signature (items=None, tfms=None, tls=None, n_inp=None, dl_type=None, use_list=None, do_setup=True, split_idx=None, train_setup=True, splits=None, types=None, verbose=False)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.signature(Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2b4b2428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
     ]
    }
   ],
   "source": [
    "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
    "model = create_barlow_twins_model(fastai_encoder, hidden_size=100,projection_size=10)# projection_size=1024)\n",
    "\n",
    "#Use aug pipeline from above\n",
    "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=False)\n",
    "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])#,ShortEpochCallback(0.001)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cec0c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid.bs = len(list(iter(dls.valid_ds))) #Set the validation dataloader batch size to be the length of the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "97bc22ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.481211</td>\n",
       "      <td>0.523312</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.550570</td>\n",
       "      <td>0.344377</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.722758</td>\n",
       "      <td>0.450311</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.305827</td>\n",
       "      <td>1.672014</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.084992</td>\n",
       "      <td>1.321896</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.908646</td>\n",
       "      <td>2.553658</td>\n",
       "      <td>00:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.778418</td>\n",
       "      <td>0.228959</td>\n",
       "      <td>00:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.681825</td>\n",
       "      <td>0.484004</td>\n",
       "      <td>00:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.611699</td>\n",
       "      <td>0.244738</td>\n",
       "      <td>00:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.552262</td>\n",
       "      <td>0.230420</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "learn.fit(10)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eb3bb43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorCategory(0.0854, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0670, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0690, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1045, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0913, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0987, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0759, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0940, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1206, grad_fn=<AliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Train Classifier on encoder(tiny_mnist)\n",
    "\n",
    "zdim=1024 #see above\n",
    "head = LinearClassifier(zdim=zdim)\n",
    "optimizer = torch.optim.Adam(head.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "EPOCHS=100\n",
    "for epoch in range(EPOCHS):\n",
    "    #break \n",
    "    b = dls.train.one_batch() #Seems need dls[0] or dls.train for training ... dls[1] is validation see here https://docs.fast.ai/data.core.html#DataLoaders.__getitem__\n",
    "    x,y = b[0],b[1]\n",
    "\n",
    "    loss = criterion(head(fastai_encoder(x)),y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch>90:\n",
    "        print(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "11306c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9756795167922974\n"
     ]
    }
   ],
   "source": [
    "#Test result of above cell on the validation set - assumes that batch size of valid-dataloader is = number of valid samples                                        \n",
    "b = dls.valid.one_batch()\n",
    "x,y = b[0],b[1]\n",
    "ypred = head(fastai_encoder(x))\n",
    "correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n",
    "print(correct.mean().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "79237ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorCategory(0.0934, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1077, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1053, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1171, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0996, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1304, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1345, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1613, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1340, grad_fn=<AliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Just train a linear classifier (no encoder)\n",
    "#Basically cell above but remove encoder and some re-shaping\n",
    "\n",
    "zdim=28*28 #see above\n",
    "head = LinearClassifier(zdim=zdim)\n",
    "optimizer = torch.optim.Adam(head.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS=100\n",
    "for epoch in range(EPOCHS):\n",
    "    #break\n",
    "    b = dls.train.one_batch() #see here https://docs.fast.ai/data.core.html#DataLoaders.__getitem__\n",
    "    x,y = b[0],b[1]\n",
    "\n",
    "    x=x.view(bs,zdim)\n",
    "    x=cast(x, Tensor) #Have to do this when using old version of fastai for some reason...\n",
    "\n",
    "    out = head(x)\n",
    "    loss = criterion(out,y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch>90:\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f7bd5db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9842632412910461\n"
     ]
    }
   ],
   "source": [
    "#Test result of above cell, (i.e. just a linear classifier), on the validation set - assumes that batch size of valid-dataloader is = number of valid samples                                        \n",
    "b = dls.valid.one_batch()\n",
    "x,y = b[0],b[1]\n",
    "x = x.view(-1,zdim)\n",
    "ypred = head(x)\n",
    "\n",
    "correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n",
    "print(correct.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f82a31d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
