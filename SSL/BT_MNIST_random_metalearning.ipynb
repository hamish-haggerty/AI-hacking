{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hamish-haggerty/AI-hacking/blob/master/SSL/BT_MNIST_new3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "X8jFsEXz_61O",
    "outputId": "5ab690f3-0404-43c3-96e6-b0a30743c8ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting torch==1.11.0\n",
      "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 750.6 MB 10 kB/s \n",
      "\u001b[?25hCollecting torchvision==0.12.0\n",
      "  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 78.8 MB/s \n",
      "\u001b[?25hCollecting torchaudio==0.11.0\n",
      "  Downloading torchaudio-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 60.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0) (4.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (1.21.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (7.1.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (2.23.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (2022.6.15)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1+cu113\n",
      "    Uninstalling torch-1.12.1+cu113:\n",
      "      Successfully uninstalled torch-1.12.1+cu113\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.13.1+cu113\n",
      "    Uninstalling torchvision-0.13.1+cu113:\n",
      "      Successfully uninstalled torchvision-0.13.1+cu113\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 0.12.1+cu113\n",
      "    Uninstalling torchaudio-0.12.1+cu113:\n",
      "      Successfully uninstalled torchaudio-0.12.1+cu113\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.11.0 torchaudio-0.11.0 torchvision-0.12.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "torch"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting fastai==2.6.3\n",
      "  Downloading fastai-2.6.3-py3-none-any.whl (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 14.0 MB/s \n",
      "\u001b[?25hInstalling collected packages: fastai\n",
      "  Attempting uninstall: fastai\n",
      "    Found existing installation: fastai 2.7.9\n",
      "    Uninstalling fastai-2.7.9:\n",
      "      Successfully uninstalled fastai-2.7.9\n",
      "Successfully installed fastai-2.6.3\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting self_supervised\n",
      "  Downloading self_supervised-1.0.4-py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 565 kB/s \n",
      "\u001b[?25hRequirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from self_supervised) (21.1.3)\n",
      "Collecting timm>=0.4.5\n",
      "  Downloading timm-0.6.11-py3-none-any.whl (548 kB)\n",
      "\u001b[K     |████████████████████████████████| 548 kB 22.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from self_supervised) (21.3)\n",
      "Requirement already satisfied: fastai>=2.2.7 in /usr/local/lib/python3.7/dist-packages (from self_supervised) (2.6.3)\n",
      "Collecting kornia>=0.5.0\n",
      "  Downloading kornia-0.6.7-py2.py3-none-any.whl (565 kB)\n",
      "\u001b[K     |████████████████████████████████| 565 kB 65.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (3.2.2)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (0.0.7)\n",
      "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (3.4.1)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (0.12.0)\n",
      "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (7.1.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.3.5)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (2.23.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.7.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (6.0)\n",
      "Collecting fastcore<1.5,>=1.3.27\n",
      "  Downloading fastcore-1.4.5-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 9.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch<1.12,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (57.4.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.6.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.1.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.4.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.64.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.9.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (8.1.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.11.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.6)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4->fastai>=2.2.7->self_supervised) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->self_supervised) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4->fastai>=2.2.7->self_supervised) (5.2.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (2022.6.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (1.24.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai>=2.2.7->self_supervised) (0.7.8)\n",
      "Collecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 69.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4->fastai>=2.2.7->self_supervised) (7.1.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm>=0.4.5->self_supervised) (3.8.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm>=0.4.5->self_supervised) (4.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4->fastai>=2.2.7->self_supervised) (2.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai>=2.2.7->self_supervised) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai>=2.2.7->self_supervised) (2022.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (1.1.0)\n",
      "Installing collected packages: fastcore, huggingface-hub, timm, kornia, self-supervised\n",
      "  Attempting uninstall: fastcore\n",
      "    Found existing installation: fastcore 1.5.25\n",
      "    Uninstalling fastcore-1.5.25:\n",
      "      Successfully uninstalled fastcore-1.5.25\n",
      "Successfully installed fastcore-1.4.5 huggingface-hub-0.10.0 kornia-0.6.7 self-supervised-1.0.4 timm-0.6.11\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (22.1.0)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest) (0.7.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.15.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (8.14.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest) (57.4.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.11.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ipytest\n",
      "  Downloading ipytest-0.12.0-py3-none-any.whl (15 kB)\n",
      "Collecting pytest>=5.4\n",
      "  Downloading pytest-7.1.3-py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 14.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (21.3)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.9.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (4.12.0)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (22.1.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (4.1.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
      "Collecting jedi>=0.10\n",
      "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 68.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (5.1.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (57.4.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.0.10)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipytest) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (0.2.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (1.15.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipytest) (3.0.9)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipytest) (0.7.0)\n",
      "Installing collected packages: pluggy, jedi, iniconfig, pytest, ipytest\n",
      "  Attempting uninstall: pluggy\n",
      "    Found existing installation: pluggy 0.7.1\n",
      "    Uninstalling pluggy-0.7.1:\n",
      "      Successfully uninstalled pluggy-0.7.1\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 3.6.4\n",
      "    Uninstalling pytest-3.6.4:\n",
      "      Successfully uninstalled pytest-3.6.4\n",
      "Successfully installed iniconfig-1.1.1 ipytest-0.12.0 jedi-0.18.1 pluggy-1.0.0 pytest-7.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0\n",
    "!pip install fastai==2.6.3 --no-deps\n",
    "!pip install self_supervised\n",
    "\n",
    "!pip install pytest\n",
    "!pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "BOv4kkJDag8r",
    "outputId": "49fee111-45ba-4c1c-898f-c4a940fae234"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "function ClickConnect(){\n",
       "console.log(\"Working\");\n",
       "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
       "}setInterval(ClickConnect,60000)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "function ClickConnect(){\n",
    "console.log(\"Working\");\n",
    "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
    "}setInterval(ClickConnect,60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Pk01WY_Dag8s"
   },
   "outputs": [],
   "source": [
    "import fastai\n",
    "import self_supervised\n",
    "import torch\n",
    "if torch.cuda.is_available():device='cuda'\n",
    "else:device='cpu'\n",
    "assert(fastai.__version__ == '2.6.3') #Check that version is 2.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AOjr_YCLag8t",
    "outputId": "7019d8a5-9001-4424-aa35-b9501f91d059"
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *\n",
    "from torchvision import transforms\n",
    "import inspect\n",
    "import warnings\n",
    "import random\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XTSdKC6bag8t"
   },
   "outputs": [],
   "source": [
    "#Like most other SSL algorithms BT's model consists of an encoder and projector (MLP) layer.\n",
    "#Definition is straightforward:\n",
    "#https://colab.research.google.com/github/KeremTurgutlu/self_supervised/blob/master/nbs/14%20-%20barlow_twins.ipynb#scrollTo=1M6QcUChcvpz\n",
    "class BarlowTwinsModel(Module):\n",
    "    \"\"\"An encoder followed by a projector\n",
    "    \"\"\"\n",
    "    def __init__(self,encoder,projector):self.encoder,self.projector = encoder,projector\n",
    "        \n",
    "    def forward(self,x): return self.projector(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZL3EE07Pag8u"
   },
   "outputs": [],
   "source": [
    "#HOWEVER instead of directly using the above, by passing both an encoder and a projector, create_barlow_twins_model\n",
    "#function can be used by minimally passing a predefined encoder and the expected input channels.\n",
    "\n",
    "#In the paper it's mentioned that MLP layer consists of 3 layers... following function will create a 3 layer\n",
    "#MLP projector with batchnorm and ReLU by default. Alternatively, you can change bn and nlayers. \n",
    "\n",
    "#Questions: Why torch.no_grad() when doing this?\n",
    "def create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n",
    "    \"Create Barlow Twins model\"\n",
    "    n_in  = in_channels(encoder)\n",
    "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
    "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n",
    "    apply_init(projector)\n",
    "    return BarlowTwinsModel(encoder, projector)\n",
    "\n",
    "#Similar to above. Simple API to make the BT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DFjGL-COag8v"
   },
   "outputs": [],
   "source": [
    "#BarlowTwins Callback\n",
    "#The following parameters can be passed:\n",
    "# - aug_pipelines\n",
    "# Imb lambda is the weight for redundancy reduction term in the loss function\n",
    "\n",
    "@delegates(get_multi_aug_pipelines)\n",
    "def get_barlow_twins_aug_pipelines(size,**kwargs): return get_multi_aug_pipelines(n=2,size=size,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xx4KsywAag8v"
   },
   "outputs": [],
   "source": [
    "#Uniform random number between a and b\n",
    "def Unif(a,b):\n",
    "    return (b-a)*torch.rand(1).item()+a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "zU4GwLruU5AD"
   },
   "outputs": [],
   "source": [
    "def random_sinusoid(x,std=0.1,seed=0):\n",
    "    device='cpu'\n",
    "    seed_everything(seed=seed)    \n",
    "    t=(std) * torch.randn(1,500).to(device)\n",
    "    s=(std) * torch.randn(1,500).to(device)\n",
    "    \n",
    "    u=torch.randn(1,500).to(device)\n",
    "    v=torch.randn(1,500).to(device)\n",
    "\n",
    "    a=(0.2) * torch.randn(1,500).to(device)\n",
    "    b=(0.2) * torch.randn(1,500).to(device)\n",
    "    # N = torch.abs(a) + torch.abs(b)\n",
    "    # a = a/N\n",
    "    # b = b/N\n",
    "\n",
    "    return a*torch.sin(t*x[:,]*math.pi+u) + b*torch.cos(s*x[:,]*math.pi+v)\n",
    "\n",
    "\n",
    "    \n",
    "    #return torch.sin(t*math.pi*x+u) + torch.cos(s*math.pi*x + v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "Xej8_KxJ71Sl"
   },
   "outputs": [],
   "source": [
    "#old version\n",
    "class Cdiff_Rand:\n",
    "    \n",
    "    def __init__(self,seed,bs,std=0.1,K=2):\n",
    "        self.seed=seed\n",
    "        self.std=std\n",
    "        self.K=2\n",
    "        self.bs=bs\n",
    "\n",
    "    def __call__(self,z1norm,z2norm):\n",
    "        \n",
    "        cdiff_rand=0\n",
    "        for i in range(self.K):\n",
    "            \n",
    "\n",
    "            z1norm_2,z2norm_2 = random_sinusoid(x=z1norm,t=t1.detach(),std=self.std,seed=self.seed+i), random_sinusoid(x=z2norm,t=t2.detach(),std=self.std,seed=2*self.seed+i)\n",
    "            cdiff_rand = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)\n",
    "\n",
    "        cdiff_rand=(1/self.K)*cdiff_rand\n",
    "    \n",
    "        return cdiff_rand,t1,t2\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new version\n",
    "class Cdiff_Rand:\n",
    "    \n",
    "    def __init__(self,I,inner_steps,seed,bs,std=0.1,K=2):\n",
    "        \n",
    "        self.I=I\n",
    "        self.inner_steps=inner_steps\n",
    "        self.seed=seed\n",
    "        self.std=std\n",
    "        self.K=2\n",
    "        self.bs=bs\n",
    "\n",
    "        \n",
    "    def inner_step(self,z1norm,z2norm):\n",
    "    \n",
    "        I=self.I\n",
    "        bs=self.bs\n",
    "        inner_steps=self.inner_steps\n",
    "\n",
    "        z1norm=z1norm.detach()\n",
    "        z2norm=z2norm.detach()\n",
    "\n",
    "\n",
    "        for i in range(inner_steps):\n",
    "            \n",
    "            t1=(0.1) * torch.randn(1,500)\n",
    "            t2=(0.1) * torch.randn(1,500)\n",
    "            \n",
    "            t1=t_gen(t1)\n",
    "            t2=t_gen(t2)\n",
    "            \n",
    "            \n",
    "            z1norm_2 = random_sinusoid(x=z1norm,t=t1)\n",
    "            z2norm_2 = random_sinusoid(x=z2norm,t=t2)\n",
    "            \n",
    "                    \n",
    "            assert (z1norm_2.shape,z2norm_2.shape) == (z1norm.shape,z2norm.shape)\n",
    "\n",
    "            # Ctem1 =  (z1norm.T @ z2norm_2) / bs\n",
    "            # Ctem2 = (z1norm_2.T @ z2norm) / bs\n",
    "            # cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2))\n",
    "\n",
    "            cdiff_2 = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)\n",
    "            #cdiff_2=Ctem.pow(2)\n",
    "\n",
    "            inner_loss=1*(cdiff_2*(1-I)).mean()\n",
    "\n",
    "            t_gen_optimizer.zero_grad()\n",
    "            inner_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "    \n",
    "    def __call__(self,z1norm,z2norm):\n",
    "        \n",
    "        cdiff_rand=0\n",
    "        for i in range(self.K):\n",
    "            \n",
    "            \n",
    "            t1=(0.1) * torch.randn(1,500)\n",
    "            t2=(0.1) * torch.randn(1,500)\n",
    "            \n",
    "            t1=t_gen(t1)\n",
    "            t2=t_gen(t2)\n",
    "\n",
    "            z1norm_2,z2norm_2 = random_sinusoid(x=z1norm,t=t1.detach(),std=self.std,seed=self.seed+i), random_sinusoid(x=z2norm,t=t2.detach(),std=self.std,seed=2*self.seed+i)\n",
    "            cdiff_rand = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)\n",
    "\n",
    "        cdiff_rand=(1/self.K)*cdiff_rand\n",
    "    \n",
    "        return cdiff_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "XsNBXcIjyjD2"
   },
   "outputs": [],
   "source": [
    "#New stuff\n",
    "def C_z1z2(z1norm,z1norm_2,z2norm,z2norm_2,bs):\n",
    "    \n",
    "    Ctem1 =  (z1norm.T @ z2norm_2) / bs\n",
    "    Ctem2 = (z1norm_2.T @ z2norm) / bs\n",
    "    cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2))\n",
    "\n",
    "    return cdiff_2\n",
    "\n",
    "class Max_Corr(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(ps,ps)\n",
    "        self.fc2 = nn.Linear(ps,ps)\n",
    "\n",
    "        self.fc3 = nn.Linear(ps,ps)\n",
    "        self.fc4 = nn.Linear(ps,ps)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x,y):\n",
    "\n",
    "        x=self.sigmoid(self.fc1(x)) #when (sigmoid,relu) GREAT results, with (sigmoid,sigmoid) TERRIBLE. Currently testing (relu,relu)\n",
    "        x=self.fc2(x)\n",
    "       \n",
    "        y=self.relu(self.fc3(y)) #originally had relu and got really good results. If we can't reproduce those results, possible reasons:\n",
    "                                    #results were due to chance; or having relu on one branch (and sigmoid on the other) helps via breaking\n",
    "                                      #the symmetry! Other idea: set fc1=fc3, fc2=fc4. \n",
    "        y=self.fc4(y)\n",
    "\n",
    "        return x,y\n",
    "\n",
    "class Cdiff_Sup:\n",
    "    \n",
    "    def __init__(self,I,inner_steps,bs):\n",
    "        \n",
    "        self.I=I\n",
    "        self.inner_steps=inner_steps\n",
    "        self.bs=bs\n",
    "        self.max_corr = Max_Corr()\n",
    "        if device == 'cuda':\n",
    "            self.max_corr.cuda()\n",
    "        \n",
    "    def inner_step(self,z1norm,z2norm):\n",
    "    \n",
    "        max_corr=self.max_corr\n",
    "        I=self.I\n",
    "        bs=self.bs\n",
    "        inner_steps=self.inner_steps\n",
    "\n",
    "        z1norm=z1norm.detach()\n",
    "        z2norm=z2norm.detach()\n",
    "\n",
    "        # z1norm=z1norm[:,0]\n",
    "        # z2norm=z2norm[:,0]\n",
    "\n",
    "        max_corr = Max_Corr()\n",
    "        #max_corr.cuda()\n",
    "    \n",
    "        # for p in max_corr.parameters():\n",
    "        #     p.requires_grad=True]\n",
    "\n",
    "        optimizer = torch.optim.Adam(list(max_corr.parameters()),lr=0.001)\n",
    "        for i in range(inner_steps):\n",
    "            z1norm_2,z2norm_2=max_corr(z1norm,z2norm)\n",
    "            #z1norm_2 = (z1norm_2 - z1norm_2.mean(0)) / z1norm_2.std(0, unbiased=False)\n",
    "        \n",
    "            assert (z1norm_2.shape,z2norm_2.shape) == (z1norm.shape,z2norm.shape)\n",
    "\n",
    "            # Ctem1 =  (z1norm.T @ z2norm_2) / bs\n",
    "            # Ctem2 = (z1norm_2.T @ z2norm) / bs\n",
    "            # cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2))\n",
    "\n",
    "            cdiff_2 = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)\n",
    "            #cdiff_2=Ctem.pow(2)\n",
    "\n",
    "            inner_loss=-1*(cdiff_2*(1-I)).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            inner_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        for p in max_corr.parameters():\n",
    "            p.requires_grad=False\n",
    "            \n",
    "        return max_corr\n",
    "    \n",
    "    def __call__(self,z1norm,z2norm):\n",
    "        \n",
    "            max_corr =  self.inner_step(z1norm,z2norm)\n",
    "            z1norm_2,z2norm_2 = max_corr(z1norm,z2norm)\n",
    "      \n",
    "            cdiff_sup = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)\n",
    "    \n",
    "            return cdiff_sup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sinusoid(x,t,std=0.1,seed=0):\n",
    "        \n",
    "    seed_everything(seed=seed)    \n",
    "    #t=(std) * torch.randn(1,500).to(device)\n",
    "    s=(std) * torch.randn(1,500).to(device)\n",
    "    \n",
    "    u=torch.randn(1,500).to(device)\n",
    "    v=torch.randn(1,500).to(device)\n",
    "\n",
    "    a=(0.2) * torch.randn(1,500).to(device)\n",
    "    b=(0.2) * torch.randn(1,500).to(device)\n",
    "    # N = torch.abs(a) + torch.abs(b)\n",
    "    # a = a/N\n",
    "    # b = b/N\n",
    "\n",
    "    return a*torch.sin(t*x[:,]*math.pi+u) + b*torch.cos(s*x[:,]*math.pi+v)\n",
    "\n",
    "\n",
    "    \n",
    "    #return torch.sin(t*math.pi*x+u) + torch.cos(s*math.pi*x + v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class t_Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(500,500)\n",
    "        self.fc2 = nn.Linear(500,500)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.sigmoid(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    \n",
    "t_gen = t_Generator()\n",
    "t_gen_optimizer = torch.optim.Adam(list(t_gen.parameters()),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "a2Exs2s3ag8z"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class BarlowTwins(Callback):\n",
    "    order,run_valid = 9,True\n",
    "    def __init__(self, aug_pipelines, lmb=5e-3, print_augs=False):\n",
    "        assert_aug_pipelines(aug_pipelines)\n",
    "        self.aug1, self.aug2 = aug_pipelines\n",
    "        if print_augs: print(self.aug1), print(self.aug2)\n",
    "        store_attr('lmb')\n",
    "        self.index=-1\n",
    "\n",
    "        self.inner_steps=4\n",
    "        \n",
    "    def before_fit(self): \n",
    "        self.learn.loss_func = self.lf\n",
    "        nf = self.learn.model.projector[-1].out_features\n",
    "        self.I = torch.eye(nf).to(self.dls.device)\n",
    "\n",
    "    def update_seed(self):\n",
    "        \n",
    "        indexmod=2\n",
    "        if self.index%indexmod == 0: #every `indexmod` index update the seed (best we have found so far)\n",
    "            self.seed = np.random.randint(0,10000)\n",
    "\n",
    "    def before_epoch(self):\n",
    "        self.index=-1\n",
    "\n",
    "        if self.epoch%10==0:\n",
    "            self.inner_steps += 1\n",
    "            \n",
    "    def before_batch(self):\n",
    "        xi,xj = self.aug1(self.x), self.aug2(self.x)\n",
    "        self.learn.xb = (torch.cat([xi, xj]),)\n",
    "\n",
    "        self.index=self.index+1\n",
    "\n",
    "        self.update_seed()\n",
    "\n",
    "\n",
    "    def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
    "        bs,nf = pred.size(0)//2,pred.size(1)\n",
    "\n",
    "        #All standard, from BT\n",
    "        z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "        z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "        z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "        \n",
    "        C = (z1norm.T @ z2norm) / bs \n",
    "        cdiff = (C - self.I)**2\n",
    "\n",
    "\n",
    "\n",
    "        CdiffSup = Cdiff_Sup(I=self.I,inner_steps=5,bs=bs)\n",
    "        cdiff_2 = CdiffSup(z1norm,z2norm)\n",
    "\n",
    "        CdiffRand = Cdiff_Rand(seed=self.seed,I=self.I,inner_steps=3,bs=bs,std=0.2,K=1)\n",
    "        cdiff_2_2 = CdiffRand(z1norm,z2norm)\n",
    "\n",
    "\n",
    "        cdiff_2 = 0.5*cdiff_2_2 + 0.5*cdiff_2\n",
    "            \n",
    "            \n",
    "        l2 = cdiff_2*(1-self.I)*self.lmb #Is either the standard term - or not.\n",
    "\n",
    "\n",
    "        loss = (cdiff*self.I + l2).sum()\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def show(self, n=1):\n",
    "        bs = self.learn.x.size(0)//2\n",
    "        x1,x2  = self.learn.x[:bs], self.learn.x[bs:] \n",
    "        idxs = np.random.choice(range(bs),n,False)\n",
    "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n",
    "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n",
    "        images = []\n",
    "        for i in range(n): images += [x1[i],x2[i]] \n",
    "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vbS1WtLiag80",
    "outputId": "8a0b4cd1-5225-4a82-a901-61ab409cb819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/200 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='4' class='' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      12.50% [4/32 00:51<05:58 188.7596]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m learn \u001b[38;5;241m=\u001b[39m Learner(dls,model, cbs\u001b[38;5;241m=\u001b[39m[BarlowTwins(aug_pipelines, print_augs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#learn = Learner(dls, model,opt_func=opt_func, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mlearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/fastai/learner.py:222\u001b[0m, in \u001b[0;36mLearner.fit\u001b[0;34m(self, n_epoch, lr, wd, cbs, reset_opt)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mset_hypers(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m lr)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch \u001b[38;5;241m=\u001b[39m n_epoch\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelFitException\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_end_cleanup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/fastai/learner.py:164\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/fastai/learner.py:213\u001b[0m, in \u001b[0;36mLearner._do_fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_epoch):\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch\u001b[38;5;241m=\u001b[39mepoch\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelEpochException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/fastai/learner.py:164\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/fastai/learner.py:207\u001b[0m, in \u001b[0;36mLearner._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_epoch_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch_validate()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/fastai/learner.py:199\u001b[0m, in \u001b[0;36mLearner._do_epoch_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_epoch_train\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdls\u001b[38;5;241m.\u001b[39mtrain\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelTrainException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/fastai/learner.py:164\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/fastai/learner.py:170\u001b[0m, in \u001b[0;36mLearner.all_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_batches\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl): \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/fastai/learner.py:195\u001b[0m, in \u001b[0;36mLearner.one_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    193\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_device(b)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split(b)\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_events\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_one_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCancelBatchException\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/fastai/learner.py:164\u001b[0m, in \u001b[0;36mLearner._with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_with_events\u001b[39m(\u001b[38;5;28mself\u001b[39m, f, event_type, ex, final\u001b[38;5;241m=\u001b[39mnoop):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ex: \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_cancel_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevent_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m);  final()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/fastai/learner.py:181\u001b[0m, in \u001b[0;36mLearner._do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myb): \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_backward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_events(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mstep, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m, CancelStepException)\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/torch/_tensor.py:355\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor w.r.t. graph leaves.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/torch/overrides.py:1394\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1388\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1389\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be an error in PyTorch 1.11, please define it as a classmethod.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1390\u001b[0m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[0;32m-> 1394\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/fastai/torch_core.py:341\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _torch_handled(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opt, func): convert,types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m),(torch\u001b[38;5;241m.\u001b[39mTensor,)\n\u001b[0;32m--> 341\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert: res \u001b[38;5;241m=\u001b[39m convert(res)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, TensorBase): res\u001b[38;5;241m.\u001b[39mset_meta(\u001b[38;5;28mself\u001b[39m, as_copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/torch/_tensor.py:1142\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunction():\n\u001b[0;32m-> 1142\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Debugging cell - delete later (similar to cell below)\n",
    "ps=500\n",
    "hs=500\n",
    "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
    "model = create_barlow_twins_model(fastai_encoder, hidden_size=hs,projection_size=ps)# projection_size=1024)\n",
    "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
    "#values for these which is tantamount to doing nothing\n",
    "#So if we choose resize_scale=(1,1) then the images look the same.\n",
    "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
    "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=True)\n",
    "#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwRMSProp(model.parameters(),lr=0.1, mom=0.9)ins(aug_pipelines, print_augs=True)])\n",
    "opt = torch.optim.RMSprop\n",
    "#partial(OptimWrapper, opt=opt)\n",
    "learn = Learner(dls,model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
    "#learn = Learner(dls, model,opt_func=opt_func, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
    "\n",
    "learn.fit(200) #300                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Fdbbd-uV8Ltr"
   },
   "outputs": [],
   "source": [
    "#new\n",
    "def seed_everything(seed=42):\n",
    "    \"\"\"\"\n",
    "    Seed everything.\n",
    "    \"\"\"   \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def tune_set(items0,seed=None,bs_tune=20):\n",
    "    \n",
    "    seed_everything(seed=seed)\n",
    "    \n",
    "    items0=items0.shuffle()\n",
    "    d = {'0':0,'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0,'8':0,'9':0}\n",
    "    ITEMS=[]\n",
    "    for i in items0:\n",
    "        s=str(i).split('/training/')[1][0]\n",
    "        if d[s] is 0 or d[s] is 1:\n",
    "            ITEMS.append(i)\n",
    "            d[s]+=1\n",
    "    #items0=ITEMS\n",
    "\n",
    "    for i in items0:\n",
    "        if i not in ITEMS:\n",
    "            ITEMS.append(i)\n",
    "            \n",
    "    split = IndexSplitter(list(range(bs_tune)))\n",
    "\n",
    "    tds_tune = Datasets(ITEMS, [PILImageBW.create, [parent_label, Categorize()]], splits=split(ITEMS)) #Or do we want this?\n",
    "    dls_tune = tds_tune.dataloaders(bs=bs_tune,num_workers=0, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
    "    \n",
    "    return dls_tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "89bdbbno8TNf"
   },
   "outputs": [],
   "source": [
    "def shuffle_items(items,seed):\n",
    "    \"\"\"Helper function to sort a list according to given random seed\n",
    "    \"\"\"\n",
    "    items.sort()\n",
    "    \n",
    "    if seed !=None:\n",
    "        seed_everything(seed=seed)\n",
    "        items=items.shuffle()\n",
    "    \n",
    "    return items\n",
    "\n",
    "class BT_Data:\n",
    "    \n",
    "    def __init__(self,items,seed=42,ts=16384,bs=512,tune_s=2000,bs_tune=20,bs_test=578):\n",
    "        \n",
    "        self.ts=ts\n",
    "        self.bs=bs\n",
    "        self.tune_s=tune_s\n",
    "        self.bs_tune=bs_tune\n",
    "        self.bs_test=bs_test\n",
    "        \n",
    "        self._seed=seed\n",
    "        self.seed=seed\n",
    "        items=shuffle_items(items,seed)\n",
    "        self.items=items\n",
    "\n",
    "    @property\n",
    "    def seed(self):\n",
    "        return self._seed\n",
    "    \n",
    "    @seed.setter #When we update the seed, we update the datasets (so items and dls objects) accordingly\n",
    "    def seed(self,val):\n",
    "        self._seed=val\n",
    "        self.items = shuffle_items(items,val)\n",
    "        self.build_items_i()\n",
    "        self.build_dls()\n",
    "        \n",
    "    def build_items_i(self):\n",
    "        self.items1 = self.items[0:ts] #train BT on these guys\n",
    "        self.items0 = self.items[self.ts:self.ts+self.tune_s] #for fine tuning - just choose 2000 guys to extract 20 for fine tuning \n",
    "        self.items2 = self.items[self.ts+self.tune_s:] #test on remainder\n",
    "        \n",
    "    def build_dls(self):\n",
    "        \n",
    "        split = RandomSplitter(valid_pct=0.0)\n",
    "        tds = Datasets(self.items1, [PILImageBW.create, [parent_label, Categorize()]], splits=split(self.items1))\n",
    "        self.dls = tds.dataloaders(bs=self.bs,num_workers=0, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
    "\n",
    "        #Evaluate linear classifier on this guy\n",
    "        split = RandomSplitter(valid_pct=0.0) #randomly split training set into training and validation\n",
    "        tds_test = Datasets(self.items2, [PILImageBW.create, [parent_label, Categorize()]], splits=split(self.items2)) #Or do we want this?\n",
    "        self.dls_test = tds_test.dataloaders(bs=self.bs_test,num_workers=0, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
    "\n",
    "\n",
    "def build_BT_Data(items,seed,tune_seed):\n",
    "    ts=16384\n",
    "    bs=512\n",
    "    tune_s=2000\n",
    "    bs_tune=20\n",
    "    bs_test=578\n",
    "    \n",
    "    k=dict(seed=seed,ts=ts,bs=bs,tune_s=tune_s,bs_tune=bs_tune,bs_test=bs_test)\n",
    "    bt_dataset = BT_Data(items=items,**k)\n",
    "\n",
    "    items = bt_dataset.items\n",
    "    items1 = bt_dataset.items1\n",
    "    items0 = bt_dataset.items0\n",
    "    items2 = bt_dataset.items2\n",
    "\n",
    "    dls = bt_dataset.dls\n",
    "    dls_test = bt_dataset.dls_test\n",
    "\n",
    "    dls_tune=tune_set(items0=items0,seed=seed,bs_tune=bs_tune)\n",
    "    \n",
    "    return dict(seed=seed,tune_seed=tune_seed,items=items,items1=items1,items0=items0,items2=items2,dls=dls,dls_test=dls_test,dls_tune=dls_tune)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "7gKpW58J8oiU"
   },
   "outputs": [],
   "source": [
    "#new\n",
    "\n",
    "path = untar_data(URLs.MNIST)\n",
    "items = get_image_files(path/'training') #i.e. NOT testing!!!\n",
    "items.sort()\n",
    "\n",
    "seed=420\n",
    "tune_seed=100\n",
    "ts=16384\n",
    "bs=512\n",
    "tune_s=2000\n",
    "bs_tune=20\n",
    "bs_test=578\n",
    "\n",
    "bt_data = BT_Data(items=items,seed=seed,ts=ts,bs=bs,tune_s=tune_s,bs_tune=bs_tune,bs_test=bs_test)\n",
    "\n",
    "items1=bt_data.items1\n",
    "items0=bt_data.items0\n",
    "items2=bt_data.items2\n",
    "\n",
    "dls=bt_data.dls\n",
    "dls_test=bt_data.dls_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1z3pyrAB8rpN",
    "outputId": "cd025c63-661c-491b-c55c-5f2ef538225e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                   [100%]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "#TODO: Rewrite so works for both (seed,tune_seed) = (42,10) and (420,100). At the moment \n",
    "\n",
    "labeller = using_attr(RegexLabeller(pat = r'(\\d+).png$'), 'name')\n",
    "convert_tensor = transforms.ToTensor()\n",
    "Expected_first_item = {42:{'items1':'19825','items0':'40684','items2':'43064'},420:{'items1':'44942','items0':'23821','items2':'908'}}\n",
    "Expected_first_dls = {42:{'dls':0.085169,'dls_test':0.099924},420:{'dls':0.183678,'dls_test':0.162825}}\n",
    "Expected_first_dls_tune = {(42,55):0.093707,(420,55):0.1513355}\n",
    "\n",
    "bt_data_42 = build_BT_Data(items=items,seed=42,tune_seed=10)\n",
    "bt_data_420 = build_BT_Data(items=items,seed=420,tune_seed=100)\n",
    "\n",
    "def verify_DatasetShape(dls_obj,batch_size,ds_settype='train'):\n",
    "    \"\"\"\"Helper function to verify shape of a dls object given the batch size; ds_settype is either `train` or \n",
    "        `valid`. The idea is we want the batch_size to divide the length of the dlsobj.\n",
    "    \"\"\"\n",
    "    \n",
    "    tem = len(getattr(dls_obj,ds_settype)) #length of dlsobj.train or dlsobj.valid depending on settpe\n",
    "    return tem*batch_size == len(getattr(dls_obj,ds_settype+'_ds'))\n",
    "\n",
    "def verify_first_item(items,expected):\n",
    "    \"\"\"Helper function to verify first element of items is as expected, given random seed of 42\n",
    "    \"\"\"\n",
    "        \n",
    "    return labeller(items[0]) == expected\n",
    "        \n",
    "def verify_first_dls(dls_obj,expected,ds_settype='train'):\n",
    "    \"\"\"Helper function to verify first element of the given dls object is as expected, given random seed of 42.\n",
    "        Note that ds_settype is either `train` or `valid\n",
    "    \"\"\"\n",
    "    \n",
    "    #All we are doing here is getting the first tensor in, for example e.g. dls_obj.train_ds and computing\n",
    "    #the mean of all the elements. If random seed is same, then it should give the same results\n",
    "    z=convert_tensor(next(iter(getattr(dls_obj,ds_settype+'_ds')))[0]).mean().item()\n",
    "    \n",
    "    #logging.debug(f'with {ds_settype} has: {z}')\n",
    "    assert z-expected < 0.0001\n",
    "\n",
    "    \n",
    "@pytest.mark.parametrize('bt_dataset',[bt_data_42,bt_data_420])\n",
    "class Test_shapes:\n",
    "    \n",
    "    def test_shape_dlsobjects(self,bt_dataset):\n",
    "        \"\"\"\"Test the shape of each dlsobj\n",
    "        \"\"\"\n",
    "    \n",
    "        assert verify_DatasetShape(bt_dataset['dls'],batch_size=bs,ds_settype='train')\n",
    "\n",
    "        assert verify_DatasetShape(bt_dataset['dls_tune'],batch_size=bs_tune,ds_settype='valid')\n",
    "\n",
    "        assert verify_DatasetShape(bt_dataset['dls_test'],batch_size=bs_test,ds_settype='train')\n",
    "    \n",
    "    def test_length_dlsobjects(self,bt_dataset):\n",
    "        \"\"\"\"Test the length of each dlsobj that we use\n",
    "        \"\"\"\n",
    "        assert len(bt_dataset['dls'].train_ds) == ts and len(bt_dataset['dls_tune'].valid_ds) == bs_tune and len(bt_dataset['dls_test'].train_ds)==41616\n",
    "    \n",
    "    \n",
    "@pytest.mark.parametrize('bt_dataset',[bt_data_42,bt_data_420])   \n",
    "class Test_first:\n",
    "    \n",
    "    def test_first_item(self,bt_dataset):\n",
    "        \"\"\"\"Verify that the first item of each items is as expected\n",
    "        \"\"\"\n",
    "\n",
    "        seed = bt_dataset['seed']\n",
    "\n",
    "        assert verify_first_item(bt_dataset['items1'],Expected_first_item[seed]['items1'])\n",
    "\n",
    "        assert verify_first_item(bt_dataset['items0'],Expected_first_item[seed]['items0'])\n",
    "\n",
    "        assert verify_first_item(bt_dataset['items2'],Expected_first_item[seed]['items2'])\n",
    "\n",
    "    def test_first_dlsobj(self,bt_dataset):\n",
    "        \"\"\"Verify that the first item of each dlsobj is as expected\n",
    "        \"\"\"\n",
    "        seed = bt_dataset['seed']\n",
    "        dls = bt_dataset['dls']\n",
    "        dls_test = bt_dataset['dls_test']\n",
    "        items0 = bt_dataset['items0']\n",
    "\n",
    "        verify_first_dls(dls,ds_settype='train',expected=Expected_first_dls[seed]['dls'])\n",
    "        verify_first_dls(dls_test,ds_settype='train',expected=Expected_first_dls[seed]['dls_test'])\n",
    "\n",
    "        tune_seed=55\n",
    "        dls_tune=tune_set(items0,seed=tune_seed,bs_tune=bs_tune)\n",
    "\n",
    "        verify_first_dls(dls_tune,ds_settype='valid',expected=Expected_first_dls_tune[(seed,tune_seed)])\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize('bt_dataset',[bt_data_42,bt_data_420])\n",
    "def test1_tune_set(bt_dataset):\n",
    "    \"\"\"Check whether the function `tune_set` gives us the expected values\"\"\"\n",
    "    \n",
    "\n",
    "    seed=bt_dataset['seed']\n",
    "    tune_seed=bt_dataset['tune_seed']\n",
    "    items0 = bt_dataset['items0']\n",
    "    \n",
    "    if tune_seed==10 and seed==42:\n",
    "        expected = {10:0.12255,11:0.153564,12:0.12781,13:0.129523,14:0.13019}\n",
    "    \n",
    "    elif tune_seed==100 and seed==420:\n",
    "        expected={100:0.136104,101:0.120989,102:0.1381390,103:0.1380412,104:0.14285138}\n",
    "        \n",
    "    for i in range(5):\n",
    "        #seed_everything(seed=seed)\n",
    "        dls_tune=tune_set(items0,seed=tune_seed+i,bs_tune=20)\n",
    "        x_mean=0\n",
    "        for x,y in dls_tune.valid:\n",
    "            x_mean += x.mean()\n",
    "\n",
    "        assert abs(x_mean-expected[tune_seed+i])<0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "AKbw2pxMag82",
    "outputId": "ad338376-6361-44a8-afcd-4c1cbc00eb5c"
   },
   "outputs": [],
   "source": [
    "#A \"reasonable\" composite augmentation: initially copy pasted BT. We run this cell a few times to check it makes sense\n",
    "#Also define encoder and model\n",
    "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
    "model = create_barlow_twins_model(fastai_encoder, hidden_size=10,projection_size=10)# projection_size=1024)\n",
    "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
    "#values for these which is tantamount to doing nothing\n",
    "#So if we choose resize_scale=(1,1) then the images look the same.\n",
    "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
    "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=False)\n",
    "#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
    "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
    "\n",
    "#dls.valid.bs = len(dls.valid_ds) #Set the validation dataloader batch size to be the length of the validation dataset\n",
    "\n",
    "b = dls.one_batch()\n",
    "learn._split(b)\n",
    "learn('before_batch')\n",
    "axes = learn.barlow_twins.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IXTxgA9-Mhih"
   },
   "outputs": [],
   "source": [
    "#Simple linear classifier\n",
    "class LinearClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,zdim):\n",
    "            \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(zdim,10) #As 10 classes for mnist\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = cast(self.fc1(x),Tensor) #so we have to use cross entropy loss. cast is because using old version fastai \n",
    "        return x\n",
    "\n",
    "def turnoffgrad_model(fastai_encoder):\n",
    "    for p in fastai_encoder.parameters():\n",
    "        p.requires_grad=False\n",
    "        \n",
    "    return fastai_encoder\n",
    "\n",
    "#NB: Will give same random 20-tune set (for fixed random seed), only if the cell\n",
    "#\"#Get the dataloader and set batch size\" is the same. Perhaps later we can make this cell a function of that one. \n",
    "#Functions to train and evaluate head\n",
    "fastai_encoder.eval()\n",
    "encoder_nograd = turnoffgrad_model(fastai_encoder) \n",
    "def train_head(encoder_nograd,tune_seed=10,bs_tune=20): #The seed choses a different (20) samples for training the head. 2 of each class\n",
    "    \"\"\"Train head on a tune_set, chosen through given tune_seed for reproducibility if needed\n",
    "    \"\"\"\n",
    "                                    # of the tune_seed)\n",
    "    \n",
    "    dls_tune=tune_set(items0,seed=tune_seed,bs_tune=bs_tune) #different random tune set each time (but as a function of tune_seed)\n",
    " \n",
    "    N=len(dls_tune.valid)*bs_tune \n",
    "    assert N == len(dls_tune.valid_ds) #Check that the tune set (valid) is divided by the batch size\n",
    "    assert len(dls_tune.valid_ds) == bs_tune\n",
    "\n",
    "    zdim=1024 #see above\n",
    "    head = LinearClassifier(zdim=zdim)\n",
    "    head.to(device)\n",
    "    optimizer = torch.optim.Adam(head.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(200):\n",
    "        #for x,y in dls_tune.valid: #Slows massively on colab but not on kaggle. Weird. \n",
    "        x,y=dls_tune.valid.one_batch() #Same every time since dataset only has length=batch size = 20.\n",
    "                                        #Will need to fix this for CIFAR10 etc\n",
    "\n",
    "        loss = criterion(head(encoder_nograd(x)),y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return head\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_head(head):\n",
    "    \"\"\"Evaluate the (typically trained) head on on the test set\n",
    "    \"\"\"\n",
    "    N=len(dls_test.train)*bs_test\n",
    "    assert N == len(dls_test.train_ds)\n",
    "\n",
    "    num_correct=0\n",
    "    for x,y in dls_test.train:\n",
    "\n",
    "        ypred = head(encoder_nograd(x))\n",
    "        correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n",
    "        num_correct += correct.sum()\n",
    "    \n",
    "    return num_correct/N\n",
    "\n",
    "def eval_encoder(encoder_nograd,tune_seed=10):\n",
    "    \"\"\"\"Evaluate the encoder, which means to train and evaluate the head - basically wrap functions train_head\n",
    "        and eval_head\n",
    "    \"\"\"\n",
    "    head=train_head(encoder_nograd,tune_seed=tune_seed)\n",
    "    pct_correct = eval_head(head)\n",
    "    return pct_correct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKgPajGBMpSn",
    "outputId": "2d8a7c13-2d7e-49c8-f5bc-a5914fa58983"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "assert tune_seed==100\n",
    "assert seed == 420\n",
    "performance_dict={}\n",
    "for num in range(5):\n",
    "    \n",
    "    pct_correct = eval_encoder(encoder_nograd,tune_seed=tune_seed+num)\n",
    "    performance_dict[f'seed_{num}'] = pct_correct \n",
    "\n",
    "print(torch.mean(tensor(list(performance_dict.values()))))\n",
    "performance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NpcqDLMJWpkB",
    "outputId": "f8f4b517-a880-48a9-908c-3c16d3d82137"
   },
   "outputs": [],
   "source": [
    "#test: 0.1361,0.1210\n",
    "#BT baseline, new random seeds, 0.7693\n",
    "\n",
    "print(torch.mean(tensor(list(performance_dict.values()))))\n",
    "performance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "G1gszrPuMuzW"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KMNc94mz5CJ",
    "outputId": "8b0b52ab-d817-4c04-b99c-8374c0d1e4a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7591)\n",
      "tensor(0.8379)\n",
      "tensor(0.7621)\n",
      "tensor(0.8126)\n"
     ]
    }
   ],
   "source": [
    "#200 learn epochs\n",
    "BT_42_10_run1={'seed_0': TensorCategory(0.7667),\n",
    " 'seed_1': TensorCategory(0.6843),\n",
    " 'seed_2': TensorCategory(0.8468),\n",
    " 'seed_3': TensorCategory(0.7687),\n",
    " 'seed_4': TensorCategory(0.7288)}\n",
    "print(torch.mean(tensor(list(BT_42_10_run1.values()))))#tensor(0.7591)\n",
    "\n",
    "MBT_42_10_run1 = {'seed_0': TensorCategory(0.8238),\n",
    " 'seed_1': TensorCategory(0.7834),\n",
    " 'seed_2': TensorCategory(0.8999),\n",
    " 'seed_3': TensorCategory(0.8601),\n",
    " 'seed_4': TensorCategory(0.8221)}\n",
    "\n",
    "print(torch.mean(tensor(list(MBT_42_10_run1.values()))))#tensor(0.8379)\n",
    "\n",
    "BT_420_100_run1 = {'seed_0': TensorCategory(0.7532),\n",
    " 'seed_1': TensorCategory(0.7501),\n",
    " 'seed_2': TensorCategory(0.7808),\n",
    " 'seed_3': TensorCategory(0.7804),\n",
    " 'seed_4': TensorCategory(0.7461)}\n",
    "\n",
    "print(torch.mean(tensor(list(BT_420_100_run1.values()))))#tensor(0.7621)\n",
    "\n",
    "MBT_420_100_run1={'seed_0': TensorCategory(0.8016),\n",
    " 'seed_1': TensorCategory(0.7849),\n",
    " 'seed_2': TensorCategory(0.8416),\n",
    " 'seed_3': TensorCategory(0.8364),\n",
    " 'seed_4': TensorCategory(0.7986)}\n",
    "\n",
    "print(torch.mean(tensor(list(MBT_420_100_run1.values())))) #tensor(0.8126)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "yUHZSinCsrt4",
    "outputId": "9c8ed2a0-eb46-423f-a0a0-c6dbeee1d333"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#100 learn_epochs, 420 and 100 random seeds used instead of 42 and 10.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m BT_1\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed_0\u001b[39m\u001b[38;5;124m'\u001b[39m: TensorCategory(\u001b[38;5;241m0.7614\u001b[39m),\n\u001b[1;32m      3\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed_1\u001b[39m\u001b[38;5;124m'\u001b[39m: TensorCategory(\u001b[38;5;241m0.7328\u001b[39m),\n\u001b[1;32m      4\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed_2\u001b[39m\u001b[38;5;124m'\u001b[39m: TensorCategory(\u001b[38;5;241m0.7640\u001b[39m),\n\u001b[1;32m      5\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed_3\u001b[39m\u001b[38;5;124m'\u001b[39m: TensorCategory(\u001b[38;5;241m0.8039\u001b[39m),\n\u001b[1;32m      6\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed_4\u001b[39m\u001b[38;5;124m'\u001b[39m: TensorCategory(\u001b[38;5;241m0.7309\u001b[39m)}\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mmean(tensor(\u001b[38;5;28mlist\u001b[39m(\u001b[43mBT\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())))) \u001b[38;5;66;03m#tensor(0.7586)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m BT_2\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed_0\u001b[39m\u001b[38;5;124m'\u001b[39m: TensorCategory(\u001b[38;5;241m0.7413\u001b[39m),\n\u001b[1;32m     10\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed_1\u001b[39m\u001b[38;5;124m'\u001b[39m: TensorCategory(\u001b[38;5;241m0.7645\u001b[39m),\n\u001b[1;32m     11\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed_2\u001b[39m\u001b[38;5;124m'\u001b[39m: TensorCategory(\u001b[38;5;241m0.7981\u001b[39m),\n\u001b[1;32m     12\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed_3\u001b[39m\u001b[38;5;124m'\u001b[39m: TensorCategory(\u001b[38;5;241m0.8010\u001b[39m),\n\u001b[1;32m     13\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed_4\u001b[39m\u001b[38;5;124m'\u001b[39m: TensorCategory(\u001b[38;5;241m0.7355\u001b[39m)}\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mmean(tensor(\u001b[38;5;28mlist\u001b[39m(BT_2\u001b[38;5;241m.\u001b[39mvalues())))) \u001b[38;5;66;03m#tensor(0.7681)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BT' is not defined"
     ]
    }
   ],
   "source": [
    "#100 learn_epochs, 420 and 100 random seeds used instead of 42 and 10.\n",
    "BT_1={'seed_0': TensorCategory(0.7614),\n",
    " 'seed_1': TensorCategory(0.7328),\n",
    " 'seed_2': TensorCategory(0.7640),\n",
    " 'seed_3': TensorCategory(0.8039),\n",
    " 'seed_4': TensorCategory(0.7309)}\n",
    "print(torch.mean(tensor(list(BT.values())))) #tensor(0.7586)\n",
    "\n",
    "BT_2={'seed_0': TensorCategory(0.7413),\n",
    " 'seed_1': TensorCategory(0.7645),\n",
    " 'seed_2': TensorCategory(0.7981),\n",
    " 'seed_3': TensorCategory(0.8010),\n",
    " 'seed_4': TensorCategory(0.7355)}\n",
    "print(torch.mean(tensor(list(BT_2.values())))) #tensor(0.7681)\n",
    "\n",
    "MBT_1 = {'seed_0': TensorCategory(0.7721),\n",
    " 'seed_1': TensorCategory(0.7693),\n",
    " 'seed_2': TensorCategory(0.8398),\n",
    " 'seed_3': TensorCategory(0.8129),\n",
    " 'seed_4': TensorCategory(0.7908)}\n",
    "print(torch.mean(tensor(list(MBT_1.values())))) #tensor(0.7970)\n",
    "\n",
    "MBT_2 = {'seed_0': TensorCategory(0.8126),\n",
    " 'seed_1': TensorCategory(0.7781),\n",
    " 'seed_2': TensorCategory(0.8234),\n",
    " 'seed_3': TensorCategory(0.8280),\n",
    " 'seed_4': TensorCategory(0.8084)}\n",
    "print(torch.mean(tensor(list(MBT_2.values())))) #tensor(0.8101)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjR-EnBubM6y"
   },
   "source": [
    "Please see commit  e849943... 4/10/22 if needed. We have edited the base functions in that file to try and make them nicer, but we need to make sure we can reproduce results (i.e. the changes make things nicer but don't actually change anything).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4bY6DPjb8MY"
   },
   "source": [
    "With: fc1,fc2,fc3,fc4 distinct. Indexmod=2, K=2 With Max_corr = (sigmoid,relu); \n",
    "$a\\sim b$ = 0.2 x N(0,1): 0.8576,0.8364,0.8393\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjPF34iucYWK"
   },
   "source": [
    "Below applies to prior implementation, we leave it here for now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Rmtuu4nxc13"
   },
   "source": [
    "All of the below have sin and cos with constant coefficients. If we take `best so far` and give it random coefficients a and b with std=0.2 then get:\n",
    "\n",
    "With fc1,fc2,fc3,fc4 distinct. Indexmod=2, K=2 With Max_corr = (sigmoid,relu);a~b = 0.2 x N(0,1) **0.8390, 0.8553** Conclusion: We need to search over the a and b parameters (coefficients of sinusoids) when we do our big search. Or rather search over std the hps controlling how we sample a and b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SYRjUytI8Fw"
   },
   "source": [
    "Results (continuing from prior commit):\n",
    "\n",
    "\n",
    "Note these are with 200 learn_epochs etc. Same random seed. See above for other details (and we mention when recording the results below that we varied)\n",
    "BT = 0.7581\n",
    "\n",
    "(These are with fc1,fc2,fc3,fc4 distinct). Indexmod=2, K=2 \n",
    "**With Max_corr = (sigmoid,relu): 0.8493,0.8332,0.8392. Best so far.**\n",
    "With Max_corr = (sigmoid,sigmoid): 0.3080,0.3219.   \n",
    "With Max_corr = (relu,relu): 0.8132,0.8142,0.8093.   \n",
    "\n",
    "(These are with fc1=fc3, fc2=fc4. i.e. just one NN applied)  \n",
    "With Max_corr = (sigmoid,relu): 0.8359,0.8258,0.8303\n",
    "\n",
    "Above are all with indexmod=2. Now we try removing the indexmod condition.\n",
    "Then get:\n",
    "Indexmod=0\n",
    "With Max_corr = (sigmoid,relu): 0.8264,0.8253 \n",
    "\n",
    "Try indexmod=4\n",
    "With Max_corr = (sigmoid,relu):0.8174\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snRrKfwCH6XD"
   },
   "source": [
    "Results on different MBT runs (see above for implementation): tensor(0.8493) (trying to reproduce now) (just changed y=self.sigmoid(self.fc3(y))\n",
    "from relu in MaxCorr). Performance went to tensor(0.3080)!!! Crazy. Let's change y back to relu and see what happens. result: tensor(0.8332)!! Wow. Similar\n",
    "to before (i.e. evidence in favour of reproducibility). All we changed was sigmoid to relu!\n",
    "\n",
    "To summarise: Max_Corr had (sigmoid,relu) and got great results (on two diff MBT runs 0.8493 and 0.8332. When we changed relu to sigmoid, got terrible results (0.3080). (As an aside the loss jumped around a lot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPEDQE-rcKjL",
    "outputId": "8e127c85-873f-4a36-d76a-66a57c566f74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8252)\n"
     ]
    }
   ],
   "source": [
    "#250 learn_epochs. To beat: (K=10,indexmod=4,std=0.1,convex=(0.2,0.8))\n",
    "tem={'seed_0': TensorCategory(0.8364),\n",
    " 'seed_1': TensorCategory(0.7424),\n",
    " 'seed_2': TensorCategory(0.8947),\n",
    " 'seed_3': TensorCategory(0.8392),\n",
    " 'seed_4': TensorCategory(0.8133)}\n",
    "print(torch.mean(tensor(list(tem.values())))) #tensor(0.8252)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZPO88U4XMgb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYI6BiCTXCmt",
    "outputId": "c99dc95d-0a71-4fc3-d791-16cee98bfa8d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "id": "hw0VjYxRx1va",
    "outputId": "659f5266-600d-4b32-a834-a84fff7d18b3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'performance_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mperformance_dict\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'performance_dict' is not defined"
     ]
    }
   ],
   "source": [
    "performance_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COooHESjoNGb"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
