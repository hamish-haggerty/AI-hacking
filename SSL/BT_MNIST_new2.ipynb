{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hamish-haggerty/AI-hacking/blob/master/SSL/BT_MNIST_new2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8jFsEXz_61O",
    "outputId": "2a7dca56-5b44-462c-d24a-f548467e119e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (1.11.0)\n",
      "Requirement already satisfied: torchvision==0.12.0 in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
      "Requirement already satisfied: torchaudio==0.11.0 in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0) (4.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (1.21.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (7.1.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (2.23.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (2022.6.15)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (3.0.4)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: fastai==2.6.3 in /usr/local/lib/python3.7/dist-packages (2.6.3)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: self_supervised in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
      "Requirement already satisfied: timm>=0.4.5 in /usr/local/lib/python3.7/dist-packages (from self_supervised) (0.6.11)\n",
      "Requirement already satisfied: kornia>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from self_supervised) (0.6.7)\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from self_supervised) (21.1.3)\n",
      "Requirement already satisfied: fastai>=2.2.7 in /usr/local/lib/python3.7/dist-packages (from self_supervised) (2.6.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from self_supervised) (21.3)\n",
      "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (3.4.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (3.2.2)\n",
      "Requirement already satisfied: fastcore<1.5,>=1.3.27 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.4.5)\n",
      "Requirement already satisfied: torch<1.12,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.11.0)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (0.12.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.0.2)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (0.0.7)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.7.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (2.23.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.3.5)\n",
      "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (7.1.2)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.0.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.10)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.11.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.3)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.1.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.9.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (8.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.21.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.4.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.64.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.6.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (57.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4->fastai>=2.2.7->self_supervised) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->self_supervised) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4->fastai>=2.2.7->self_supervised) (5.2.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (2022.6.15)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (3.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai>=2.2.7->self_supervised) (0.7.8)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.7/dist-packages (from timm>=0.4.5->self_supervised) (0.10.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4->fastai>=2.2.7->self_supervised) (7.1.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm>=0.4.5->self_supervised) (4.12.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm>=0.4.5->self_supervised) (3.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4->fastai>=2.2.7->self_supervised) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai>=2.2.7->self_supervised) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai>=2.2.7->self_supervised) (2022.2.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (3.1.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (7.1.3)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest) (1.1.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (22.1.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.0.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytest) (21.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest) (4.12.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (2.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytest) (3.0.9)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: ipytest in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
      "Requirement already satisfied: pytest>=5.4 in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.1.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (21.3)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.9.0)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.1.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (22.1.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (4.12.0)\n",
      "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.11.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (4.1.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.0.10)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (57.4.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipytest) (0.8.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (1.15.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (0.2.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipytest) (3.0.9)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipytest) (0.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0\n",
    "!pip install fastai==2.6.3 --no-deps\n",
    "!pip install self_supervised\n",
    "\n",
    "!pip install pytest\n",
    "!pip install ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "BOv4kkJDag8r",
    "outputId": "ae74e137-b70a-4266-a956-66fa59550ea7"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "function ClickConnect(){\n",
       "console.log(\"Working\");\n",
       "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
       "}setInterval(ClickConnect,60000)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "function ClickConnect(){\n",
    "console.log(\"Working\");\n",
    "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
    "}setInterval(ClickConnect,60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Pk01WY_Dag8s"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamishhaggerty/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import fastai\n",
    "import self_supervised\n",
    "import torch\n",
    "assert(fastai.__version__ == '2.6.3') #Check that version is 2.6.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AOjr_YCLag8t"
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *\n",
    "import inspect\n",
    "import warnings\n",
    "import random\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#from Base_Stein.SVGD_classes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XTSdKC6bag8t"
   },
   "outputs": [],
   "source": [
    "#Like most other SSL algorithms BT's model consists of an encoder and projector (MLP) layer.\n",
    "#Definition is straightforward:\n",
    "#https://colab.research.google.com/github/KeremTurgutlu/self_supervised/blob/master/nbs/14%20-%20barlow_twins.ipynb#scrollTo=1M6QcUChcvpz\n",
    "class BarlowTwinsModel(Module):\n",
    "    \"\"\"An encoder followed by a projector\n",
    "    \"\"\"\n",
    "    def __init__(self,encoder,projector):self.encoder,self.projector = encoder,projector\n",
    "        \n",
    "    def forward(self,x): return self.projector(self.encoder(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZL3EE07Pag8u"
   },
   "outputs": [],
   "source": [
    "#HOWEVER instead of directly using the above, by passing both an encoder and a projector, create_barlow_twins_model\n",
    "#function can be used by minimally passing a predefined encoder and the expected input channels.\n",
    "\n",
    "#In the paper it's mentioned that MLP layer consists of 3 layers... following function will create a 3 layer\n",
    "#MLP projector with batchnorm and ReLU by default. Alternatively, you can change bn and nlayers. \n",
    "\n",
    "#Questions: Why torch.no_grad() when doing this?\n",
    "def create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n",
    "    \"Create Barlow Twins model\"\n",
    "    n_in  = in_channels(encoder)\n",
    "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
    "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n",
    "    apply_init(projector)\n",
    "    return BarlowTwinsModel(encoder, projector)\n",
    "\n",
    "#Similar to above. Simple API to make the BT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DFjGL-COag8v"
   },
   "outputs": [],
   "source": [
    "#BarlowTwins Callback\n",
    "#The following parameters can be passed:\n",
    "# - aug_pipelines\n",
    "# Imb lambda is the weight for redundancy reduction term in the loss function\n",
    "\n",
    "@delegates(get_multi_aug_pipelines)\n",
    "def get_barlow_twins_aug_pipelines(size,**kwargs): return get_multi_aug_pipelines(n=2,size=size,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xx4KsywAag8v"
   },
   "outputs": [],
   "source": [
    "#Uniform random number between a and b\n",
    "def Unif(a,b):\n",
    "    return (b-a)*torch.rand(1).item()+a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhLAitFezOTd",
    "outputId": "629a4ab0-6c97-4116-eb95-c5538ab79889"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9810, -0.0651,  1.3501,  1.2751, -1.1664, -0.9559,  0.9603, -0.1899,\n",
       "          1.0485, -1.4752,  0.2482, -2.3171,  0.0653,  1.3667, -0.5276,  0.9628,\n",
       "         -0.9903,  0.8655,  0.2853,  0.1416, -1.0290,  1.3198,  1.0890,  0.3825,\n",
       "          0.3016,  0.8725, -0.0105,  0.0520,  1.0456, -1.8506]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zU4GwLruU5AD"
   },
   "outputs": [],
   "source": [
    "def random_sinusoid(x,std=0.1,seed=0):\n",
    "    \n",
    "    seed_everything(seed=seed)    \n",
    "    t=(std) * torch.randn(1,500).to(device)\n",
    "    s=(std) * torch.randn(1,500).to(device)\n",
    "    \n",
    "    u=torch.randn(1,500).to(device)\n",
    "    v=torch.randn(1,500).to(device)\n",
    "\n",
    "    a=(0.2) * torch.randn(1,500).to(device)\n",
    "    b=(0.2) * torch.randn(1,500).to(device)\n",
    "    # N = torch.abs(a) + torch.abs(b)\n",
    "    # a = a/N\n",
    "    # b = b/N\n",
    "\n",
    "    return a*torch.sin(t*x[:,]*math.pi+u) + b*torch.cos(s*x[:,]*math.pi+v)\n",
    "\n",
    "\n",
    "    \n",
    "    #return torch.sin(t*math.pi*x+u) + torch.cos(s*math.pi*x + v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "id": "Xej8_KxJ71Sl"
   },
   "outputs": [],
   "source": [
    "#New stuff\n",
    "class Cdiff_Rand:\n",
    "    \n",
    "    def __init__(self,seed,bs,std=0.1,K=2):\n",
    "        self.seed=seed\n",
    "        self.std=std\n",
    "        self.K=2\n",
    "        self.bs=bs\n",
    "\n",
    "    def __call__(self,z1norm,z2norm):\n",
    "        \n",
    "        cdiff_rand=0\n",
    "        for i in range(self.K):\n",
    "\n",
    "            z1norm_2,z2norm_2 = random_sinusoid(z1norm,std=self.std,seed=self.seed+i), random_sinusoid(z2norm,std=self.std,seed=2*self.seed+i)\n",
    "            cdiff_rand = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)\n",
    "\n",
    "        cdiff_rand=(1/self.K)*cdiff_rand\n",
    "    \n",
    "        return cdiff_rand\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "id": "2O8Jwpc0ReJi"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Old stuff\n",
    "def inner_step(z1norm,z2norm,I,inner_steps=5):\n",
    " \n",
    "    z1norm=z1norm.detach()\n",
    "    z2norm=z2norm.detach()\n",
    "\n",
    "    # z1norm=z1norm[:,0]\n",
    "    # z2norm=z2norm[:,0]\n",
    "\n",
    "    max_corr = Max_Corr()\n",
    "    max_corr.cuda()\n",
    " \n",
    "    # for p in max_corr.parameters():\n",
    "    #     p.requires_grad=True]\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(max_corr.parameters()),lr=0.001)\n",
    "    for i in range(inner_steps):\n",
    "        z1norm_2,z2norm_2=max_corr(z1norm,z2norm)\n",
    "        #z1norm_2 = (z1norm_2 - z1norm_2.mean(0)) / z1norm_2.std(0, unbiased=False)\n",
    "    \n",
    "    \n",
    "        assert (z1norm_2.shape,z2norm_2.shape) == (z1norm.shape,z2norm.shape)\n",
    "\n",
    "\n",
    "        #Ctem =  (z1norm_2.T @ z2norm_2) / bs\n",
    "        Ctem1 =  (z1norm.T @ z2norm_2) / bs\n",
    "        Ctem2 = (z1norm_2.T @ z2norm) / bs\n",
    "\n",
    "\n",
    "        cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2))\n",
    "        #cdiff_2=Ctem.pow(2)\n",
    "\n",
    "        inner_loss=-1*(cdiff_2*(1-I)).mean()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        inner_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "            \n",
    "    #z1norm_2,z2norm_2=max_corr(z1norm,z2norm)\n",
    "    \n",
    "    for p in max_corr.parameters():\n",
    "        p.requires_grad=False\n",
    "        \n",
    "    return max_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "XsNBXcIjyjD2"
   },
   "outputs": [],
   "source": [
    "#New stuff\n",
    "def C_z1z2(z1norm,z1norm_2,z2norm,z2norm_2,bs):\n",
    "    \n",
    "    Ctem1 =  (z1norm.T @ z2norm_2) / bs\n",
    "    Ctem2 = (z1norm_2.T @ z2norm) / bs\n",
    "    cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2))\n",
    "\n",
    "    return cdiff_2\n",
    "\n",
    "class Max_Corr(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(ps,ps)\n",
    "        self.fc2 = nn.Linear(ps,ps)\n",
    "\n",
    "        self.fc3 = nn.Linear(ps,ps)\n",
    "        self.fc4 = nn.Linear(ps,ps)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x,y):\n",
    "\n",
    "        x=self.sigmoid(self.fc1(x)) #when (sigmoid,relu) GREAT results, with (sigmoid,sigmoid) TERRIBLE. Currently testing (relu,relu)\n",
    "        x=self.fc2(x)\n",
    "       \n",
    "        y=self.relu(self.fc3(y)) #originally had relu and got really good results. If we can't reproduce those results, possible reasons:\n",
    "                                    #results were due to chance; or having relu on one branch (and sigmoid on the other) helps via breaking\n",
    "                                      #the symmetry! Other idea: set fc1=fc3, fc2=fc4. \n",
    "        y=self.fc4(y)\n",
    "\n",
    "        return x,y\n",
    "\n",
    "class Cdiff_Sup:\n",
    "    \n",
    "    def __init__(self,I,inner_steps,bs):\n",
    "        \n",
    "        self.I=I\n",
    "        self.inner_steps=inner_steps\n",
    "        self.bs=bs\n",
    "        self.max_corr = Max_Corr()\n",
    "        if device == 'cuda':\n",
    "            self.max_corr.cuda()\n",
    "        \n",
    "    def inner_step(self,z1norm,z2norm):\n",
    "    \n",
    "        max_corr=self.max_corr\n",
    "        I=self.I\n",
    "        bs=self.bs\n",
    "        inner_steps=self.inner_steps\n",
    "\n",
    "        z1norm=z1norm.detach()\n",
    "        z2norm=z2norm.detach()\n",
    "\n",
    "        # z1norm=z1norm[:,0]\n",
    "        # z2norm=z2norm[:,0]\n",
    "\n",
    "        max_corr = Max_Corr()\n",
    "        max_corr.cuda()\n",
    "    \n",
    "        # for p in max_corr.parameters():\n",
    "        #     p.requires_grad=True]\n",
    "\n",
    "        optimizer = torch.optim.Adam(list(max_corr.parameters()),lr=0.001)\n",
    "        for i in range(inner_steps):\n",
    "            z1norm_2,z2norm_2=max_corr(z1norm,z2norm)\n",
    "            #z1norm_2 = (z1norm_2 - z1norm_2.mean(0)) / z1norm_2.std(0, unbiased=False)\n",
    "        \n",
    "            assert (z1norm_2.shape,z2norm_2.shape) == (z1norm.shape,z2norm.shape)\n",
    "\n",
    "            # Ctem1 =  (z1norm.T @ z2norm_2) / bs\n",
    "            # Ctem2 = (z1norm_2.T @ z2norm) / bs\n",
    "            # cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2))\n",
    "\n",
    "            cdiff_2 = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)\n",
    "            #cdiff_2=Ctem.pow(2)\n",
    "\n",
    "            inner_loss=-1*(cdiff_2*(1-I)).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            inner_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        for p in max_corr.parameters():\n",
    "            p.requires_grad=False\n",
    "            \n",
    "        return max_corr\n",
    "    \n",
    "    def __call__(self,z1norm,z2norm):\n",
    "        \n",
    "            max_corr =  self.inner_step(z1norm,z2norm)\n",
    "            z1norm_2,z2norm_2 = max_corr(z1norm,z2norm)\n",
    "      \n",
    "            cdiff_sup = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)\n",
    "    \n",
    "            return cdiff_sup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "a2Exs2s3ag8z"
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class BarlowTwins(Callback):\n",
    "    order,run_valid = 9,True\n",
    "    def __init__(self, aug_pipelines, lmb=5e-3, print_augs=False):\n",
    "        assert_aug_pipelines(aug_pipelines)\n",
    "        self.aug1, self.aug2 = aug_pipelines\n",
    "        if print_augs: print(self.aug1), print(self.aug2)\n",
    "        store_attr('lmb')\n",
    "\n",
    "        self.inner_steps=4\n",
    "        \n",
    "    def before_fit(self): \n",
    "        self.learn.loss_func = self.lf\n",
    "        nf = self.learn.model.projector[-1].out_features\n",
    "        self.I = torch.eye(nf).to(self.dls.device)\n",
    "\n",
    "    def update_seed(self):\n",
    "        \n",
    "        indexmod=2\n",
    "        if self.index%indexmod == 0: #every `indexmod` index update the seed (best we have found so far)\n",
    "            self.seed = np.random.randint(0,10000)\n",
    "\n",
    "    def before_epoch(self):\n",
    "        self.index=-1\n",
    "\n",
    "        if self.epoch%10==0:\n",
    "            self.inner_steps += 1\n",
    "            \n",
    "    def before_batch(self):\n",
    "        xi,xj = self.aug1(self.x), self.aug2(self.x)\n",
    "        self.learn.xb = (torch.cat([xi, xj]),)\n",
    "\n",
    "        self.index=self.index+1\n",
    "\n",
    "        self.update_seed()\n",
    "\n",
    "        \n",
    "        #Uncomment to run standard BT\n",
    "    # def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
    "    #     bs,nf = pred.size(0)//2,pred.size(1)\n",
    "\n",
    "    #     z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "\n",
    "    #     z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "    #     z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "        \n",
    "    #     C = (z1norm.T @ z2norm) / bs \n",
    "    #     cdiff = (C - self.I)**2\n",
    "    #     loss = (cdiff*self.I + cdiff*(1-self.I)*self.lmb).sum() \n",
    "    #     return loss\n",
    "\n",
    "\n",
    "    def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
    "        bs,nf = pred.size(0)//2,pred.size(1)\n",
    "\n",
    "        #All standard, from BT\n",
    "        z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "        z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "        z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "        \n",
    "        C = (z1norm.T @ z2norm) / bs \n",
    "        cdiff = (C - self.I)**2\n",
    "\n",
    "\n",
    "\n",
    "        # #Let's change this block to rewritten (should do same thing)\n",
    "        # max_corr = inner_step(z1norm,z2norm,I=self.I,inner_steps=5)#,inner_steps=self.inner_steps)\n",
    "        # z1norm_2,z2norm_2 = max_corr(z1norm,z2norm)\n",
    "        # Ctem1 =  (z1norm.T @ z2norm_2) / bs\n",
    "        # Ctem2 = (z1norm_2.T @ z2norm) / bs\n",
    "        # #Ctem = (z1norm_2.T @ z2norm_2) / bs\n",
    "        # #cdiff_2 = Ctem.pow(2)\n",
    "        # cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2)) #+ 0.1*Ctem.pow(2)\n",
    "\n",
    "\n",
    "        CdiffSup = Cdiff_Sup(I=self.I,inner_steps=5,bs=bs)\n",
    "        cdiff_2 = CdiffSup(z1norm,z2norm)\n",
    "\n",
    "        CdiffRand = Cdiff_Rand(seed=self.seed,bs=bs,std=0.2,K=2)\n",
    "        cdiff_2_2 = CdiffRand(z1norm,z2norm)\n",
    "\n",
    "        # #Let's change this block to rewritten (should do same thing)\n",
    "        # K=2\n",
    "        # cdiff_2_2=0\n",
    "        # for i in range(K):\n",
    "        #     #p=Exp_sample(loc=1.5,scale=2.0)\n",
    "        #     #p=Unif(0.9,2.5)\n",
    "        #     #z1norm_2 = p_norm(z1norm,p=p)\n",
    "\n",
    "        #     #p=Exp_sample(loc=1.5,scale=2.0)\n",
    "        #     #p=Unif(0.9,2.5)\n",
    "        #     #z2norm_2 = p_norm(z2norm,p=p)\n",
    "\n",
    "        #     z1norm_2 = random_sinusoid(z1norm,std=0.1,seed=self.seed+i)\n",
    "        #     z2norm_2 = random_sinusoid(z2norm,std=0.1,seed=2*self.seed+i)\n",
    "\n",
    "        #     #C = (z1norm_2.T @ z2norm_2) / bs\n",
    "        #     C_1 = (z1norm.T @ z2norm_2) / bs\n",
    "        #     C_2 = (z1norm_2.T @ z2norm) / bs\n",
    "\n",
    "        #     #cdiff_2 = 0.5*(C_1).pow(2) + 0.5*(C_2).pow(2)\n",
    "        #     #cdiff_2_2 = cdiff_2_2 + cdiff_2\n",
    "        #     cdiff_2_2 = cdiff_2_2 + 0.5*C_1.pow(2)+0.5*C_2.pow(2)\n",
    "        \n",
    "        # cdiff_2_2=(1/K)*cdiff_2_2\n",
    "\n",
    "\n",
    "        cdiff_2 = 0.5*cdiff_2_2 + 0.5*cdiff_2\n",
    "            \n",
    "            \n",
    "        l2 = cdiff_2*(1-self.I)*self.lmb #Is either the standard term - or not.\n",
    "\n",
    "\n",
    "        loss = (cdiff*self.I + l2).sum()\n",
    "        torch.cuda.empty_cache()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def show(self, n=1):\n",
    "        bs = self.learn.x.size(0)//2\n",
    "        x1,x2  = self.learn.x[:bs], self.learn.x[bs:] \n",
    "        idxs = np.random.choice(range(bs),n,False)\n",
    "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n",
    "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n",
    "        images = []\n",
    "        for i in range(n): images += [x1[i],x2[i]] \n",
    "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vbS1WtLiag80",
    "outputId": "c469ea1d-18a9-49a3-bf1d-bdeefe789cd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>114.182457</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>72.701714</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>50.954372</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>38.303207</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>30.082090</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>25.593954</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>21.243341</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>17.654123</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>14.494215</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>14.207288</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>14.445746</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>12.680450</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>11.417379</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>10.896379</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>9.451530</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>8.287415</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>7.724482</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>7.120658</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>6.771980</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>8.284552</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>9.812275</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>8.789126</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>7.867405</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>6.749163</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>5.977350</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>5.598652</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>5.363124</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>5.143140</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>5.258729</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>6.457084</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.180355</td>\n",
       "      <td>None</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>5.779800</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>5.362846</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>4.752480</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>4.309053</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>4.253700</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.948820</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.816268</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>4.484355</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>4.571619</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.506097</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>4.482444</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>4.014374</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.637718</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.527894</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.317650</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.163520</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.686815</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>4.565703</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>4.271856</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.995637</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.655202</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.230526</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.972187</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.850840</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.666236</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.711145</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.863390</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.750707</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.803160</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.985286</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.780259</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.592879</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>2.555425</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.483386</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.418040</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.722931</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>3.244174</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.970295</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>3.021848</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.827740</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.483430</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.357151</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.294851</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.119609</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.090992</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.049364</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.974623</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.134863</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.403521</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.218940</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.052779</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.040607</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1.937630</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.866547</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.012180</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.274429</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.181250</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.221633</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.032379</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.866962</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.801539</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.754134</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.665959</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>1.649742</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.780502</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1.800673</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.881401</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1.938303</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.772448</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.641052</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.672266</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.576836</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.556239</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>1.647277</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.879973</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.832054</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>1.773372</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.621994</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.498591</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.483575</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>1.456506</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.417044</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.472541</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>1.631420</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.551221</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>1.631723</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.651435</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.507700</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.389638</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.409607</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>1.310202</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>1.293356</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>1.300058</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>1.236682</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.286955</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>1.519489</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>1.452597</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>1.371741</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>1.352545</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.285387</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>1.235670</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>1.508692</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>1.576052</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>1.408549</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.425310</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>1.361339</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>1.239687</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>1.208207</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>1.184081</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.113524</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>1.138775</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>1.139396</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>1.114036</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>1.156173</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.253129</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>1.172933</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>1.126209</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>1.148908</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>1.092357</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.126868</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>1.215163</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>1.224793</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>1.143547</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.217522</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.152001</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>1.071696</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>1.063171</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>1.039535</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.986403</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.018660</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>1.001738</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.986611</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>1.057943</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>1.190038</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.119198</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>1.063609</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>1.102506</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>1.020749</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.978344</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.955429</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.913549</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.917025</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.998493</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.983900</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.971090</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1.001431</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>1.009095</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.993213</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.986372</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.972211</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.925100</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.960600</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.984197</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.939248</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.914574</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.950489</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.894401</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.893025</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>1.181287</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.231542</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>1.202979</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>1.149298</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>1.014230</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.924835</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.939302</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.898284</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.872086</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.872007</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.826338</td>\n",
       "      <td>None</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Debugging cell - delete later (similar to cell below)\n",
    "ps=500\n",
    "hs=500\n",
    "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
    "model = create_barlow_twins_model(fastai_encoder, hidden_size=hs,projection_size=ps)# projection_size=1024)\n",
    "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
    "#values for these which is tantamount to doing nothing\n",
    "#So if we choose resize_scale=(1,1) then the images look the same.\n",
    "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
    "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=True)\n",
    "#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwRMSProp(model.parameters(),lr=0.1, mom=0.9)ins(aug_pipelines, print_augs=True)])\n",
    "opt = torch.optim.RMSprop\n",
    "#partial(OptimWrapper, opt=opt)\n",
    "learn = Learner(dls,model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
    "#learn = Learner(dls, model,opt_func=opt_func, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
    "\n",
    "learn.fit(200) #300                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Y5FN3Safag80"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"\"\n",
    "    Seed everything.\n",
    "    \"\"\"   \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def tune_set(items0,tune_s=1000):\n",
    "    \n",
    "    items0=items0.shuffle()\n",
    "    d = {'0':0,'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0,'8':0,'9':0}\n",
    "    ITEMS=[]\n",
    "    for i in items0:\n",
    "        s=str(i).split('/training/')[1][0]\n",
    "        if d[s] is 0 or d[s] is 1:\n",
    "            ITEMS.append(i)\n",
    "            d[s]+=1\n",
    "    #items0=ITEMS\n",
    "\n",
    "    for i in items0:\n",
    "        if i not in ITEMS:\n",
    "            ITEMS.append(i)\n",
    "            \n",
    "    split = IndexSplitter(list(range(20)))\n",
    "\n",
    "    tds_tune = Datasets(ITEMS, [PILImageBW.create, [parent_label, Categorize()]], splits=split(ITEMS)) #Or do we want this?\n",
    "    dls_tune = tds_tune.dataloaders(bs=20, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
    "    \n",
    "    return dls_tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xv4RE3O0ag81",
    "outputId": "b49606e1-02e8-4568-f303-8d7a5c496b3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assert passed\n"
     ]
    }
   ],
   "source": [
    "#TODO: Do this in a slicker way. Write more tests \n",
    "#Get the dataloader and set batch size \n",
    "ts=16384 #training set size - most everything\n",
    "bs=512\n",
    "device='cuda'\n",
    "path = untar_data(URLs.MNIST)\n",
    "\n",
    "items = get_image_files(path/'training') #i.e. NOT testing!!!\n",
    "items.sort() \n",
    "\n",
    "seed=42\n",
    "seed_everything(seed=seed)\n",
    "labeller = using_attr(RegexLabeller(pat = r'(\\d+).png$'), 'name') \n",
    "\n",
    "items=items.shuffle()\n",
    "items1 = items[0:ts] #train on these guys\n",
    "\n",
    "l=labeller(items1[0])\n",
    "if seed is 42:\n",
    "    print('assert passed')\n",
    "    assert labeller(items1[0]) == '19825' #check that random seed is working\n",
    "\n",
    "else:\n",
    "    input('Careful! New random seed ~= 42. Is that ok?')\n",
    "\n",
    "split = RandomSplitter(valid_pct=0.0) #randomly split training set into training and validation\n",
    "#tds = Datasets(items,splits=split(items)) #Do we want this?\n",
    "tds = Datasets(items1, [PILImageBW.create, [parent_label, Categorize()]], splits=split(items1)) #Or do we want this?\n",
    "dls = tds.dataloaders(bs=bs,num_workers=6, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
    "\n",
    "tune_s=2000 #we choose 20 guys (randomly) out of 1000 to tune on\n",
    "items0 = items[ts:ts+tune_s] #for fine tuning - just choose 2000 guys to extract 20 for fine tuning \n",
    "dls_tune=tune_set(items0,tune_s=tune_s)\n",
    "\n",
    "\n",
    "#NB: Uncomment and compare in colab and kaggle\n",
    "# for x,y in dls_tune.train:\n",
    "#   print(x.mean())\n",
    "#   input()\n",
    "#   break\n",
    "\n",
    "# for x,y in dls_tune.train:\n",
    "#   print(x.mean())\n",
    "#   input()\n",
    "#   break\n",
    "\n",
    "\n",
    "#Evaluate linear classifier on this guy\n",
    "test_bs=578\n",
    "items2 = items[ts+tune_s:]\n",
    "split = RandomSplitter(valid_pct=0.0) #randomly split training set into training and validation\n",
    "tds_test = Datasets(items2, [PILImageBW.create, [parent_label, Categorize()]], splits=split(items2)) #Or do we want this?\n",
    "dls_test = tds_test.dataloaders(bs=test_bs, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
    "\n",
    "#Check that test_bs divides length of test set\n",
    "tem=len(dls_test.train_ds)/test_bs\n",
    "assert tem-math.floor(tem) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "AKbw2pxMag82",
    "outputId": "07e65765-a9ae-42b5-9e8a-c05f4dab18b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#A \"reasonable\" composite augmentation: initially copy pasted BT. We run this cell a few times to check it makes sense\n",
    "#Also define encoder and model\n",
    "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
    "model = create_barlow_twins_model(fastai_encoder, hidden_size=10,projection_size=10)# projection_size=1024)\n",
    "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
    "#values for these which is tantamount to doing nothing\n",
    "#So if we choose resize_scale=(1,1) then the images look the same.\n",
    "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
    "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=False)\n",
    "#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
    "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
    "\n",
    "#dls.valid.bs = len(dls.valid_ds) #Set the validation dataloader batch size to be the length of the validation dataset\n",
    "\n",
    "b = dls.one_batch()\n",
    "learn._split(b)\n",
    "learn('before_batch')\n",
    "axes = learn.barlow_twins.show(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "Kujpa-Lvag82"
   },
   "outputs": [],
   "source": [
    "#Simple linear classifier\n",
    "class LinearClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,zdim):\n",
    "            \n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(zdim,10) #As 10 classes for mnist\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = cast(self.fc1(x),Tensor) #so we have to use cross entropy loss. cast is because using old version fastai \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "QJTzqSefag83"
   },
   "outputs": [],
   "source": [
    "#NB: Will give same random 20-tune set (for fixed random seed), only if the cell\n",
    "#\"#Get the dataloader and set batch size\" is the same. Perhaps later we can make this cell a function of that one. \n",
    "#Functions to train and evaluate head\n",
    "fastai_encoder.eval()\n",
    "\n",
    "def train_head(seed=10): #The seed choses a different (20) samples for training the head. 2 of each class\n",
    "\n",
    "    seed_everything(seed=seed)\n",
    "    dls_tune=tune_set(items0,tune_s=tune_s) #different random tune set each time (but random seed same for consistency)\n",
    "  \n",
    "    zdim=1024 #see above\n",
    "    head = LinearClassifier(zdim=zdim)\n",
    "    device='cuda'\n",
    "    head.to(device)\n",
    "    optimizer = torch.optim.Adam(head.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    #EPOCHS=100\n",
    "\n",
    "    for epoch in range(200):\n",
    "\n",
    "        #for x,y in dls_tune.valid: #Slows massively on colab but not on kaggle. Weird. \n",
    "        x,y=dls_tune.valid.one_batch() #Same every time since dataset only has length=batch size = 20\n",
    "  \n",
    "        loss = criterion(head(fastai_encoder(x)),y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(loss)\n",
    "\n",
    "    return head\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_head(head):\n",
    "\n",
    "    N=len(dls_test.train)*test_bs #close to len(dls_test.train_ds) but not quite...\n",
    "\n",
    "    assert N == len(dls_test.train_ds)\n",
    "\n",
    "    num_correct=0\n",
    "\n",
    "    for x,y in dls_test.train:\n",
    "    #for i in range(3):\n",
    "\n",
    "        ypred = head(fastai_encoder(x))\n",
    "        correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n",
    "        num_correct += correct.sum()\n",
    "    \n",
    "    return num_correct/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VULhbDWawO_J",
    "outputId": "ae006616-72c1-4bae-c6c9-c6f2f6ba0936"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8393)\n",
      "CPU times: user 41 s, sys: 4.95 s, total: 45.9 s\n",
      "Wall time: 1min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'seed_0': TensorCategory(0.8306),\n",
       " 'seed_1': TensorCategory(0.7650),\n",
       " 'seed_2': TensorCategory(0.9018),\n",
       " 'seed_3': TensorCategory(0.8488),\n",
       " 'seed_4': TensorCategory(0.8502)}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "seed=10\n",
    "performance_dict={}\n",
    "for num in range(5):\n",
    "\n",
    "    head=train_head(seed=seed+num)\n",
    "    pct_correct = eval_head(head)\n",
    "    performance_dict[f'seed_{num}'] = pct_correct\n",
    "    \n",
    "print(torch.mean(tensor(list(performance_dict.values()))))\n",
    "performance_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "NpcqDLMJWpkB",
    "outputId": "f318047d-bce9-4a9b-9995-96faaeb3f966"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-128373cac84b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Current: With Max_cor = (sigmoid,relu), and fc1=fc3, fc2=fc4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperformance_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mperformance_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "#Current: With Max_cor = (sigmoid,relu), and fc1=fc3, fc2=fc4\n",
    "\n",
    "print(torch.mean(tensor(list(performance_dict.values()))))\n",
    "performance_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjR-EnBubM6y"
   },
   "source": [
    "Please see commit  e849943... 4/10/22 if needed. We have edited the base functions in that file to try and make them nicer, but we need to make sure we can reproduce results (i.e. the changes make things nicer but don't actually change anything).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4bY6DPjb8MY"
   },
   "source": [
    "With: fc1,fc2,fc3,fc4 distinct. Indexmod=2, K=2 With Max_corr = (sigmoid,relu); \n",
    "$a\\sim b$ = 0.2 x N(0,1): 0.8576,0.8364,0.8393\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjPF34iucYWK"
   },
   "source": [
    "Below applies to prior implementation, we leave it here for now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Rmtuu4nxc13"
   },
   "source": [
    "All of the below have sin and cos with constant coefficients. If we take `best so far` and give it random coefficients a and b with std=0.2 then get:\n",
    "\n",
    "With fc1,fc2,fc3,fc4 distinct. Indexmod=2, K=2 With Max_corr = (sigmoid,relu);a~b = 0.2 x N(0,1) **0.8390, 0.8553** Conclusion: We need to search over the a and b parameters (coefficients of sinusoids) when we do our big search. Or rather search over std the hps controlling how we sample a and b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SYRjUytI8Fw"
   },
   "source": [
    "Results (continuing from prior commit):\n",
    "\n",
    "\n",
    "Note these are with 200 learn_epochs etc. Same random seed. See above for other details (and we mention when recording the results below that we varied)\n",
    "BT = 0.7581\n",
    "\n",
    "(These are with fc1,fc2,fc3,fc4 distinct). Indexmod=2, K=2 \n",
    "**With Max_corr = (sigmoid,relu): 0.8493,0.8332,0.8392. Best so far.**\n",
    "With Max_corr = (sigmoid,sigmoid): 0.3080,0.3219.   \n",
    "With Max_corr = (relu,relu): 0.8132,0.8142,0.8093.   \n",
    "\n",
    "(These are with fc1=fc3, fc2=fc4. i.e. just one NN applied)  \n",
    "With Max_corr = (sigmoid,relu): 0.8359,0.8258,0.8303\n",
    "\n",
    "Above are all with indexmod=2. Now we try removing the indexmod condition.\n",
    "Then get:\n",
    "Indexmod=0\n",
    "With Max_corr = (sigmoid,relu): 0.8264,0.8253 \n",
    "\n",
    "Try indexmod=4\n",
    "With Max_corr = (sigmoid,relu):0.8174\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snRrKfwCH6XD"
   },
   "source": [
    "Results on different MBT runs (see above for implementation): tensor(0.8493) (trying to reproduce now) (just changed y=self.sigmoid(self.fc3(y))\n",
    "from relu in MaxCorr). Performance went to tensor(0.3080)!!! Crazy. Let's change y back to relu and see what happens. result: tensor(0.8332)!! Wow. Similar\n",
    "to before (i.e. evidence in favour of reproducibility). All we changed was sigmoid to relu!\n",
    "\n",
    "To summarise: Max_Corr had (sigmoid,relu) and got great results (on two diff MBT runs 0.8493 and 0.8332. When we changed relu to sigmoid, got terrible results (0.3080). (As an aside the loss jumped around a lot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPEDQE-rcKjL",
    "outputId": "8e127c85-873f-4a36-d76a-66a57c566f74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8252)\n"
     ]
    }
   ],
   "source": [
    "#250 learn_epochs. To beat: (K=10,indexmod=4,std=0.1,convex=(0.2,0.8))\n",
    "tem={'seed_0': TensorCategory(0.8364),\n",
    " 'seed_1': TensorCategory(0.7424),\n",
    " 'seed_2': TensorCategory(0.8947),\n",
    " 'seed_3': TensorCategory(0.8392),\n",
    " 'seed_4': TensorCategory(0.8133)}\n",
    "print(torch.mean(tensor(list(tem.values())))) #tensor(0.8252)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZPO88U4XMgb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYI6BiCTXCmt",
    "outputId": "c99dc95d-0a71-4fc3-d791-16cee98bfa8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8487)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 169
    },
    "id": "hw0VjYxRx1va",
    "outputId": "659f5266-600d-4b32-a834-a84fff7d18b3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c8c02caa2cf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperformance_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'performance_dict' is not defined"
     ]
    }
   ],
   "source": [
    "performance_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COooHESjoNGb"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
