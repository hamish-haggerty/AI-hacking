{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamish-haggerty/AI-hacking/blob/master/SSL/BT_MNIST_new2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8jFsEXz_61O",
        "outputId": "ba8efc1e-3582-4a54-b183-98af734334e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 8.6 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.12.0\n",
            "  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 269 kB/s \n",
            "\u001b[?25hCollecting torchaudio==0.11.0\n",
            "  Downloading torchaudio-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 92.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (2022.6.15)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.12.1+cu113\n",
            "    Uninstalling torchaudio-0.12.1+cu113:\n",
            "      Successfully uninstalled torchaudio-0.12.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.11.0 torchaudio-0.11.0 torchvision-0.12.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastai==2.6.3\n",
            "  Downloading fastai-2.6.3-py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 14.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: fastai\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 2.7.9\n",
            "    Uninstalling fastai-2.7.9:\n",
            "      Successfully uninstalled fastai-2.7.9\n",
            "Successfully installed fastai-2.6.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting self_supervised\n",
            "  Downloading self_supervised-1.0.4-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 566 kB/s \n",
            "\u001b[?25hCollecting kornia>=0.5.0\n",
            "  Downloading kornia-0.6.7-py2.py3-none-any.whl (565 kB)\n",
            "\u001b[K     |████████████████████████████████| 565 kB 25.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from self_supervised) (21.3)\n",
            "Collecting timm>=0.4.5\n",
            "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[K     |████████████████████████████████| 509 kB 96.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fastai>=2.2.7 in /usr/local/lib/python3.7/dist-packages (from self_supervised) (2.6.3)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from self_supervised) (21.1.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (6.0)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.0.3)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (0.0.7)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (3.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (3.2.2)\n",
            "Requirement already satisfied: torch<1.12,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.11.0)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (0.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.7.3)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (7.1.2)\n",
            "Collecting fastcore<1.5,>=1.3.27\n",
            "  Downloading fastcore-1.4.5-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 9.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.0.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.9.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.4.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.10)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.64.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.6.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.21.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (8.1.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.1.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.11.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.4.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4->fastai>=2.2.7->self_supervised) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->self_supervised) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4->fastai>=2.2.7->self_supervised) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai>=2.2.7->self_supervised) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4->fastai>=2.2.7->self_supervised) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4->fastai>=2.2.7->self_supervised) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (1.4.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai>=2.2.7->self_supervised) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai>=2.2.7->self_supervised) (2022.2.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (1.1.0)\n",
            "Installing collected packages: fastcore, timm, kornia, self-supervised\n",
            "  Attempting uninstall: fastcore\n",
            "    Found existing installation: fastcore 1.5.25\n",
            "    Uninstalling fastcore-1.5.25:\n",
            "      Successfully uninstalled fastcore-1.5.25\n",
            "Successfully installed fastcore-1.4.5 kornia-0.6.7 self-supervised-1.0.4 timm-0.6.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.15.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (8.14.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (22.1.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest) (57.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipytest\n",
            "  Downloading ipytest-0.12.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (21.3)\n",
            "Collecting pytest>=5.4\n",
            "  Downloading pytest-7.1.3-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 13.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (4.12.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (22.1.0)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (3.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (57.4.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 85.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipytest) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipytest) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipytest) (0.7.0)\n",
            "Installing collected packages: pluggy, jedi, iniconfig, pytest, ipytest\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed iniconfig-1.1.1 ipytest-0.12.0 jedi-0.18.1 pluggy-1.0.0 pytest-7.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0\n",
        "!pip install fastai==2.6.3 --no-deps\n",
        "!pip install self_supervised\n",
        "\n",
        "!pip install pytest\n",
        "!pip install ipytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BOv4kkJDag8r",
        "outputId": "59cbf232-ed01-4474-f024-e59c28b0bfce"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Pk01WY_Dag8s"
      },
      "outputs": [],
      "source": [
        "import fastai\n",
        "import self_supervised\n",
        "import torch\n",
        "assert(fastai.__version__ == '2.6.3') #Check that version is 2.6.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AOjr_YCLag8t"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *\n",
        "from self_supervised.augmentations import *\n",
        "from self_supervised.layers import *\n",
        "import inspect\n",
        "import warnings\n",
        "import random\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#from Base_Stein.SVGD_classes import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XTSdKC6bag8t"
      },
      "outputs": [],
      "source": [
        "#Like most other SSL algorithms BT's model consists of an encoder and projector (MLP) layer.\n",
        "#Definition is straightforward:\n",
        "#https://colab.research.google.com/github/KeremTurgutlu/self_supervised/blob/master/nbs/14%20-%20barlow_twins.ipynb#scrollTo=1M6QcUChcvpz\n",
        "class BarlowTwinsModel(Module):\n",
        "    \"\"\"An encoder followed by a projector\n",
        "    \"\"\"\n",
        "    def __init__(self,encoder,projector):self.encoder,self.projector = encoder,projector\n",
        "        \n",
        "    def forward(self,x): return self.projector(self.encoder(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZL3EE07Pag8u"
      },
      "outputs": [],
      "source": [
        "#HOWEVER instead of directly using the above, by passing both an encoder and a projector, create_barlow_twins_model\n",
        "#function can be used by minimally passing a predefined encoder and the expected input channels.\n",
        "\n",
        "#In the paper it's mentioned that MLP layer consists of 3 layers... following function will create a 3 layer\n",
        "#MLP projector with batchnorm and ReLU by default. Alternatively, you can change bn and nlayers. \n",
        "\n",
        "#Questions: Why torch.no_grad() when doing this?\n",
        "def create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n",
        "    \"Create Barlow Twins model\"\n",
        "    n_in  = in_channels(encoder)\n",
        "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
        "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n",
        "    apply_init(projector)\n",
        "    return BarlowTwinsModel(encoder, projector)\n",
        "\n",
        "#Similar to above. Simple API to make the BT model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DFjGL-COag8v"
      },
      "outputs": [],
      "source": [
        "#BarlowTwins Callback\n",
        "#The following parameters can be passed:\n",
        "# - aug_pipelines\n",
        "# Imb lambda is the weight for redundancy reduction term in the loss function\n",
        "\n",
        "@delegates(get_multi_aug_pipelines)\n",
        "def get_barlow_twins_aug_pipelines(size,**kwargs): return get_multi_aug_pipelines(n=2,size=size,**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx4KsywAag8v"
      },
      "outputs": [],
      "source": [
        "#Uniform random number between a and b\n",
        "def Unif(a,b):\n",
        "    return (b-a)*torch.rand(1).item()+a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v0NL3f1ag8v"
      },
      "outputs": [],
      "source": [
        "def random_polynomial(A):\n",
        "    \n",
        "    #B=torch.normal(mean=0, std=0.025, size=(1, 1)).item() #First Horner term (and is coefficient of x^4)\n",
        "    \n",
        "    #Btem = torch.normal(mean=0,std=0.05, size=(1, 1)).item() #Sample coefficient of x^3\n",
        "    #B = Btem + B*A #Third Horner term\n",
        "    \n",
        "    B = torch.normal(mean=0,std=0.5, size=(1, 1)).item() #Sample coefficient of x^2\n",
        "    \n",
        "#     Btem = torch.normal(mean=0,std=0.5, size=(1, 1)).item() #Sample coefficient of x^2\n",
        "#     B = Btem + B*A #Third Horner term\n",
        "    \n",
        "    Btem = random.choice([-1,1])*torch.normal(mean=0,std=1, size=(1, 1)).item() #Sample coefficient of x\n",
        "    #Btem=1\n",
        "    B = Btem + B*A #Fourth Horner term\n",
        "    \n",
        "    Btem = torch.normal(mean=0,std=1, size=(1, 1)).item() #Sample coefficient of x^0\n",
        "    B = Btem + B*A #Fifth Horner term\n",
        "    \n",
        "    \n",
        "    return B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdWjbdMdag8w"
      },
      "outputs": [],
      "source": [
        "def random_polynomial_bestsofar(A):\n",
        "    \n",
        "    \n",
        "    B=torch.normal(mean=0, std=0.025, size=(1, 1)).item() #First Horner term (and is coefficient of x^4)\n",
        "    \n",
        "    Btem = torch.normal(mean=0,std=0.05, size=(1, 1)).item() #Sample coefficient of x^3\n",
        "    B = Btem + B*A #Third Horner term\n",
        "    \n",
        "    Btem = torch.normal(mean=0,std=0.5, size=(1, 1)).item() #Sample coefficient of x^2\n",
        "    B = Btem + B*A #Third Horner term\n",
        "    \n",
        "    Btem = random.choice([-1,1])*torch.normal(mean=1,std=2, size=(1, 1)).item() #Sample coefficient of x\n",
        "    B = Btem + B*A #Fourth Horner term\n",
        "    \n",
        "    Btem = torch.normal(mean=0,std=1, size=(1, 1)).item() #Sample coefficient of x^0\n",
        "    B = Btem + B*A #Fifth Horner term\n",
        "    \n",
        "    \n",
        "    return B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_nknWDoag8x"
      },
      "outputs": [],
      "source": [
        "def low_deg(A):\n",
        "    power=Unif(1,1.25)\n",
        "    coeff2 = torch.normal(mean=0, std=1, size=(1, 1)).item() #degree 2 term\n",
        "    coeff1 = torch.normal(mean=1, std=1, size=(1, 1)).item() #degree 2 term\n",
        "    \n",
        "#     coeff2 = torch.normal(mean=0, std=1, size=(1, 1)).item() #degree 2 term\n",
        "#     coeff1 = torch.normal(mean=1, std=0.7, size=(1, 1)).item() #degree 2 term\n",
        "\n",
        "#     coeff2 = torch.normal(mean=0, std=1, size=(1, 1)).item() #degree 2 term\n",
        "#     coeff1 = torch.normal(mean=0, std=1, size=(1, 1)).item() #degree 2 term\n",
        "\n",
        "    \n",
        "    B = (coeff1*A + coeff2*torch.abs(A).pow(power))\n",
        "    \n",
        "    #B = (1/power)*torch.abs(A).pow(power)\n",
        "    return B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtX22j1Vag8y"
      },
      "outputs": [],
      "source": [
        "def random_quintic(A):\n",
        "\n",
        "    \n",
        "    B=torch.normal(mean=0, std=0.125, size=(1, 1)).item() #First Horner term (and is coefficient of x^4)\n",
        "    \n",
        "    \n",
        "    Btem=torch.normal(mean=0, std=0.25, size=(1, 1)).item()#Sample coefficient of x^3\n",
        "    B = Btem + B*A #Second Horner term\n",
        "    \n",
        "    Btem = torch.normal(mean=0,std=0.5, size=(1, 1)).item() #Sample coefficient of x^2\n",
        "    B = Btem + B*A #Third Horner term\n",
        "    \n",
        "    Btem = random.choice([-1,1])*torch.normal(mean=1,std=2, size=(1, 1)).item() #Sample coefficient of x\n",
        "    B = Btem + B*A #Fourth Horner term\n",
        "    \n",
        "    Btem = torch.normal(mean=0,std=1, size=(1, 1)).item() #Sample coefficient of x^0\n",
        "    B = Btem + B*A #Fifth Horner term\n",
        "    \n",
        "    \n",
        "    return B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "l6gBw66Iag8y"
      },
      "outputs": [],
      "source": [
        "def random_sinusoid(x,std=0.15,seed=0):\n",
        "\n",
        "    seed_everything(seed=seed)\n",
        "    \n",
        "    t=torch.normal(mean=0,std=std,size=(1,1)).item()\n",
        "    s=torch.normal(mean=0,std=std,size=(1,1)).item()\n",
        "    \n",
        "    u=torch.normal(mean=0,std=1,size=(1,1)).item()\n",
        "    v=torch.normal(mean=0,std=1,size=(1,1)).item()\n",
        "    \n",
        "    a=torch.normal(mean=0,std=1,size=(1,1)).item()\n",
        "    b=torch.normal(mean=0,std=1,size=(1,1)).item()\n",
        "    \n",
        "    return torch.sin(t*math.pi*x+u) + torch.cos(s*math.pi*x + v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnodDXFoag8z"
      },
      "outputs": [],
      "source": [
        "def poly_sinusoid(x):\n",
        "    \n",
        "    return (x) + 0.2*random_sinusoid(x,std=(0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "li3BQ1ukX0XB"
      },
      "outputs": [],
      "source": [
        "def Exp_sample(scale=3.,loc=1.8):\n",
        "    \n",
        "    Expo = torch.distributions.exponential.Exponential\n",
        "    E = Expo(torch.tensor([scale]))\n",
        "#     if random.random()<0.5:\n",
        "#         return 1\n",
        "#     else:\n",
        "    return loc+E.sample().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "g7ZZMQJmRqMH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb3da90-9c4d-4f7a-94ec-1ebae3af8968"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "471"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "np.random.randint(0,1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MDYVO2u1U4We"
      },
      "outputs": [],
      "source": [
        "def p_norm(x,p):\n",
        "\n",
        "    eps=1e-8\n",
        "    if p<1:\n",
        "        x=x+eps\n",
        "    return (1/p)*torch.abs(x).pow(p)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def update_seed(seed,epoch,index):\n",
        "\n",
        "    if index%4: \n",
        "        return epoch+seed\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "ntNkDyg768e2"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "a2Exs2s3ag8z"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class BarlowTwins(Callback):\n",
        "    order,run_valid = 9,True\n",
        "    def __init__(self, aug_pipelines, lmb=5e-3, print_augs=False):\n",
        "        assert_aug_pipelines(aug_pipelines)\n",
        "        self.aug1, self.aug2 = aug_pipelines\n",
        "        if print_augs: print(self.aug1), print(self.aug2)\n",
        "        store_attr('lmb')\n",
        "        \n",
        "    def before_fit(self): \n",
        "        self.learn.loss_func = self.lf\n",
        "        nf = self.learn.model.projector[-1].out_features\n",
        "        self.I = torch.eye(nf).to(self.dls.device)\n",
        "\n",
        "    def update_seed(self):\n",
        "\n",
        "        if self.index%4 == 0: #every fourth index update the seed (best we have found so far)\n",
        "\n",
        "            self.seed = np.random.randint(0,10000)\n",
        "\n",
        "    def before_epoch(self):\n",
        "        self.index=-1\n",
        "            \n",
        "    def before_batch(self):\n",
        "        xi,xj = self.aug1(self.x), self.aug2(self.x)\n",
        "        self.learn.xb = (torch.cat([xi, xj]),)\n",
        "\n",
        "        self.index=self.index+1\n",
        "\n",
        "        self.update_seed()\n",
        "\n",
        "        \n",
        "        #Uncomment to run standard BT\n",
        "    # def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
        "    #     bs,nf = pred.size(0)//2,pred.size(1)\n",
        "\n",
        "    #     z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
        "\n",
        "    #     z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
        "    #     z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
        "        \n",
        "    #     C = (z1norm.T @ z2norm) / bs \n",
        "    #     cdiff = (C - self.I)**2\n",
        "    #     loss = (cdiff*self.I + cdiff*(1-self.I)*self.lmb).sum() \n",
        "    #     return loss\n",
        "\n",
        "\n",
        "    def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
        "        bs,nf = pred.size(0)//2,pred.size(1)\n",
        "\n",
        "        #All standard, from BT\n",
        "        z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
        "        z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
        "        z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
        "        \n",
        "        \n",
        "        C = (z1norm.T @ z2norm) / bs \n",
        "        cdiff = (C - self.I)**2\n",
        "\n",
        "        #{'Pr': 0.45546983480453496, 'dist': 'Exp', 'loc': 0.49445648193359376, 'scale': 2.5828861594200134, 'percent_correct': 0.8277832269668579} #Best so far\n",
        "\n",
        "        #self.update_seed()\n",
        "\n",
        "        #input('hi')\n",
        "\n",
        "        #polyprob=0.1\n",
        "        #polyprob=0.45546\n",
        "        #polyprob=1.0\n",
        "        polyprob=1.0\n",
        "        temrand = random.random()\n",
        "        if temrand < polyprob: #With some probability we want off diag terms to be (quadratic) say.\n",
        "\n",
        "            K=10\n",
        "            cdiff_2=0\n",
        "            for i in range(K):\n",
        "            #p=Exp_sample(loc=1.5,scale=2.0)\n",
        "            # #p=Unif(1.0,2.5)\n",
        "                z1norm_2 = random_sinusoid(z1norm,seed=self.seed+i)\n",
        "                z2norm_2 = z2norm\n",
        "                C_2 = (z1norm_2.T @ z2norm_2) / bs\n",
        "                #cdiff_2 = (C_2)**2 #don't need to subtract I as only looking at off diag terms\n",
        "                cdiff_2 = cdiff_2 + (C_2)**2\n",
        "            \n",
        "            cdiff_2=(1/K)*cdiff_2\n",
        "\n",
        "            #symmetrize loss - so copy paste above block but swap place of 1 and 2\n",
        "            #p=Exp_sample(loc=1.5,scale=2.0)\n",
        "            #p=Unif(1.0,2.5)\n",
        "            \n",
        "            cdiff_2_sym=0\n",
        "            for i in range(K):\n",
        "                z1norm_2 = z1norm\n",
        "                z2norm_2 = random_sinusoid(z2norm,seed=2*self.seed+i)\n",
        "\n",
        "                C_2_sym = (z1norm_2.T @ z2norm_2) / bs\n",
        "                cdiff_2_sym = cdiff_2_sym + C_2_sym**2\n",
        "            \n",
        "            cdiff_2_sym=(1/K)*cdiff_2_sym\n",
        "\n",
        "            cdiff_2 = 0.5*cdiff_2 + 0.5*cdiff_2_sym #Symmetrized random loss\n",
        "\n",
        "            cdiff_2 = 0.2*cdiff +  0.8*cdiff_2 #convex comb of BT loss with random loss -> assumes polyprob=1.0\n",
        "            \n",
        "        else:\n",
        "            cdiff_2 = cdiff\n",
        "            \n",
        "        l2 = cdiff_2*(1-self.I)*self.lmb #Is either the standard term - or not.\n",
        "\n",
        "        loss = (cdiff*self.I + l2).sum() \n",
        "        return loss\n",
        "\n",
        "\n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def show(self, n=1):\n",
        "        bs = self.learn.x.size(0)//2\n",
        "        x1,x2  = self.learn.x[:bs], self.learn.x[bs:] \n",
        "        idxs = np.random.choice(range(bs),n,False)\n",
        "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n",
        "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n",
        "        images = []\n",
        "        for i in range(n): images += [x1[i],x2[i]] \n",
        "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDSTosCQw2Xh"
      },
      "outputs": [],
      "source": [
        "#opt_func = partial(RMSProp, mom=0.9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vbS1WtLiag80",
        "outputId": "b89812be-d18a-4d88-c932-38422d8cdc64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>83.119751</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>62.040192</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>53.689972</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>45.603691</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>37.645702</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>31.377258</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>26.378590</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>23.697859</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>23.641733</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>21.895990</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>20.645563</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>19.156464</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>19.676394</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>17.989386</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>16.363874</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>15.073554</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>15.353421</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>13.951173</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>13.194398</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>12.226227</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>11.582327</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>10.598998</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>11.062474</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>12.242533</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>11.719015</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>10.658706</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>10.028643</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>9.627053</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>9.116250</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>9.012151</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>8.821733</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>8.301952</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>7.957975</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>8.343376</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>8.884116</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>8.640471</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>8.044062</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>8.204444</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>7.277482</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>6.920727</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>6.975362</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>6.721383</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>6.796715</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>6.563899</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>6.913794</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>7.198557</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>6.790254</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>6.225935</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>6.443394</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>5.822672</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>5.587991</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>5.867161</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>5.629492</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>5.323660</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>5.108668</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>5.503509</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>5.626668</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>5.375610</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>5.176741</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>5.369936</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.094254</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>5.165677</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>5.124200</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>4.962611</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>4.709483</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>4.833877</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>5.213385</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>5.074213</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>4.734271</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>4.790353</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>4.600065</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>4.365434</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>4.499856</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>4.358800</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>4.097075</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>4.122776</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>4.237435</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>4.394070</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>4.152189</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>4.136753</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>4.609492</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>4.179654</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>3.971082</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>4.081258</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>3.884886</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>4.011860</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>3.887190</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>3.907094</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>3.882479</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>3.760386</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>3.590500</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>3.920376</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>3.633023</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>3.544614</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>3.690650</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>3.586017</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>3.534677</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>3.418925</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>3.704090</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>3.742415</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Debugging cell - delete later (similar to cell below)\n",
        "ps=500\n",
        "hs=500\n",
        "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
        "model = create_barlow_twins_model(fastai_encoder, hidden_size=hs,projection_size=ps)# projection_size=1024)\n",
        "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
        "#values for these which is tantamount to doing nothing\n",
        "#So if we choose resize_scale=(1,1) then the images look the same.\n",
        "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
        "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=True)\n",
        "#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwRMSProp(model.parameters(),lr=0.1, mom=0.9)ins(aug_pipelines, print_augs=True)])\n",
        "opt = torch.optim.RMSprop\n",
        "#partial(OptimWrapper, opt=opt)\n",
        "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "#learn = Learner(dls, model,opt_func=opt_func, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "\n",
        "\n",
        "\n",
        "learn.fit(100) #300                            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Y5FN3Safag80"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=42):\n",
        "    \"\"\"\"\n",
        "    Seed everything.\n",
        "    \"\"\"   \n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def tune_set(items0,tune_s=1000):\n",
        "    \n",
        "  \n",
        "    items0=items0.shuffle()\n",
        "    d = {'0':0,'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0,'8':0,'9':0}\n",
        "    ITEMS=[]\n",
        "    for i in items0:\n",
        "        s=str(i).split('/training/')[1][0]\n",
        "        if d[s] is 0 or d[s] is 1:\n",
        "            ITEMS.append(i)\n",
        "            d[s]+=1\n",
        "    #items0=ITEMS\n",
        "\n",
        "    for i in items0:\n",
        "        if i not in ITEMS:\n",
        "            ITEMS.append(i)\n",
        "            \n",
        "    split = IndexSplitter(list(range(20)))\n",
        "\n",
        "    tds_tune = Datasets(ITEMS, [PILImageBW.create, [parent_label, Categorize()]], splits=split(ITEMS)) #Or do we want this?\n",
        "    dls_tune = tds_tune.dataloaders(bs=20, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "    \n",
        "    return dls_tune\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BRL2DuoRXc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "xv4RE3O0ag81",
        "outputId": "66dc6e7c-8fd0-474a-bd25-138411b3eb57"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='15687680' class='' max='15683414' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.03% [15687680/15683414 00:01&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assert passed\n"
          ]
        }
      ],
      "source": [
        "#TODO: Do this in a slicker way. Write more tests \n",
        "#Get the dataloader and set batch size \n",
        "ts=16384 #training set size - most everything\n",
        "bs=512\n",
        "device='cuda'\n",
        "path = untar_data(URLs.MNIST)\n",
        "\n",
        "items = get_image_files(path/'training') #i.e. NOT testing!!!\n",
        "items.sort() \n",
        "\n",
        "seed=42\n",
        "seed_everything(seed=seed)\n",
        "labeller = using_attr(RegexLabeller(pat = r'(\\d+).png$'), 'name') \n",
        "\n",
        "items=items.shuffle()\n",
        "items1 = items[0:ts] #train on these guys\n",
        "\n",
        "l=labeller(items1[0])\n",
        "if seed is 42:\n",
        "    print('assert passed')\n",
        "    assert labeller(items1[0]) == '19825' #check that random seed is working\n",
        "\n",
        "else:\n",
        "    input('Careful! New random seed ~= 42. Is that ok?')\n",
        "\n",
        "split = RandomSplitter(valid_pct=0.0) #randomly split training set into training and validation\n",
        "#tds = Datasets(items,splits=split(items)) #Do we want this?\n",
        "tds = Datasets(items1, [PILImageBW.create, [parent_label, Categorize()]], splits=split(items1)) #Or do we want this?\n",
        "dls = tds.dataloaders(bs=bs, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "\n",
        "tune_s=2000 #we choose 20 guys (randomly) out of 1000 to tune on\n",
        "items0 = items[ts:ts+tune_s] #for fine tuning - just choose 2000 guys to extract 20 for fine tuning \n",
        "dls_tune=tune_set(items0,tune_s=tune_s)\n",
        "\n",
        "\n",
        "#NB: Uncomment and compare in colab and kaggle\n",
        "# for x,y in dls_tune.train:\n",
        "#   print(x.mean())\n",
        "#   input()\n",
        "#   break\n",
        "\n",
        "# for x,y in dls_tune.train:\n",
        "#   print(x.mean())\n",
        "#   input()\n",
        "#   break\n",
        "\n",
        "\n",
        "#Evaluate linear classifier on this guy\n",
        "test_bs=578\n",
        "items2 = items[ts+tune_s:]\n",
        "split = RandomSplitter(valid_pct=0.0) #randomly split training set into training and validation\n",
        "tds_test = Datasets(items2, [PILImageBW.create, [parent_label, Categorize()]], splits=split(items2)) #Or do we want this?\n",
        "dls_test = tds_test.dataloaders(bs=test_bs, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "\n",
        "#Check that test_bs divides length of test set\n",
        "tem=len(dls_test.train_ds)/test_bs\n",
        "assert tem-math.floor(tem) == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z08cU2rMJd0",
        "outputId": "9b8d8f52-cfb0-404d-bbd5-ae6ee2d3002a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<iterator at 0x7fb925255510>"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_bs=578\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "AKbw2pxMag82",
        "outputId": "07e65765-a9ae-42b5-9e8a-c05f4dab18b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#A \"reasonable\" composite augmentation: initially copy pasted BT. We run this cell a few times to check it makes sense\n",
        "#Also define encoder and model\n",
        "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
        "model = create_barlow_twins_model(fastai_encoder, hidden_size=10,projection_size=10)# projection_size=1024)\n",
        "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
        "#values for these which is tantamount to doing nothing\n",
        "#So if we choose resize_scale=(1,1) then the images look the same.\n",
        "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
        "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=False)\n",
        "#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "\n",
        "#dls.valid.bs = len(dls.valid_ds) #Set the validation dataloader batch size to be the length of the validation dataset\n",
        "\n",
        "b = dls.one_batch()\n",
        "learn._split(b)\n",
        "learn('before_batch')\n",
        "axes = learn.barlow_twins.show(n=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47Qijb8Dunqs",
        "outputId": "6393cf6b-82a1-4bf5-8635-e6b7c196aeaa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function fastai.optimizer.SGD(params, lr, mom=0.0, wd=0.0, decouple_wd=True)>"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Kujpa-Lvag82"
      },
      "outputs": [],
      "source": [
        "#Simple linear classifier\n",
        "class LinearClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self,zdim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(zdim,10) #As 10 classes for mnist\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = cast(self.fc1(x),Tensor) #so we have to use cross entropy loss. cast is because using old version fastai \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "QJTzqSefag83"
      },
      "outputs": [],
      "source": [
        "#NB: Will give same random 20-tune set (for fixed random seed), only if the cell\n",
        "#\"#Get the dataloader and set batch size\" is the same. Perhaps later we can make this cell a function of that one. \n",
        "#Functions to train and evaluate head\n",
        "fastai_encoder.eval()\n",
        "\n",
        "def train_head(seed=10): #The seed choses a different (20) samples for training the head. 2 of each class\n",
        "\n",
        "    seed_everything(seed=seed)\n",
        "    dls_tune=tune_set(items0,tune_s=tune_s) #different random tune set each time (but random seed same for consistency)\n",
        "  \n",
        "    zdim=1024 #see above\n",
        "    head = LinearClassifier(zdim=zdim)\n",
        "    device='cuda'\n",
        "    head.to(device)\n",
        "    optimizer = torch.optim.Adam(head.parameters())\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    #EPOCHS=100\n",
        "\n",
        "    for epoch in range(200):\n",
        "\n",
        "        #for x,y in dls_tune.valid: #Slows massively on colab but not on kaggle. Weird. \n",
        "        x,y=dls_tune.valid.one_batch() #Same every time since dataset only has length=batch size = 20\n",
        "  \n",
        "        loss = criterion(head(fastai_encoder(x)),y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(loss)\n",
        "\n",
        "    return head\n",
        "\n",
        "\n",
        "def eval_head(head):\n",
        "\n",
        "    N=len(dls_test.train)*test_bs #close to len(dls_test.train_ds) but not quite...\n",
        "\n",
        "    assert N == len(dls_test.train_ds)\n",
        "\n",
        "    num_correct=0\n",
        "\n",
        "    for x,y in dls_test.train:\n",
        "    #for i in range(3):\n",
        "\n",
        "        ypred = head(fastai_encoder(x))\n",
        "        correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n",
        "        num_correct += correct.sum()\n",
        "    \n",
        "    return num_correct/N\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VULhbDWawO_J",
        "outputId": "f8b311c4-ee78-4e7e-a196-54aea4d2e33a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'seed_0': TensorCategory(0.8221), 'seed_1': TensorCategory(0.7424), 'seed_2': TensorCategory(0.8724), 'seed_3': TensorCategory(0.8159), 'seed_4': TensorCategory(0.8162)}\n",
            "tensor(0.8138)\n",
            "CPU times: user 40.7 s, sys: 3.79 s, total: 44.5 s\n",
            "Wall time: 1min 22s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "seed=10\n",
        "performance_dict={}\n",
        "for num in range(5):\n",
        "\n",
        "    head=train_head(seed=seed+num)\n",
        "    pct_correct = eval_head(head)\n",
        "\n",
        "    performance_dict[f'seed_{num}'] = pct_correct \n",
        "\n",
        "print(torch.mean(tensor(list(performance_dict.values()))))\n",
        "performance_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jttHXUwsQvd7",
        "outputId": "9e1a432b-97f5-4dd4-b024-8017fb287c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8138)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'seed_0': TensorCategory(0.8221),\n",
              " 'seed_1': TensorCategory(0.7424),\n",
              " 'seed_2': TensorCategory(0.8724),\n",
              " 'seed_3': TensorCategory(0.8159),\n",
              " 'seed_4': TensorCategory(0.8162)}"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ],
      "source": [
        "#To beat: 0.7756. All we changed is batch size random function\n",
        "print(torch.mean(tensor(list(performance_dict.values()))))\n",
        "performance_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPEDQE-rcKjL",
        "outputId": "1bfc3914-d20f-48e0-812d-49ede28b3661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8138)\n",
            "tensor(0.7672)\n"
          ]
        }
      ],
      "source": [
        "#100 learn_epochs, new baseline!\n",
        "sinu={'seed_0': TensorCategory(0.8221),\n",
        " 'seed_1': TensorCategory(0.7424),\n",
        " 'seed_2': TensorCategory(0.8724),\n",
        " 'seed_3': TensorCategory(0.8159),\n",
        " 'seed_4': TensorCategory(0.8162)}\n",
        "print(torch.mean(tensor(list(sinu.values())))) #tensor(0.8138)\n",
        "\n",
        "\n",
        "#100 learn_epochs, baseline for BT\n",
        "Baseline_BT = {'seed_0': TensorCategory(0.7621),\n",
        " 'seed_1': TensorCategory(0.7099),\n",
        " 'seed_2': TensorCategory(0.8630),\n",
        " 'seed_3': TensorCategory(0.7644),\n",
        " 'seed_4': TensorCategory(0.7367)}\n",
        "\n",
        "print(torch.mean(tensor(list(Baseline_BT.values())))) #tensor(0.7672)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZPO88U4XMgb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYI6BiCTXCmt",
        "outputId": "c99dc95d-0a71-4fc3-d791-16cee98bfa8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.8487)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "hw0VjYxRx1va",
        "outputId": "659f5266-600d-4b32-a834-a84fff7d18b3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c8c02caa2cf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperformance_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'performance_dict' is not defined"
          ]
        }
      ],
      "source": [
        "performance_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COooHESjoNGb"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}