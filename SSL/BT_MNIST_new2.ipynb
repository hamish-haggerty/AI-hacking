{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamish-haggerty/AI-hacking/blob/master/SSL/BT_MNIST_new2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8jFsEXz_61O",
        "outputId": "bf61a600-3452-4b2c-d239-af3ca1df90de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.7/dist-packages (1.11.0)\n",
            "Requirement already satisfied: torchvision==0.12.0 in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: torchaudio==0.11.0 in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastai==2.6.3 in /usr/local/lib/python3.7/dist-packages (2.6.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: self_supervised in /usr/local/lib/python3.7/dist-packages (1.0.4)\n",
            "Requirement already satisfied: kornia>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from self_supervised) (0.6.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from self_supervised) (21.3)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from self_supervised) (21.1.3)\n",
            "Requirement already satisfied: timm>=0.4.5 in /usr/local/lib/python3.7/dist-packages (from self_supervised) (0.6.7)\n",
            "Requirement already satisfied: fastai>=2.2.7 in /usr/local/lib/python3.7/dist-packages (from self_supervised) (2.6.3)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (7.1.2)\n",
            "Requirement already satisfied: fastcore<1.5,>=1.3.27 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.4.5)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (0.0.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (3.2.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (6.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.7.3)\n",
            "Requirement already satisfied: torch<1.12,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.11.0)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (3.4.1)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (0.12.0)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.4.4)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.21.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.9.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.6.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (57.4.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (8.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4->fastai>=2.2.7->self_supervised) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->self_supervised) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4->fastai>=2.2.7->self_supervised) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai>=2.2.7->self_supervised) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4->fastai>=2.2.7->self_supervised) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4->fastai>=2.2.7->self_supervised) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai>=2.2.7->self_supervised) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai>=2.2.7->self_supervised) (2022.2.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (7.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (2.0.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest) (1.1.1)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.11.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (22.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytest) (21.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytest) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ipytest in /usr/local/lib/python3.7/dist-packages (0.12.0)\n",
            "Requirement already satisfied: pytest>=5.4 in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.1.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (21.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.1.1)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.11.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (22.1.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (3.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
            "Requirement already satisfied: jedi>=0.10 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.18.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.0.10)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (5.1.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipytest) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipytest) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipytest) (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0\n",
        "!pip install fastai==2.6.3 --no-deps\n",
        "!pip install self_supervised\n",
        "\n",
        "!pip install pytest\n",
        "!pip install ipytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BOv4kkJDag8r",
        "outputId": "199fc323-1b10-4a27-c8a4-20a6bb04c904"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk01WY_Dag8s"
      },
      "outputs": [],
      "source": [
        "import fastai\n",
        "import self_supervised\n",
        "import torch\n",
        "assert(fastai.__version__ == '2.6.3') #Check that version is 2.6.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOjr_YCLag8t"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *\n",
        "from self_supervised.augmentations import *\n",
        "from self_supervised.layers import *\n",
        "import inspect\n",
        "import warnings\n",
        "import random\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#from Base_Stein.SVGD_classes import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTSdKC6bag8t"
      },
      "outputs": [],
      "source": [
        "#Like most other SSL algorithms BT's model consists of an encoder and projector (MLP) layer.\n",
        "#Definition is straightforward:\n",
        "#https://colab.research.google.com/github/KeremTurgutlu/self_supervised/blob/master/nbs/14%20-%20barlow_twins.ipynb#scrollTo=1M6QcUChcvpz\n",
        "class BarlowTwinsModel(Module):\n",
        "    \"\"\"An encoder followed by a projector\n",
        "    \"\"\"\n",
        "    def __init__(self,encoder,projector):self.encoder,self.projector = encoder,projector\n",
        "        \n",
        "    def forward(self,x): return self.projector(self.encoder(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL3EE07Pag8u"
      },
      "outputs": [],
      "source": [
        "#HOWEVER instead of directly using the above, by passing both an encoder and a projector, create_barlow_twins_model\n",
        "#function can be used by minimally passing a predefined encoder and the expected input channels.\n",
        "\n",
        "#In the paper it's mentioned that MLP layer consists of 3 layers... following function will create a 3 layer\n",
        "#MLP projector with batchnorm and ReLU by default. Alternatively, you can change bn and nlayers. \n",
        "\n",
        "#Questions: Why torch.no_grad() when doing this?\n",
        "def create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n",
        "    \"Create Barlow Twins model\"\n",
        "    n_in  = in_channels(encoder)\n",
        "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
        "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n",
        "    apply_init(projector)\n",
        "    return BarlowTwinsModel(encoder, projector)\n",
        "\n",
        "#Similar to above. Simple API to make the BT model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFjGL-COag8v"
      },
      "outputs": [],
      "source": [
        "#BarlowTwins Callback\n",
        "#The following parameters can be passed:\n",
        "# - aug_pipelines\n",
        "# Imb lambda is the weight for redundancy reduction term in the loss function\n",
        "\n",
        "@delegates(get_multi_aug_pipelines)\n",
        "def get_barlow_twins_aug_pipelines(size,**kwargs): return get_multi_aug_pipelines(n=2,size=size,**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx4KsywAag8v"
      },
      "outputs": [],
      "source": [
        "#Uniform random number between a and b\n",
        "def Unif(a,b):\n",
        "    return (b-a)*torch.rand(1).item()+a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6gBw66Iag8y"
      },
      "outputs": [],
      "source": [
        "def random_sinusoid(x,std=0.1,seed=0):\n",
        "\n",
        "    seed_everything(seed=seed)\n",
        "    \n",
        "    t=torch.normal(mean=0,std=std,size=(1,1)).item()\n",
        "    s=torch.normal(mean=0,std=std,size=(1,1)).item()\n",
        "    \n",
        "    u=torch.normal(mean=0,std=1,size=(1,1)).item()\n",
        "    v=torch.normal(mean=0,std=1,size=(1,1)).item()\n",
        "    \n",
        "    a=torch.normal(mean=0,std=1,size=(1,1)).item()\n",
        "    b=torch.normal(mean=0,std=1,size=(1,1)).item()\n",
        "    \n",
        "    return torch.sin(t*math.pi*x+u) + torch.cos(s*math.pi*x + v)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def random_sinusoid_new(x,std=0.1,seed=0):\n",
        "    \n",
        "    seed_everything(seed=seed)    \n",
        "    t=(std) * torch.randn(1,500).to(device)\n",
        "    s=(std) * torch.randn(1,500).to(device)\n",
        "    \n",
        "    u=torch.randn(1,500).to(device)\n",
        "    v=torch.randn(1,500).to(device)\n",
        "\n",
        "    return torch.sin(t*x[:,]*math.pi+u) + torch.cos(s*x[:,]*math.pi+v)\n",
        "\n",
        "\n",
        "    \n",
        "    #return torch.sin(t*math.pi*x+u) + torch.cos(s*math.pi*x + v)"
      ],
      "metadata": {
        "id": "zU4GwLruU5AD"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z = torch.rand(512,500)\n",
        "\n",
        "t=torch.randn(1,500)\n",
        "\n",
        "#then want to multiply\n",
        "\n",
        "(z[:,]*t).shape\n",
        "\n",
        "random_sinusoid(z).shape\n",
        "\n",
        "#random_sinusoid(z)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP80LtvVT8xp",
        "outputId": "9a9f4b44-f4f2-45c1-bda4-caf32dff57c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([512, 500])"
            ]
          },
          "metadata": {},
          "execution_count": 418
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li3BQ1ukX0XB"
      },
      "outputs": [],
      "source": [
        "def Exp_sample(scale=3.,loc=1.8):\n",
        "    \n",
        "    Expo = torch.distributions.exponential.Exponential\n",
        "    E = Expo(torch.tensor([scale]))\n",
        "#     if random.random()<0.5:\n",
        "#         return 1\n",
        "#     else:\n",
        "    return loc+E.sample().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDYVO2u1U4We"
      },
      "outputs": [],
      "source": [
        "def p_norm(x,p):\n",
        "\n",
        "    eps=1e-8\n",
        "    if p<1:\n",
        "        x=x+eps\n",
        "    return (1/p)*torch.abs(x).pow(p)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Max_Corr(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(ps,ps)\n",
        "        self.fc2 = nn.Linear(ps,ps)\n",
        "\n",
        "        self.fc3 = nn.Linear(ps,ps)\n",
        "        self.fc4 = nn.Linear(ps,ps)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self,x,y):\n",
        "\n",
        "        x=self.sigmoid(self.fc1(x))\n",
        "        x=self.fc2(x)\n",
        "\n",
        "        y=self.relu(self.fc3(y))\n",
        "        y=self.fc4(y)\n",
        "\n",
        "        return x,y\n",
        "     \n",
        "        "
      ],
      "metadata": {
        "id": "C9yOuK3z3RdD"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "00BvEKTd_VZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def inner_step(z1norm,z2norm,I,inner_steps=7):\n",
        " \n",
        "    z1norm=z1norm.detach()\n",
        "    z2norm=z2norm.detach()\n",
        "\n",
        "    # z1norm=z1norm[:,0]\n",
        "    # z2norm=z2norm[:,0]\n",
        "\n",
        "    max_corr = Max_Corr()\n",
        "    max_corr.cuda()\n",
        " \n",
        "    # for p in max_corr.parameters():\n",
        "    #     p.requires_grad=True\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(list(max_corr.parameters()),lr=0.001)\n",
        "\n",
        "\n",
        "    for i in range(inner_steps):\n",
        "        z1norm_2,z2norm_2=max_corr(z1norm,z2norm)\n",
        "        #z1norm_2 = (z1norm_2 - z1norm_2.mean(0)) / z1norm_2.std(0, unbiased=False)\n",
        "    \n",
        "    \n",
        "        assert (z1norm_2.shape,z2norm_2.shape) == (z1norm.shape,z2norm.shape)\n",
        "\n",
        "\n",
        "        #Ctem =  (z1norm_2.T @ z2norm_2) / bs\n",
        "        Ctem1 =  (z1norm.T @ z2norm_2) / bs\n",
        "        Ctem2 = (z1norm_2.T @ z2norm) / bs\n",
        "\n",
        "\n",
        "        cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2))\n",
        "        #cdiff_2=Ctem.pow(2)\n",
        "\n",
        "        inner_loss=-1*(cdiff_2*(1-I)).mean()\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        inner_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "            \n",
        "    #z1norm_2,z2norm_2=max_corr(z1norm,z2norm)\n",
        "    \n",
        "    for p in max_corr.parameters():\n",
        "        p.requires_grad=False\n",
        "        \n",
        "    return max_corr"
      ],
      "metadata": {
        "id": "2O8Jwpc0ReJi"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "id": "a2Exs2s3ag8z"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class BarlowTwins(Callback):\n",
        "    order,run_valid = 9,True\n",
        "    def __init__(self, aug_pipelines, lmb=5e-3, print_augs=False):\n",
        "        assert_aug_pipelines(aug_pipelines)\n",
        "        self.aug1, self.aug2 = aug_pipelines\n",
        "        if print_augs: print(self.aug1), print(self.aug2)\n",
        "        store_attr('lmb')\n",
        "\n",
        "        self.inner_steps=4\n",
        "        \n",
        "    def before_fit(self): \n",
        "        self.learn.loss_func = self.lf\n",
        "        nf = self.learn.model.projector[-1].out_features\n",
        "        self.I = torch.eye(nf).to(self.dls.device)\n",
        "\n",
        "    def update_seed(self):\n",
        "        \n",
        "        indexmod=2\n",
        "        if self.index%indexmod == 0: #every `indexmod` index update the seed (best we have found so far)\n",
        "\n",
        "            self.seed = np.random.randint(0,10000)\n",
        "\n",
        "    def before_epoch(self):\n",
        "        self.index=-1\n",
        "\n",
        "        if self.epoch%10==0:\n",
        "            self.inner_steps += 1\n",
        "            \n",
        "    def before_batch(self):\n",
        "        xi,xj = self.aug1(self.x), self.aug2(self.x)\n",
        "        self.learn.xb = (torch.cat([xi, xj]),)\n",
        "\n",
        "        self.index=self.index+1\n",
        "\n",
        "        self.update_seed()\n",
        "\n",
        "        \n",
        "        #Uncomment to run standard BT\n",
        "    # def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
        "    #     bs,nf = pred.size(0)//2,pred.size(1)\n",
        "\n",
        "    #     z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
        "\n",
        "    #     z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
        "    #     z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
        "        \n",
        "    #     C = (z1norm.T @ z2norm) / bs \n",
        "    #     cdiff = (C - self.I)**2\n",
        "    #     loss = (cdiff*self.I + cdiff*(1-self.I)*self.lmb).sum() \n",
        "    #     return loss\n",
        "\n",
        "\n",
        "    def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
        "        bs,nf = pred.size(0)//2,pred.size(1)\n",
        "\n",
        "        #All standard, from BT\n",
        "        z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
        "        z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
        "        z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
        "        \n",
        "        C = (z1norm.T @ z2norm) / bs \n",
        "        cdiff = (C - self.I)**2\n",
        "\n",
        "        #New method\n",
        "        #max_corr=\n",
        "        max_corr = inner_step(z1norm,z2norm,I=self.I,inner_steps=5)#,inner_steps=self.inner_steps)\n",
        "        z1norm_2,z2norm_2 = max_corr(z1norm,z2norm)\n",
        "        Ctem1 =  (z1norm.T @ z2norm_2) / bs\n",
        "        Ctem2 = (z1norm_2.T @ z2norm) / bs\n",
        "        #Ctem = (z1norm_2.T @ z2norm_2) / bs\n",
        "        #cdiff_2 = Ctem.pow(2)\n",
        "        cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2)) #+ 0.1*Ctem.pow(2)\n",
        "\n",
        "        #cdiff_2 = Ctem.pow(2)\n",
        "        #cdiff_2 = (0.8*cdiff_2 + 0.2*cdiff)\n",
        "\n",
        "        #del max_corr\n",
        "\n",
        "\n",
        "        #{'Pr': 0.45546983480453496, 'dist': 'Exp', 'loc': 0.49445648193359376, 'scale': 2.5828861594200134, 'percent_correct': 0.8277832269668579} #Best so far\n",
        "\n",
        "        self.update_seed()\n",
        "\n",
        "        #input('hi')\n",
        "\n",
        "        #polyprob=0.1\n",
        "        #polyprob=0.45546\n",
        "        # polyprob=1.0\n",
        "        polyprob=1.0\n",
        "        temrand = random.random()\n",
        "        if temrand < polyprob: #With some probability we want off diag terms to be (quadratic) say.\n",
        "\n",
        "            K=2\n",
        "            cdiff_2_2=0\n",
        "            for i in range(K):\n",
        "                #p=Exp_sample(loc=1.5,scale=2.0)\n",
        "                #p=Unif(0.9,2.5)\n",
        "                #z1norm_2 = p_norm(z1norm,p=p)\n",
        "\n",
        "                #p=Exp_sample(loc=1.5,scale=2.0)\n",
        "                #p=Unif(0.9,2.5)\n",
        "                #z2norm_2 = p_norm(z2norm,p=p)\n",
        "\n",
        "                z1norm_2 = random_sinusoid_new(z1norm,std=0.10,seed=self.seed+i)\n",
        "                z2norm_2 = random_sinusoid_new(z2norm,std=0.10,seed=2*self.seed+i)\n",
        "\n",
        "                #C = (z1norm_2.T @ z2norm_2) / bs\n",
        "                C_1 = (z1norm.T @ z2norm_2) / bs\n",
        "                C_2 = (z1norm_2.T @ z2norm) / bs\n",
        "\n",
        "                #cdiff_2 = 0.5*(C_1).pow(2) + 0.5*(C_2).pow(2)\n",
        "                #cdiff_2_2 = cdiff_2_2 + cdiff_2\n",
        "                cdiff_2_2 = cdiff_2_2 + 0.5*C_1.pow(2)+0.5*C_2.pow(2)\n",
        "            \n",
        "            cdiff_2_2=(1/K)*cdiff_2_2\n",
        "\n",
        "            #cdiff_2 = 1.0*cdiff_2_2 + 0.2*cdiff_2 #+ 0.1*cdiff\n",
        "            \n",
        "            cdiff_2 = 0.8*cdiff_2_2 + 0.2*cdiff_2\n",
        "\n",
        "            #symmetrize loss - so copy paste above block but swap place of 1 and 2\n",
        "            #p=Exp_sample(loc=1.5,scale=2.0)\n",
        "            #p=Unif(1.0,2.5)\n",
        "            \n",
        "            # cdiff_2_sym=0\n",
        "            # for i in range(K):\n",
        "            #     z1norm_2 = random_sinusoid_new(z1norm,std=0.1,seed=4*self.seed+i)\n",
        "            #     z2norm_2 = random_sinusoid_new(z2norm,std=0.1,seed=8*self.seed+i)\n",
        "\n",
        "            #     C_2_sym = (z1norm_2.T @ z2norm_2) / bs\n",
        "            #     cdiff_2_sym = cdiff_2_sym + C_2_sym**2\n",
        "            \n",
        "            # cdiff_2_sym=(1/K)*cdiff_2_sym\n",
        "\n",
        "            #cdiff_2 = cdiff_2 #+ 0.5*cdiff_2_sym #Symmetrized random loss\n",
        "\n",
        "            #cdiff_2 = 0.2*cdiff +  0.8*cdiff_2 #convex comb of BT loss with random loss -> assumes polyprob=1.0\n",
        "            \n",
        "        else:\n",
        "            cdiff_2 = cdiff\n",
        "            \n",
        "        l2 = cdiff_2*(1-self.I)*self.lmb #Is either the standard term - or not.\n",
        "\n",
        "\n",
        "        loss = (cdiff*self.I + l2).sum()\n",
        "        torch.cuda.empty_cache()\n",
        "        return loss\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def show(self, n=1):\n",
        "        bs = self.learn.x.size(0)//2\n",
        "        x1,x2  = self.learn.x[:bs], self.learn.x[bs:] \n",
        "        idxs = np.random.choice(range(bs),n,False)\n",
        "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n",
        "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n",
        "        images = []\n",
        "        for i in range(n): images += [x1[i],x2[i]] \n",
        "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vbS1WtLiag80",
        "outputId": "64ce2554-28a6-4683-8c09-39771431452f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>83.600571</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>55.786327</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>39.787407</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>29.730427</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>24.088152</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>20.077597</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>17.221245</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>15.107199</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>12.885223</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>11.127546</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>9.706683</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>8.808406</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>8.172922</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>7.346352</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>6.989358</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>6.253189</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>6.035928</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>6.072076</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>5.648910</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>5.203657</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.748878</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>4.622414</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>4.226252</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>3.887390</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>3.832292</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.621683</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>3.526616</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>3.504732</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>3.575911</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>3.371763</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.227535</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>3.044689</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.828780</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>2.633257</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>2.652194</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>2.551313</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>2.530160</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>2.461350</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>2.398165</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>2.255348</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.217808</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>2.232314</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>2.110712</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>2.011721</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.994477</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.979108</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.995912</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.935176</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.980031</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.042627</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.998684</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.966694</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.878651</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.790056</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.721835</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.683371</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.601198</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.608366</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.652232</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.621634</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.705680</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.797671</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.692975</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.579865</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.534480</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.511224</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.429476</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.407277</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.638877</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.811748</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.755633</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.623654</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.477288</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.386681</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.440787</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.506939</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>1.446662</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.601413</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.738421</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.610303</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.457644</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>1.386402</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.332985</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.277590</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.315492</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.325563</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.269067</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>1.227654</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.222573</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.175319</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.168504</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.220278</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>1.300376</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>1.255671</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.284784</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>1.306303</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>1.223812</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>1.176061</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.151958</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.140831</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.111004</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>1.102037</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>1.108709</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>1.081590</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>1.101576</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.166968</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>1.139426</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>1.144249</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>1.184642</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>1.145892</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.090715</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>1.147153</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>1.145051</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>1.109538</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>1.079639</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.074460</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>1.036815</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>1.019664</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>1.019458</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>1.032366</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.004802</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>1.054508</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>1.113905</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>1.083403</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>1.041134</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.019277</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>1.003847</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.957088</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.928852</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.950634</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.914499</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.981944</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>1.063363</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>1.036700</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>1.025994</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>1.027856</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>1.037218</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>1.039911</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>1.072956</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>1.029713</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.960194</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.902010</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.903417</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.896895</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.881540</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.877292</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.896696</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.868103</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.857866</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.921005</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.932019</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.939841</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.922034</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.879694</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.874580</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.859925</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.885590</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.863719</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.833106</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.849429</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.853689</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.835187</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.826627</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.841320</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.838385</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.933449</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.928602</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.887000</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.887625</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.865175</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.804565</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.777447</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.760835</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.778460</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.816374</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.910504</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.985908</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.948712</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.893853</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.844432</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.808173</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.772847</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.735751</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.744290</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.786316</td>\n",
              "      <td>None</td>\n",
              "      <td>00:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.774094</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.783767</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.782852</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.758860</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.749381</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.743145</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.738649</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.737261</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.789388</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.816455</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.822694</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.825101</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.774660</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.787204</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.926949</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Debugging cell - delete later (similar to cell below)\n",
        "ps=500\n",
        "hs=500\n",
        "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
        "model = create_barlow_twins_model(fastai_encoder, hidden_size=hs,projection_size=ps)# projection_size=1024)\n",
        "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
        "#values for these which is tantamount to doing nothing\n",
        "#So if we choose resize_scale=(1,1) then the images look the same.\n",
        "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
        "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=True)\n",
        "#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwRMSProp(model.parameters(),lr=0.1, mom=0.9)ins(aug_pipelines, print_augs=True)])\n",
        "opt = torch.optim.RMSprop\n",
        "#partial(OptimWrapper, opt=opt)\n",
        "learn = Learner(dls,model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "#learn = Learner(dls, model,opt_func=opt_func, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "\n",
        "learn.fit(200) #300                            "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lCdaOZXQy6WF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5FN3Safag80"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=42):\n",
        "    \"\"\"\"\n",
        "    Seed everything.\n",
        "    \"\"\"   \n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def tune_set(items0,tune_s=1000):\n",
        "    \n",
        "  \n",
        "    items0=items0.shuffle()\n",
        "    d = {'0':0,'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0,'8':0,'9':0}\n",
        "    ITEMS=[]\n",
        "    for i in items0:\n",
        "        s=str(i).split('/training/')[1][0]\n",
        "        if d[s] is 0 or d[s] is 1:\n",
        "            ITEMS.append(i)\n",
        "            d[s]+=1\n",
        "    #items0=ITEMS\n",
        "\n",
        "    for i in items0:\n",
        "        if i not in ITEMS:\n",
        "            ITEMS.append(i)\n",
        "            \n",
        "    split = IndexSplitter(list(range(20)))\n",
        "\n",
        "    tds_tune = Datasets(ITEMS, [PILImageBW.create, [parent_label, Categorize()]], splits=split(ITEMS)) #Or do we want this?\n",
        "    dls_tune = tds_tune.dataloaders(bs=20, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "    \n",
        "    return dls_tune\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xv4RE3O0ag81",
        "outputId": "727a2f16-0c0c-4e54-acf2-9b12eaa75666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assert passed\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-242-2931c8e9f51c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_pct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#randomly split training set into training and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mtds_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPILImageBW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparent_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Or do we want this?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mdls_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtds_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_bs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter_item\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntToFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#Check that test_bs divides length of test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/data/core.py\u001b[0m in \u001b[0;36mdataloaders\u001b[0;34m(self, bs, shuffle_train, shuffle, val_shuffle, n, path, dl_type, dl_kwargs, device, drop_last, val_bs, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mval_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mdef_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'bs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'shuffle'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'drop_last'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdrop_last\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdef_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mdef_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'bs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mval_bs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mval_bs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'shuffle'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_shuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'drop_last'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         dls = [dl] + [dl.new(self.subset(i), **merge(kwargs,def_kwargs,val_kwargs,dl_kwargs[i]))\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/data/core.py\u001b[0m in \u001b[0;36msubset\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mcoll_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_inp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_setup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moverlapping_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverlapping_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastcore/foundation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastcore/foundation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, items, use_list, match, *rest)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_list\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastcore/basics.py\u001b[0m in \u001b[0;36mlistify\u001b[0;34m(o, use_list, match, *rest)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mis_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/data/core.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mcoll_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtuplify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_inp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_inp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_setup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moverlapping_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverlapping_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/data/core.py\u001b[0m in \u001b[0;36msubset\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"Tried to grab subset {i} in the Dataset, but it contained no items.\\n\\t{e.args[0]}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_after_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0;34mf\"{self.__class__.__name__}: {self.items}\\ntfms - {self.tfms.fs}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastcore/foundation.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    117\u001b[0m         return (self.items.iloc[list(i)] if hasattr(self.items,'iloc')\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'__array__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 else [self.items[i_] for i_ in i])\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastcore/foundation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    117\u001b[0m         return (self.items.iloc[list(i)] if hasattr(self.items,'iloc')\n\u001b[1;32m    118\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'__array__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 else [self.items[i_] for i_ in i])\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#TODO: Do this in a slicker way. Write more tests \n",
        "#Get the dataloader and set batch size \n",
        "ts=16384 #training set size - most everything\n",
        "bs=512\n",
        "device='cuda'\n",
        "path = untar_data(URLs.MNIST)\n",
        "\n",
        "items = get_image_files(path/'training') #i.e. NOT testing!!!\n",
        "items.sort() \n",
        "\n",
        "seed=42\n",
        "seed_everything(seed=seed)\n",
        "labeller = using_attr(RegexLabeller(pat = r'(\\d+).png$'), 'name') \n",
        "\n",
        "items=items.shuffle()\n",
        "items1 = items[0:ts] #train on these guys\n",
        "\n",
        "l=labeller(items1[0])\n",
        "if seed is 42:\n",
        "    print('assert passed')\n",
        "    assert labeller(items1[0]) == '19825' #check that random seed is working\n",
        "\n",
        "else:\n",
        "    input('Careful! New random seed ~= 42. Is that ok?')\n",
        "\n",
        "split = RandomSplitter(valid_pct=0.0) #randomly split training set into training and validation\n",
        "#tds = Datasets(items,splits=split(items)) #Do we want this?\n",
        "tds = Datasets(items1, [PILImageBW.create, [parent_label, Categorize()]], splits=split(items1)) #Or do we want this?\n",
        "dls = tds.dataloaders(bs=bs,num_workers=6, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "\n",
        "tune_s=2000 #we choose 20 guys (randomly) out of 1000 to tune on\n",
        "items0 = items[ts:ts+tune_s] #for fine tuning - just choose 2000 guys to extract 20 for fine tuning \n",
        "dls_tune=tune_set(items0,tune_s=tune_s)\n",
        "\n",
        "\n",
        "#NB: Uncomment and compare in colab and kaggle\n",
        "# for x,y in dls_tune.train:\n",
        "#   print(x.mean())\n",
        "#   input()\n",
        "#   break\n",
        "\n",
        "# for x,y in dls_tune.train:\n",
        "#   print(x.mean())\n",
        "#   input()\n",
        "#   break\n",
        "\n",
        "\n",
        "#Evaluate linear classifier on this guy\n",
        "test_bs=578\n",
        "items2 = items[ts+tune_s:]\n",
        "split = RandomSplitter(valid_pct=0.0) #randomly split training set into training and validation\n",
        "tds_test = Datasets(items2, [PILImageBW.create, [parent_label, Categorize()]], splits=split(items2)) #Or do we want this?\n",
        "dls_test = tds_test.dataloaders(bs=test_bs, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "\n",
        "#Check that test_bs divides length of test set\n",
        "tem=len(dls_test.train_ds)/test_bs\n",
        "assert tem-math.floor(tem) == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z08cU2rMJd0",
        "outputId": "9b8d8f52-cfb0-404d-bbd5-ae6ee2d3002a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<iterator at 0x7fb925255510>"
            ]
          },
          "execution_count": 167,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_bs=578\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "AKbw2pxMag82",
        "outputId": "07e65765-a9ae-42b5-9e8a-c05f4dab18b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#A \"reasonable\" composite augmentation: initially copy pasted BT. We run this cell a few times to check it makes sense\n",
        "#Also define encoder and model\n",
        "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
        "model = create_barlow_twins_model(fastai_encoder, hidden_size=10,projection_size=10)# projection_size=1024)\n",
        "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
        "#values for these which is tantamount to doing nothing\n",
        "#So if we choose resize_scale=(1,1) then the images look the same.\n",
        "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
        "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=False)\n",
        "#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "\n",
        "#dls.valid.bs = len(dls.valid_ds) #Set the validation dataloader batch size to be the length of the validation dataset\n",
        "\n",
        "b = dls.one_batch()\n",
        "learn._split(b)\n",
        "learn('before_batch')\n",
        "axes = learn.barlow_twins.show(n=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kujpa-Lvag82"
      },
      "outputs": [],
      "source": [
        "#Simple linear classifier\n",
        "class LinearClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self,zdim):\n",
        "            \n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(zdim,10) #As 10 classes for mnist\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = cast(self.fc1(x),Tensor) #so we have to use cross entropy loss. cast is because using old version fastai \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJTzqSefag83"
      },
      "outputs": [],
      "source": [
        "#NB: Will give same random 20-tune set (for fixed random seed), only if the cell\n",
        "#\"#Get the dataloader and set batch size\" is the same. Perhaps later we can make this cell a function of that one. \n",
        "#Functions to train and evaluate head\n",
        "fastai_encoder.eval()\n",
        "\n",
        "def train_head(seed=10): #The seed choses a different (20) samples for training the head. 2 of each class\n",
        "\n",
        "    seed_everything(seed=seed)\n",
        "    dls_tune=tune_set(items0,tune_s=tune_s) #different random tune set each time (but random seed same for consistency)\n",
        "  \n",
        "    zdim=1024 #see above\n",
        "    head = LinearClassifier(zdim=zdim)\n",
        "    device='cuda'\n",
        "    head.to(device)\n",
        "    optimizer = torch.optim.Adam(head.parameters())\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    #EPOCHS=100\n",
        "\n",
        "    for epoch in range(200):\n",
        "\n",
        "        #for x,y in dls_tune.valid: #Slows massively on colab but not on kaggle. Weird. \n",
        "        x,y=dls_tune.valid.one_batch() #Same every time since dataset only has length=batch size = 20\n",
        "  \n",
        "        loss = criterion(head(fastai_encoder(x)),y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #print(loss)\n",
        "\n",
        "    return head\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_head(head):\n",
        "\n",
        "    N=len(dls_test.train)*test_bs #close to len(dls_test.train_ds) but not quite...\n",
        "\n",
        "    assert N == len(dls_test.train_ds)\n",
        "\n",
        "    num_correct=0\n",
        "\n",
        "    for x,y in dls_test.train:\n",
        "    #for i in range(3):\n",
        "\n",
        "        ypred = head(fastai_encoder(x))\n",
        "        correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n",
        "        num_correct += correct.sum()\n",
        "    \n",
        "    return num_correct/N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VULhbDWawO_J",
        "outputId": "a721ef9d-a219-489e-faac-f9309b0f806c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8118)\n",
            "CPU times: user 46.4 s, sys: 5.09 s, total: 51.5 s\n",
            "Wall time: 1min 42s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'seed_0': TensorCategory(0.8062),\n",
              " 'seed_1': TensorCategory(0.7429),\n",
              " 'seed_2': TensorCategory(0.8765),\n",
              " 'seed_3': TensorCategory(0.8440),\n",
              " 'seed_4': TensorCategory(0.7893)}"
            ]
          },
          "metadata": {},
          "execution_count": 304
        }
      ],
      "source": [
        "%%time\n",
        "seed=10\n",
        "performance_dict={}\n",
        "for num in range(5):\n",
        "\n",
        "    head=train_head(seed=seed+num)\n",
        "    pct_correct = eval_head(head)\n",
        "\n",
        "    performance_dict[f'seed_{num}'] = pct_correct \n",
        "\n",
        "print(torch.mean(tensor(list(performance_dict.values()))))\n",
        "performance_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Current to equal: Results on different MBT runs (see above for implementation): tensor(0.8419),tensor(0.8069),tensor(0.8118)\n",
        "\n",
        "print(torch.mean(tensor(list(performance_dict.values()))))\n",
        "performance_dict\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpcqDLMJWpkB",
        "outputId": "b897ceb5-dde6-4574-de88-dd57a16e5c1f"
      },
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8419)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'seed_0': TensorCategory(0.8478),\n",
              " 'seed_1': TensorCategory(0.7656),\n",
              " 'seed_2': TensorCategory(0.9092),\n",
              " 'seed_3': TensorCategory(0.8410),\n",
              " 'seed_4': TensorCategory(0.8462)}"
            ]
          },
          "metadata": {},
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ECKoTFDdmQvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPEDQE-rcKjL",
        "outputId": "d07175db-d8c4-4168-89a7-9a0a3ae47863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8252)\n"
          ]
        }
      ],
      "source": [
        "#250 learn_epochs. To beat: (K=10,indexmod=4,std=0.1,convex=(0.2,0.8))\n",
        "tem={'seed_0': TensorCategory(0.8364),\n",
        " 'seed_1': TensorCategory(0.7424),\n",
        " 'seed_2': TensorCategory(0.8947),\n",
        " 'seed_3': TensorCategory(0.8392),\n",
        " 'seed_4': TensorCategory(0.8133)}\n",
        "print(torch.mean(tensor(list(tem.values())))) #tensor(0.8252)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZPO88U4XMgb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYI6BiCTXCmt",
        "outputId": "c99dc95d-0a71-4fc3-d791-16cee98bfa8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.8487)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "hw0VjYxRx1va",
        "outputId": "659f5266-600d-4b32-a834-a84fff7d18b3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c8c02caa2cf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperformance_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'performance_dict' is not defined"
          ]
        }
      ],
      "source": [
        "performance_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COooHESjoNGb"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}