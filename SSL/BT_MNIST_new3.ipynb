{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamish-haggerty/AI-hacking/blob/master/SSL/BT_MNIST_new3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X8jFsEXz_61O",
        "outputId": "5ab690f3-0404-43c3-96e6-b0a30743c8ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 10 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.12.0\n",
            "  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 78.8 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.11.0\n",
            "  Downloading torchaudio-0.11.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 60.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0) (2022.6.15)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.12.1+cu113\n",
            "    Uninstalling torchaudio-0.12.1+cu113:\n",
            "      Successfully uninstalled torchaudio-0.12.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.11.0 torchaudio-0.11.0 torchvision-0.12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastai==2.6.3\n",
            "  Downloading fastai-2.6.3-py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 14.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: fastai\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 2.7.9\n",
            "    Uninstalling fastai-2.7.9:\n",
            "      Successfully uninstalled fastai-2.7.9\n",
            "Successfully installed fastai-2.6.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting self_supervised\n",
            "  Downloading self_supervised-1.0.4-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 565 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from self_supervised) (21.1.3)\n",
            "Collecting timm>=0.4.5\n",
            "  Downloading timm-0.6.11-py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 22.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from self_supervised) (21.3)\n",
            "Requirement already satisfied: fastai>=2.2.7 in /usr/local/lib/python3.7/dist-packages (from self_supervised) (2.6.3)\n",
            "Collecting kornia>=0.5.0\n",
            "  Downloading kornia-0.6.7-py2.py3-none-any.whl (565 kB)\n",
            "\u001b[K     |████████████████████████████████| 565 kB 65.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (3.2.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (0.0.7)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (3.4.1)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (0.12.0)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (7.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.3.5)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.7.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (6.0)\n",
            "Collecting fastcore<1.5,>=1.3.27\n",
            "  Downloading fastcore-1.4.5-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<1.12,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (57.4.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.6.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.10)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.4.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.64.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.9.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (8.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.3.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4->fastai>=2.2.7->self_supervised) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->self_supervised) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4->fastai>=2.2.7->self_supervised) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai>=2.2.7->self_supervised) (0.7.8)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 69.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4->fastai>=2.2.7->self_supervised) (7.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm>=0.4.5->self_supervised) (3.8.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm>=0.4.5->self_supervised) (4.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4->fastai>=2.2.7->self_supervised) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai>=2.2.7->self_supervised) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai>=2.2.7->self_supervised) (2022.2.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (1.1.0)\n",
            "Installing collected packages: fastcore, huggingface-hub, timm, kornia, self-supervised\n",
            "  Attempting uninstall: fastcore\n",
            "    Found existing installation: fastcore 1.5.25\n",
            "    Uninstalling fastcore-1.5.25:\n",
            "      Successfully uninstalled fastcore-1.5.25\n",
            "Successfully installed fastcore-1.4.5 huggingface-hub-0.10.0 kornia-0.6.7 self-supervised-1.0.4 timm-0.6.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (22.1.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.15.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (8.14.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest) (57.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.11.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipytest\n",
            "  Downloading ipytest-0.12.0-py3-none-any.whl (15 kB)\n",
            "Collecting pytest>=5.4\n",
            "  Downloading pytest-7.1.3-py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 14.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (21.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.9.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (4.12.0)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (22.1.0)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (1.11.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (4.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 68.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (5.1.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (57.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.0.10)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipytest) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipytest) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipytest) (0.7.0)\n",
            "Installing collected packages: pluggy, jedi, iniconfig, pytest, ipytest\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed iniconfig-1.1.1 ipytest-0.12.0 jedi-0.18.1 pluggy-1.0.0 pytest-7.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0\n",
        "!pip install fastai==2.6.3 --no-deps\n",
        "!pip install self_supervised\n",
        "\n",
        "!pip install pytest\n",
        "!pip install ipytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "BOv4kkJDag8r",
        "outputId": "49fee111-45ba-4c1c-898f-c4a940fae234"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pk01WY_Dag8s"
      },
      "outputs": [],
      "source": [
        "import fastai\n",
        "import self_supervised\n",
        "import torch\n",
        "if torch.cuda.is_available():device='cuda'\n",
        "else:device='cpu'\n",
        "assert(fastai.__version__ == '2.6.3') #Check that version is 2.6.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOjr_YCLag8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7019d8a5-9001-4424-aa35-b9501f91d059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /usr/local/lib/python3.7/dist-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
            "  warn(f\"Failed to load image Python extension: {e}\")\n"
          ]
        }
      ],
      "source": [
        "from fastai.vision.all import *\n",
        "from self_supervised.augmentations import *\n",
        "from self_supervised.layers import *\n",
        "from torchvision import transforms\n",
        "import inspect\n",
        "import warnings\n",
        "import random\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import ipytest\n",
        "ipytest.autoconfig()\n",
        "import pytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTSdKC6bag8t"
      },
      "outputs": [],
      "source": [
        "#Like most other SSL algorithms BT's model consists of an encoder and projector (MLP) layer.\n",
        "#Definition is straightforward:\n",
        "#https://colab.research.google.com/github/KeremTurgutlu/self_supervised/blob/master/nbs/14%20-%20barlow_twins.ipynb#scrollTo=1M6QcUChcvpz\n",
        "class BarlowTwinsModel(Module):\n",
        "    \"\"\"An encoder followed by a projector\n",
        "    \"\"\"\n",
        "    def __init__(self,encoder,projector):self.encoder,self.projector = encoder,projector\n",
        "        \n",
        "    def forward(self,x): return self.projector(self.encoder(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZL3EE07Pag8u"
      },
      "outputs": [],
      "source": [
        "#HOWEVER instead of directly using the above, by passing both an encoder and a projector, create_barlow_twins_model\n",
        "#function can be used by minimally passing a predefined encoder and the expected input channels.\n",
        "\n",
        "#In the paper it's mentioned that MLP layer consists of 3 layers... following function will create a 3 layer\n",
        "#MLP projector with batchnorm and ReLU by default. Alternatively, you can change bn and nlayers. \n",
        "\n",
        "#Questions: Why torch.no_grad() when doing this?\n",
        "def create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n",
        "    \"Create Barlow Twins model\"\n",
        "    n_in  = in_channels(encoder)\n",
        "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
        "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n",
        "    apply_init(projector)\n",
        "    return BarlowTwinsModel(encoder, projector)\n",
        "\n",
        "#Similar to above. Simple API to make the BT model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFjGL-COag8v"
      },
      "outputs": [],
      "source": [
        "#BarlowTwins Callback\n",
        "#The following parameters can be passed:\n",
        "# - aug_pipelines\n",
        "# Imb lambda is the weight for redundancy reduction term in the loss function\n",
        "\n",
        "@delegates(get_multi_aug_pipelines)\n",
        "def get_barlow_twins_aug_pipelines(size,**kwargs): return get_multi_aug_pipelines(n=2,size=size,**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx4KsywAag8v"
      },
      "outputs": [],
      "source": [
        "#Uniform random number between a and b\n",
        "def Unif(a,b):\n",
        "    return (b-a)*torch.rand(1).item()+a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zU4GwLruU5AD"
      },
      "outputs": [],
      "source": [
        "def random_sinusoid(x,std=0.1,seed=0):\n",
        "    \n",
        "    seed_everything(seed=seed)    \n",
        "    t=(std) * torch.randn(1,500).to(device)\n",
        "    s=(std) * torch.randn(1,500).to(device)\n",
        "    \n",
        "    u=torch.randn(1,500).to(device)\n",
        "    v=torch.randn(1,500).to(device)\n",
        "\n",
        "    a=(0.2) * torch.randn(1,500).to(device)\n",
        "    b=(0.2) * torch.randn(1,500).to(device)\n",
        "    # N = torch.abs(a) + torch.abs(b)\n",
        "    # a = a/N\n",
        "    # b = b/N\n",
        "\n",
        "    return a*torch.sin(t*x[:,]*math.pi+u) + b*torch.cos(s*x[:,]*math.pi+v)\n",
        "\n",
        "\n",
        "    \n",
        "    #return torch.sin(t*math.pi*x+u) + torch.cos(s*math.pi*x + v)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#New stuff\n",
        "class Cdiff_Rand:\n",
        "    \n",
        "    def __init__(self,seed,bs,std=0.1,K=2):\n",
        "        self.seed=seed\n",
        "        self.std=std\n",
        "        self.K=2\n",
        "        self.bs=bs\n",
        "\n",
        "    def __call__(self,z1norm,z2norm):\n",
        "        \n",
        "        cdiff_rand=0\n",
        "        for i in range(self.K):\n",
        "\n",
        "            z1norm_2,z2norm_2 = random_sinusoid(z1norm,std=self.std,seed=self.seed+i), random_sinusoid(z2norm,std=self.std,seed=2*self.seed+i)\n",
        "            cdiff_rand = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)\n",
        "\n",
        "        cdiff_rand=(1/self.K)*cdiff_rand\n",
        "    \n",
        "        return cdiff_rand\n",
        "  "
      ],
      "metadata": {
        "id": "Xej8_KxJ71Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#New stuff\n",
        "def C_z1z2(z1norm,z1norm_2,z2norm,z2norm_2,bs):\n",
        "    \n",
        "    Ctem1 =  (z1norm.T @ z2norm_2) / bs\n",
        "    Ctem2 = (z1norm_2.T @ z2norm) / bs\n",
        "    cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2))\n",
        "\n",
        "    return cdiff_2\n",
        "\n",
        "class Max_Corr(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(ps,ps)\n",
        "        self.fc2 = nn.Linear(ps,ps)\n",
        "\n",
        "        self.fc3 = nn.Linear(ps,ps)\n",
        "        self.fc4 = nn.Linear(ps,ps)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self,x,y):\n",
        "\n",
        "        x=self.sigmoid(self.fc1(x)) #when (sigmoid,relu) GREAT results, with (sigmoid,sigmoid) TERRIBLE. Currently testing (relu,relu)\n",
        "        x=self.fc2(x)\n",
        "       \n",
        "        y=self.relu(self.fc3(y)) #originally had relu and got really good results. If we can't reproduce those results, possible reasons:\n",
        "                                    #results were due to chance; or having relu on one branch (and sigmoid on the other) helps via breaking\n",
        "                                      #the symmetry! Other idea: set fc1=fc3, fc2=fc4. \n",
        "        y=self.fc4(y)\n",
        "\n",
        "        return x,y\n",
        "\n",
        "class Cdiff_Sup:\n",
        "    \n",
        "    def __init__(self,I,inner_steps,bs):\n",
        "        \n",
        "        self.I=I\n",
        "        self.inner_steps=inner_steps\n",
        "        self.bs=bs\n",
        "        self.max_corr = Max_Corr()\n",
        "        if device == 'cuda':\n",
        "            self.max_corr.cuda()\n",
        "        \n",
        "    def inner_step(self,z1norm,z2norm):\n",
        "    \n",
        "        max_corr=self.max_corr\n",
        "        I=self.I\n",
        "        bs=self.bs\n",
        "        inner_steps=self.inner_steps\n",
        "\n",
        "        z1norm=z1norm.detach()\n",
        "        z2norm=z2norm.detach()\n",
        "\n",
        "        # z1norm=z1norm[:,0]\n",
        "        # z2norm=z2norm[:,0]\n",
        "\n",
        "        max_corr = Max_Corr()\n",
        "        max_corr.cuda()\n",
        "    \n",
        "        # for p in max_corr.parameters():\n",
        "        #     p.requires_grad=True]\n",
        "\n",
        "        optimizer = torch.optim.Adam(list(max_corr.parameters()),lr=0.001)\n",
        "        for i in range(inner_steps):\n",
        "            z1norm_2,z2norm_2=max_corr(z1norm,z2norm)\n",
        "            #z1norm_2 = (z1norm_2 - z1norm_2.mean(0)) / z1norm_2.std(0, unbiased=False)\n",
        "        \n",
        "            assert (z1norm_2.shape,z2norm_2.shape) == (z1norm.shape,z2norm.shape)\n",
        "\n",
        "            # Ctem1 =  (z1norm.T @ z2norm_2) / bs\n",
        "            # Ctem2 = (z1norm_2.T @ z2norm) / bs\n",
        "            # cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2))\n",
        "\n",
        "            cdiff_2 = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)\n",
        "            #cdiff_2=Ctem.pow(2)\n",
        "\n",
        "            inner_loss=-1*(cdiff_2*(1-I)).mean()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            inner_loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        for p in max_corr.parameters():\n",
        "            p.requires_grad=False\n",
        "            \n",
        "        return max_corr\n",
        "    \n",
        "    def __call__(self,z1norm,z2norm):\n",
        "        \n",
        "            max_corr =  self.inner_step(z1norm,z2norm)\n",
        "            z1norm_2,z2norm_2 = max_corr(z1norm,z2norm)\n",
        "      \n",
        "            cdiff_sup = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs)\n",
        "    \n",
        "            return cdiff_sup\n"
      ],
      "metadata": {
        "id": "XsNBXcIjyjD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2Exs2s3ag8z"
      },
      "outputs": [],
      "source": [
        "#export\n",
        "class BarlowTwins(Callback):\n",
        "    order,run_valid = 9,True\n",
        "    def __init__(self, aug_pipelines, lmb=5e-3, print_augs=False):\n",
        "        assert_aug_pipelines(aug_pipelines)\n",
        "        self.aug1, self.aug2 = aug_pipelines\n",
        "        if print_augs: print(self.aug1), print(self.aug2)\n",
        "        store_attr('lmb')\n",
        "        self.index=-1\n",
        "\n",
        "        self.inner_steps=4\n",
        "        \n",
        "    def before_fit(self): \n",
        "        self.learn.loss_func = self.lf\n",
        "        nf = self.learn.model.projector[-1].out_features\n",
        "        self.I = torch.eye(nf).to(self.dls.device)\n",
        "\n",
        "    def update_seed(self):\n",
        "        \n",
        "        indexmod=2\n",
        "        if self.index%indexmod == 0: #every `indexmod` index update the seed (best we have found so far)\n",
        "            self.seed = np.random.randint(0,10000)\n",
        "\n",
        "    def before_epoch(self):\n",
        "        self.index=-1\n",
        "\n",
        "        if self.epoch%10==0:\n",
        "            self.inner_steps += 1\n",
        "            \n",
        "    def before_batch(self):\n",
        "        xi,xj = self.aug1(self.x), self.aug2(self.x)\n",
        "        self.learn.xb = (torch.cat([xi, xj]),)\n",
        "\n",
        "        self.index=self.index+1\n",
        "\n",
        "        self.update_seed()\n",
        "\n",
        "        \n",
        "        #Uncomment to run standard BT\n",
        "    # def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
        "    #     bs,nf = pred.size(0)//2,pred.size(1)\n",
        "\n",
        "    #     z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
        "\n",
        "    #     z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
        "    #     z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
        "        \n",
        "    #     C = (z1norm.T @ z2norm) / bs \n",
        "    #     cdiff = (C - self.I)**2\n",
        "    #     loss = (cdiff*self.I + cdiff*(1-self.I)*self.lmb).sum() \n",
        "    #     return loss\n",
        "\n",
        "\n",
        "    def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
        "        bs,nf = pred.size(0)//2,pred.size(1)\n",
        "\n",
        "        #All standard, from BT\n",
        "        z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
        "        z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
        "        z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
        "        \n",
        "        C = (z1norm.T @ z2norm) / bs \n",
        "        cdiff = (C - self.I)**2\n",
        "\n",
        "\n",
        "\n",
        "        # #Let's change this block to rewritten (should do same thing)\n",
        "        # max_corr = inner_step(z1norm,z2norm,I=self.I,inner_steps=5)#,inner_steps=self.inner_steps)\n",
        "        # z1norm_2,z2norm_2 = max_corr(z1norm,z2norm)\n",
        "        # Ctem1 =  (z1norm.T @ z2norm_2) / bs\n",
        "        # Ctem2 = (z1norm_2.T @ z2norm) / bs\n",
        "        # #Ctem = (z1norm_2.T @ z2norm_2) / bs\n",
        "        # #cdiff_2 = Ctem.pow(2)\n",
        "        # cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2)) #+ 0.1*Ctem.pow(2)\n",
        "\n",
        "\n",
        "        CdiffSup = Cdiff_Sup(I=self.I,inner_steps=5,bs=bs)\n",
        "        cdiff_2 = CdiffSup(z1norm,z2norm)\n",
        "\n",
        "        CdiffRand = Cdiff_Rand(seed=self.seed,bs=bs,std=0.2,K=2)\n",
        "        cdiff_2_2 = CdiffRand(z1norm,z2norm)\n",
        "\n",
        "        # #Let's change this block to rewritten (should do same thing)\n",
        "        # K=2\n",
        "        # cdiff_2_2=0\n",
        "        # for i in range(K):\n",
        "        #     #p=Exp_sample(loc=1.5,scale=2.0)\n",
        "        #     #p=Unif(0.9,2.5)\n",
        "        #     #z1norm_2 = p_norm(z1norm,p=p)\n",
        "\n",
        "        #     #p=Exp_sample(loc=1.5,scale=2.0)\n",
        "        #     #p=Unif(0.9,2.5)\n",
        "        #     #z2norm_2 = p_norm(z2norm,p=p)\n",
        "\n",
        "        #     z1norm_2 = random_sinusoid(z1norm,std=0.1,seed=self.seed+i)\n",
        "        #     z2norm_2 = random_sinusoid(z2norm,std=0.1,seed=2*self.seed+i)\n",
        "\n",
        "        #     #C = (z1norm_2.T @ z2norm_2) / bs\n",
        "        #     C_1 = (z1norm.T @ z2norm_2) / bs\n",
        "        #     C_2 = (z1norm_2.T @ z2norm) / bs\n",
        "\n",
        "        #     #cdiff_2 = 0.5*(C_1).pow(2) + 0.5*(C_2).pow(2)\n",
        "        #     #cdiff_2_2 = cdiff_2_2 + cdiff_2\n",
        "        #     cdiff_2_2 = cdiff_2_2 + 0.5*C_1.pow(2)+0.5*C_2.pow(2)\n",
        "        \n",
        "        # cdiff_2_2=(1/K)*cdiff_2_2\n",
        "\n",
        "\n",
        "        cdiff_2 = 0.5*cdiff_2_2 + 0.5*cdiff_2\n",
        "            \n",
        "            \n",
        "        l2 = cdiff_2*(1-self.I)*self.lmb #Is either the standard term - or not.\n",
        "\n",
        "\n",
        "        loss = (cdiff*self.I + l2).sum()\n",
        "        torch.cuda.empty_cache()\n",
        "        return loss\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def show(self, n=1):\n",
        "        bs = self.learn.x.size(0)//2\n",
        "        x1,x2  = self.learn.x[:bs], self.learn.x[bs:] \n",
        "        idxs = np.random.choice(range(bs),n,False)\n",
        "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n",
        "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n",
        "        images = []\n",
        "        for i in range(n): images += [x1[i],x2[i]] \n",
        "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vbS1WtLiag80",
        "outputId": "8a0b4cd1-5225-4a82-a901-61ab409cb819"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='143' class='' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      71.50% [143/200 49:33&lt;19:45]\n",
              "    </div>\n",
              "    \n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>110.779106</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>74.234634</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>52.492622</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>40.072422</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>33.836143</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>27.253843</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>21.949947</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>18.688208</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>16.096043</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>13.687980</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>14.963575</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>15.612312</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>13.335078</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>11.976429</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>11.286292</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>9.762931</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>8.806207</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>8.308605</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>7.475433</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>7.323233</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>9.338412</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>8.760827</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>7.971734</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>7.619959</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>6.749820</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>6.052350</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>5.758234</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>5.340474</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>5.059634</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>5.435072</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>5.658566</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>5.318317</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>5.204023</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>4.866459</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>4.458207</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>4.315938</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>4.216592</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>3.905951</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>3.998163</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>4.230433</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>4.131938</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>4.252696</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>4.419564</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>3.974334</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>3.673074</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>3.549070</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>3.287975</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>3.182761</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>3.163358</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.957541</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.899845</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>3.211954</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>3.314447</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>3.098312</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>2.972043</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>2.865613</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>2.732401</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>2.676212</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>3.001448</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>3.097114</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>3.138004</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>2.981827</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.675734</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>2.441550</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>2.451653</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>2.275048</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>2.207660</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>2.563385</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>2.612322</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>2.621104</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.636045</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>2.430530</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>2.228288</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>2.218294</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>2.104448</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>2.025349</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>2.159915</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>2.584899</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>2.437349</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>2.428231</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.312792</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>2.082015</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.988639</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.919531</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.781552</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.791064</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.800270</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>1.737029</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.799587</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.922585</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.832519</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.768391</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>1.823798</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>1.898142</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.789963</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>1.835227</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>1.892405</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>1.776964</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.814967</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.783964</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.644699</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>1.603525</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>1.578169</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>1.476819</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>1.505750</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.833710</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>1.896795</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>1.844095</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>1.792684</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>1.614474</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.503160</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>1.527770</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>1.447817</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>1.476062</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>1.568738</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.513561</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>1.441953</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>1.479098</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>1.400714</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>1.347095</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.364249</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>1.377636</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>1.325125</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>1.307904</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>1.616989</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.666601</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>1.704428</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>1.653838</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>1.483139</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>1.349764</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.326517</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>1.248228</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>1.225686</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>1.246639</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>1.296876</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>1.326470</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>1.371469</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>1.271662</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>1.196941</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>1.250569</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.260166</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>1.286321</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>1.363767</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "\n",
              "    <div>\n",
              "      <progress value='16' class='' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      50.00% [16/32 00:10&lt;00:10 1.3343]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>110.779106</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>74.234634</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>52.492622</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>40.072422</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>33.836143</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>27.253843</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>21.949947</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>18.688208</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>16.096043</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>13.687980</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>14.963575</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>15.612312</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>13.335078</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>11.976429</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>11.286292</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>9.762931</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>8.806207</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>8.308605</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>7.475433</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>7.323233</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>9.338412</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>8.760827</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>7.971734</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>7.619959</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>6.749820</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>6.052350</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>5.758234</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>5.340474</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>5.059634</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>5.435072</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>5.658566</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>5.318317</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>5.204023</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>4.866459</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>4.458207</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>4.315938</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>4.216592</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>3.905951</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>3.998163</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>4.230433</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>4.131938</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>4.252696</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>4.419564</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>3.974334</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>3.673074</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>3.549070</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>3.287975</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>3.182761</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>3.163358</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>2.957541</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.899845</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>3.211954</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>3.314447</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>3.098312</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>2.972043</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>2.865613</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>2.732401</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>2.676212</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>3.001448</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>3.097114</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>3.138004</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>2.981827</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.675734</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>2.441550</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>2.451653</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>2.275048</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>2.207660</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>2.563385</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>2.612322</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>2.621104</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.636045</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>2.430530</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>2.228288</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>2.218294</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>2.104448</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>2.025349</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>2.159915</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>2.584899</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>2.437349</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>2.428231</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>2.312792</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>2.082015</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.988639</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.919531</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.781552</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.791064</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.800270</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>1.737029</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.799587</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.922585</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.832519</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.768391</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>1.823798</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>1.898142</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.789963</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>1.835227</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>1.892405</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>1.776964</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.814967</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.783964</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.644699</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>1.603525</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>1.578169</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>1.476819</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>1.505750</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.833710</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>1.896795</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>1.844095</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>1.792684</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>1.614474</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.503160</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>1.527770</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>1.447817</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>1.476062</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>1.568738</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.513561</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>1.441953</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>1.479098</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>1.400714</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>1.347095</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.364249</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>1.377636</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>1.325125</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>1.307904</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>1.616989</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.666601</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>1.704428</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>1.653838</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>1.483139</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>1.349764</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.326517</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>1.248228</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>1.225686</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>1.246639</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>1.296876</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>1.326470</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>1.371469</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>1.271662</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>1.196941</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>1.250569</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.260166</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>1.286321</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>1.363767</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>1.299913</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>1.195943</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>1.250136</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>1.240992</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>1.173416</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>1.122516</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>1.156976</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.087450</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>1.114413</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>1.126918</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>1.062569</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>1.088364</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>1.171441</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>1.107440</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>1.072858</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>1.089849</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>1.060916</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.029933</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>1.051197</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>1.101220</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>1.046269</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>1.110683</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>1.124739</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>1.050475</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>1.039570</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>1.026834</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.961482</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.956406</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.940043</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.910351</td>\n",
              "      <td>None</td>\n",
              "      <td>00:22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.978821</td>\n",
              "      <td>None</td>\n",
              "      <td>00:21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>1.093229</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>1.027253</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>1.003491</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>1.036735</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.989067</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.964535</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.239282</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>1.432462</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>1.295599</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>1.289830</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>1.155230</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>1.033739</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.996277</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.958274</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.895120</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.909563</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.874292</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.852096</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.894293</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.966190</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.922976</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.891186</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.941380</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.912025</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.896458</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.868281</td>\n",
              "      <td>None</td>\n",
              "      <td>00:20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Debugging cell - delete later (similar to cell below)\n",
        "ps=500\n",
        "hs=500\n",
        "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
        "model = create_barlow_twins_model(fastai_encoder, hidden_size=hs,projection_size=ps)# projection_size=1024)\n",
        "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
        "#values for these which is tantamount to doing nothing\n",
        "#So if we choose resize_scale=(1,1) then the images look the same.\n",
        "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
        "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=True)\n",
        "#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwRMSProp(model.parameters(),lr=0.1, mom=0.9)ins(aug_pipelines, print_augs=True)])\n",
        "opt = torch.optim.RMSprop\n",
        "#partial(OptimWrapper, opt=opt)\n",
        "learn = Learner(dls,model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "#learn = Learner(dls, model,opt_func=opt_func, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "\n",
        "learn.fit(200) #300                            "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#new\n",
        "def seed_everything(seed=42):\n",
        "    \"\"\"\"\n",
        "    Seed everything.\n",
        "    \"\"\"   \n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def tune_set(items0,seed=None,bs_tune=20):\n",
        "    \n",
        "    seed_everything(seed=seed)\n",
        "    \n",
        "    items0=items0.shuffle()\n",
        "    d = {'0':0,'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0,'8':0,'9':0}\n",
        "    ITEMS=[]\n",
        "    for i in items0:\n",
        "        s=str(i).split('/training/')[1][0]\n",
        "        if d[s] is 0 or d[s] is 1:\n",
        "            ITEMS.append(i)\n",
        "            d[s]+=1\n",
        "    #items0=ITEMS\n",
        "\n",
        "    for i in items0:\n",
        "        if i not in ITEMS:\n",
        "            ITEMS.append(i)\n",
        "            \n",
        "    split = IndexSplitter(list(range(bs_tune)))\n",
        "\n",
        "    tds_tune = Datasets(ITEMS, [PILImageBW.create, [parent_label, Categorize()]], splits=split(ITEMS)) #Or do we want this?\n",
        "    dls_tune = tds_tune.dataloaders(bs=bs_tune,num_workers=0, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "    \n",
        "    return dls_tune\n"
      ],
      "metadata": {
        "id": "Fdbbd-uV8Ltr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_items(items,seed):\n",
        "    \"\"\"Helper function to sort a list according to given random seed\n",
        "    \"\"\"\n",
        "    items.sort()\n",
        "    \n",
        "    if seed !=None:\n",
        "        seed_everything(seed=seed)\n",
        "        items=items.shuffle()\n",
        "    \n",
        "    return items\n",
        "\n",
        "class BT_Data:\n",
        "    \n",
        "    def __init__(self,items,seed=42,ts=16384,bs=512,tune_s=2000,bs_tune=20,bs_test=578):\n",
        "        \n",
        "        self.ts=ts\n",
        "        self.bs=bs\n",
        "        self.tune_s=tune_s\n",
        "        self.bs_tune=bs_tune\n",
        "        self.bs_test=bs_test\n",
        "        \n",
        "        self._seed=seed\n",
        "        self.seed=seed\n",
        "        items=shuffle_items(items,seed)\n",
        "        self.items=items\n",
        "\n",
        "    @property\n",
        "    def seed(self):\n",
        "        return self._seed\n",
        "    \n",
        "    @seed.setter #When we update the seed, we update the datasets (so items and dls objects) accordingly\n",
        "    def seed(self,val):\n",
        "        self._seed=val\n",
        "        self.items = shuffle_items(items,val)\n",
        "        self.build_items_i()\n",
        "        self.build_dls()\n",
        "        \n",
        "    def build_items_i(self):\n",
        "        self.items1 = self.items[0:ts] #train BT on these guys\n",
        "        self.items0 = self.items[self.ts:self.ts+self.tune_s] #for fine tuning - just choose 2000 guys to extract 20 for fine tuning \n",
        "        self.items2 = self.items[self.ts+self.tune_s:] #test on remainder\n",
        "        \n",
        "    def build_dls(self):\n",
        "        \n",
        "        split = RandomSplitter(valid_pct=0.0)\n",
        "        tds = Datasets(self.items1, [PILImageBW.create, [parent_label, Categorize()]], splits=split(self.items1))\n",
        "        self.dls = tds.dataloaders(bs=self.bs,num_workers=0, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "\n",
        "        #Evaluate linear classifier on this guy\n",
        "        split = RandomSplitter(valid_pct=0.0) #randomly split training set into training and validation\n",
        "        tds_test = Datasets(self.items2, [PILImageBW.create, [parent_label, Categorize()]], splits=split(self.items2)) #Or do we want this?\n",
        "        self.dls_test = tds_test.dataloaders(bs=self.bs_test,num_workers=0, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "\n",
        "\n",
        "def build_BT_Data(items,seed,tune_seed):\n",
        "    ts=16384\n",
        "    bs=512\n",
        "    tune_s=2000\n",
        "    bs_tune=20\n",
        "    bs_test=578\n",
        "    \n",
        "    k=dict(seed=seed,ts=ts,bs=bs,tune_s=tune_s,bs_tune=bs_tune,bs_test=bs_test)\n",
        "    bt_dataset = BT_Data(items=items,**k)\n",
        "\n",
        "    items = bt_dataset.items\n",
        "    items1 = bt_dataset.items1\n",
        "    items0 = bt_dataset.items0\n",
        "    items2 = bt_dataset.items2\n",
        "\n",
        "    dls = bt_dataset.dls\n",
        "    dls_test = bt_dataset.dls_test\n",
        "\n",
        "    dls_tune=tune_set(items0=items0,seed=seed,bs_tune=bs_tune)\n",
        "    \n",
        "    return dict(seed=seed,tune_seed=tune_seed,items=items,items1=items1,items0=items0,items2=items2,dls=dls,dls_test=dls_test,dls_tune=dls_tune)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "89bdbbno8TNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new\n",
        "\n",
        "path = untar_data(URLs.MNIST)\n",
        "items = get_image_files(path/'training') #i.e. NOT testing!!!\n",
        "items.sort()\n",
        "\n",
        "seed=420\n",
        "tune_seed=100\n",
        "ts=16384\n",
        "bs=512\n",
        "tune_s=2000\n",
        "bs_tune=20\n",
        "bs_test=578\n",
        "\n",
        "bt_data = BT_Data(items=items,seed=seed,ts=ts,bs=bs,tune_s=tune_s,bs_tune=bs_tune,bs_test=bs_test)\n",
        "\n",
        "items1=bt_data.items1\n",
        "items0=bt_data.items0\n",
        "items2=bt_data.items2\n",
        "\n",
        "dls=bt_data.dls\n",
        "dls_test=bt_data.dls_test\n",
        "\n"
      ],
      "metadata": {
        "id": "7gKpW58J8oiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%ipytest -qq\n",
        "#TODO: Rewrite so works for both (seed,tune_seed) = (42,10) and (420,100). At the moment \n",
        "\n",
        "labeller = using_attr(RegexLabeller(pat = r'(\\d+).png$'), 'name')\n",
        "convert_tensor = transforms.ToTensor()\n",
        "Expected_first_item = {42:{'items1':'19825','items0':'40684','items2':'43064'},420:{'items1':'44942','items0':'23821','items2':'908'}}\n",
        "Expected_first_dls = {42:{'dls':0.085169,'dls_test':0.099924},420:{'dls':0.183678,'dls_test':0.162825}}\n",
        "Expected_first_dls_tune = {(42,55):0.093707,(420,55):0.1513355}\n",
        "\n",
        "bt_data_42 = build_BT_Data(items=items,seed=42,tune_seed=10)\n",
        "bt_data_420 = build_BT_Data(items=items,seed=420,tune_seed=100)\n",
        "\n",
        "def verify_DatasetShape(dls_obj,batch_size,ds_settype='train'):\n",
        "    \"\"\"\"Helper function to verify shape of a dls object given the batch size; ds_settype is either `train` or \n",
        "        `valid`. The idea is we want the batch_size to divide the length of the dlsobj.\n",
        "    \"\"\"\n",
        "    \n",
        "    tem = len(getattr(dls_obj,ds_settype)) #length of dlsobj.train or dlsobj.valid depending on settpe\n",
        "    return tem*batch_size == len(getattr(dls_obj,ds_settype+'_ds'))\n",
        "\n",
        "def verify_first_item(items,expected):\n",
        "    \"\"\"Helper function to verify first element of items is as expected, given random seed of 42\n",
        "    \"\"\"\n",
        "        \n",
        "    return labeller(items[0]) == expected\n",
        "        \n",
        "def verify_first_dls(dls_obj,expected,ds_settype='train'):\n",
        "    \"\"\"Helper function to verify first element of the given dls object is as expected, given random seed of 42.\n",
        "        Note that ds_settype is either `train` or `valid\n",
        "    \"\"\"\n",
        "    \n",
        "    #All we are doing here is getting the first tensor in, for example e.g. dls_obj.train_ds and computing\n",
        "    #the mean of all the elements. If random seed is same, then it should give the same results\n",
        "    z=convert_tensor(next(iter(getattr(dls_obj,ds_settype+'_ds')))[0]).mean().item()\n",
        "    \n",
        "    #logging.debug(f'with {ds_settype} has: {z}')\n",
        "    assert z-expected < 0.0001\n",
        "\n",
        "    \n",
        "@pytest.mark.parametrize('bt_dataset',[bt_data_42,bt_data_420])\n",
        "class Test_shapes:\n",
        "    \n",
        "    def test_shape_dlsobjects(self,bt_dataset):\n",
        "        \"\"\"\"Test the shape of each dlsobj\n",
        "        \"\"\"\n",
        "    \n",
        "        assert verify_DatasetShape(bt_dataset['dls'],batch_size=bs,ds_settype='train')\n",
        "\n",
        "        assert verify_DatasetShape(bt_dataset['dls_tune'],batch_size=bs_tune,ds_settype='valid')\n",
        "\n",
        "        assert verify_DatasetShape(bt_dataset['dls_test'],batch_size=bs_test,ds_settype='train')\n",
        "    \n",
        "    def test_length_dlsobjects(self,bt_dataset):\n",
        "        \"\"\"\"Test the length of each dlsobj that we use\n",
        "        \"\"\"\n",
        "        assert len(bt_dataset['dls'].train_ds) == ts and len(bt_dataset['dls_tune'].valid_ds) == bs_tune and len(bt_dataset['dls_test'].train_ds)==41616\n",
        "    \n",
        "    \n",
        "@pytest.mark.parametrize('bt_dataset',[bt_data_42,bt_data_420])   \n",
        "class Test_first:\n",
        "    \n",
        "    def test_first_item(self,bt_dataset):\n",
        "        \"\"\"\"Verify that the first item of each items is as expected\n",
        "        \"\"\"\n",
        "\n",
        "        seed = bt_dataset['seed']\n",
        "\n",
        "        assert verify_first_item(bt_dataset['items1'],Expected_first_item[seed]['items1'])\n",
        "\n",
        "        assert verify_first_item(bt_dataset['items0'],Expected_first_item[seed]['items0'])\n",
        "\n",
        "        assert verify_first_item(bt_dataset['items2'],Expected_first_item[seed]['items2'])\n",
        "\n",
        "    def test_first_dlsobj(self,bt_dataset):\n",
        "        \"\"\"Verify that the first item of each dlsobj is as expected\n",
        "        \"\"\"\n",
        "        seed = bt_dataset['seed']\n",
        "        dls = bt_dataset['dls']\n",
        "        dls_test = bt_dataset['dls_test']\n",
        "        items0 = bt_dataset['items0']\n",
        "\n",
        "        verify_first_dls(dls,ds_settype='train',expected=Expected_first_dls[seed]['dls'])\n",
        "        verify_first_dls(dls_test,ds_settype='train',expected=Expected_first_dls[seed]['dls_test'])\n",
        "\n",
        "        tune_seed=55\n",
        "        dls_tune=tune_set(items0,seed=tune_seed,bs_tune=bs_tune)\n",
        "\n",
        "        verify_first_dls(dls_tune,ds_settype='valid',expected=Expected_first_dls_tune[(seed,tune_seed)])\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize('bt_dataset',[bt_data_42,bt_data_420])\n",
        "def test1_tune_set(bt_dataset):\n",
        "    \"\"\"Check whether the function `tune_set` gives us the expected values\"\"\"\n",
        "    \n",
        "\n",
        "    seed=bt_dataset['seed']\n",
        "    tune_seed=bt_dataset['tune_seed']\n",
        "    items0 = bt_dataset['items0']\n",
        "    \n",
        "    if tune_seed==10 and seed==42:\n",
        "        expected = {10:0.12255,11:0.153564,12:0.12781,13:0.129523,14:0.13019}\n",
        "    \n",
        "    elif tune_seed==100 and seed==420:\n",
        "        expected={100:0.136104,101:0.120989,102:0.1381390,103:0.1380412,104:0.14285138}\n",
        "        \n",
        "    for i in range(5):\n",
        "        #seed_everything(seed=seed)\n",
        "        dls_tune=tune_set(items0,seed=tune_seed+i,bs_tune=20)\n",
        "        x_mean=0\n",
        "        for x,y in dls_tune.valid:\n",
        "            x_mean += x.mean()\n",
        "\n",
        "        assert abs(x_mean-expected[tune_seed+i])<0.0001\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z3pyrAB8rpN",
        "outputId": "cd025c63-661c-491b-c55c-5f2ef538225e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                   [100%]\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "AKbw2pxMag82",
        "outputId": "ad338376-6361-44a8-afcd-4c1cbc00eb5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#A \"reasonable\" composite augmentation: initially copy pasted BT. We run this cell a few times to check it makes sense\n",
        "#Also define encoder and model\n",
        "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
        "model = create_barlow_twins_model(fastai_encoder, hidden_size=10,projection_size=10)# projection_size=1024)\n",
        "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
        "#values for these which is tantamount to doing nothing\n",
        "#So if we choose resize_scale=(1,1) then the images look the same.\n",
        "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
        "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=False)\n",
        "#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "\n",
        "#dls.valid.bs = len(dls.valid_ds) #Set the validation dataloader batch size to be the length of the validation dataset\n",
        "\n",
        "b = dls.one_batch()\n",
        "learn._split(b)\n",
        "learn('before_batch')\n",
        "axes = learn.barlow_twins.show(n=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple linear classifier\n",
        "class LinearClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self,zdim):\n",
        "            \n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(zdim,10) #As 10 classes for mnist\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = cast(self.fc1(x),Tensor) #so we have to use cross entropy loss. cast is because using old version fastai \n",
        "        return x\n",
        "\n",
        "def turnoffgrad_model(fastai_encoder):\n",
        "    for p in fastai_encoder.parameters():\n",
        "        p.requires_grad=False\n",
        "        \n",
        "    return fastai_encoder\n",
        "\n",
        "#NB: Will give same random 20-tune set (for fixed random seed), only if the cell\n",
        "#\"#Get the dataloader and set batch size\" is the same. Perhaps later we can make this cell a function of that one. \n",
        "#Functions to train and evaluate head\n",
        "fastai_encoder.eval()\n",
        "encoder_nograd = turnoffgrad_model(fastai_encoder) \n",
        "def train_head(encoder_nograd,tune_seed=10,bs_tune=20): #The seed choses a different (20) samples for training the head. 2 of each class\n",
        "    \"\"\"Train head on a tune_set, chosen through given tune_seed for reproducibility if needed\n",
        "    \"\"\"\n",
        "                                    # of the tune_seed)\n",
        "    \n",
        "    dls_tune=tune_set(items0,seed=tune_seed,bs_tune=bs_tune) #different random tune set each time (but as a function of tune_seed)\n",
        " \n",
        "    N=len(dls_tune.valid)*bs_tune \n",
        "    assert N == len(dls_tune.valid_ds) #Check that the tune set (valid) is divided by the batch size\n",
        "    assert len(dls_tune.valid_ds) == bs_tune\n",
        "\n",
        "    zdim=1024 #see above\n",
        "    head = LinearClassifier(zdim=zdim)\n",
        "    head.to(device)\n",
        "    optimizer = torch.optim.Adam(head.parameters())\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(200):\n",
        "        #for x,y in dls_tune.valid: #Slows massively on colab but not on kaggle. Weird. \n",
        "        x,y=dls_tune.valid.one_batch() #Same every time since dataset only has length=batch size = 20.\n",
        "                                        #Will need to fix this for CIFAR10 etc\n",
        "\n",
        "        loss = criterion(head(encoder_nograd(x)),y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return head\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_head(head):\n",
        "    \"\"\"Evaluate the (typically trained) head on on the test set\n",
        "    \"\"\"\n",
        "    N=len(dls_test.train)*bs_test\n",
        "    assert N == len(dls_test.train_ds)\n",
        "\n",
        "    num_correct=0\n",
        "    for x,y in dls_test.train:\n",
        "\n",
        "        ypred = head(encoder_nograd(x))\n",
        "        correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n",
        "        num_correct += correct.sum()\n",
        "    \n",
        "    return num_correct/N\n",
        "\n",
        "def eval_encoder(encoder_nograd,tune_seed=10):\n",
        "    \"\"\"\"Evaluate the encoder, which means to train and evaluate the head - basically wrap functions train_head\n",
        "        and eval_head\n",
        "    \"\"\"\n",
        "    head=train_head(encoder_nograd,tune_seed=tune_seed)\n",
        "    pct_correct = eval_head(head)\n",
        "    return pct_correct\n",
        "    "
      ],
      "metadata": {
        "id": "IXTxgA9-Mhih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "assert tune_seed==100\n",
        "assert seed == 420\n",
        "performance_dict={}\n",
        "for num in range(5):\n",
        "    \n",
        "    pct_correct = eval_encoder(encoder_nograd,tune_seed=tune_seed+num)\n",
        "    performance_dict[f'seed_{num}'] = pct_correct \n",
        "\n",
        "print(torch.mean(tensor(list(performance_dict.values()))))\n",
        "performance_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKgPajGBMpSn",
        "outputId": "2d8a7c13-2d7e-49c8-f5bc-a5914fa58983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8126)\n",
            "CPU times: user 4min 5s, sys: 9.22 s, total: 4min 14s\n",
            "Wall time: 4min 13s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'seed_0': TensorCategory(0.8016),\n",
              " 'seed_1': TensorCategory(0.7849),\n",
              " 'seed_2': TensorCategory(0.8416),\n",
              " 'seed_3': TensorCategory(0.8364),\n",
              " 'seed_4': TensorCategory(0.7986)}"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpcqDLMJWpkB",
        "outputId": "f8f4b517-a880-48a9-908c-3c16d3d82137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8126)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'seed_0': TensorCategory(0.8016),\n",
              " 'seed_1': TensorCategory(0.7849),\n",
              " 'seed_2': TensorCategory(0.8416),\n",
              " 'seed_3': TensorCategory(0.8364),\n",
              " 'seed_4': TensorCategory(0.7986)}"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "#test: 0.1361,0.1210\n",
        "#BT baseline, new random seeds, 0.7693\n",
        "\n",
        "print(torch.mean(tensor(list(performance_dict.values()))))\n",
        "performance_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "G1gszrPuMuzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#200 learn epochs\n",
        "BT_42_10_run1={'seed_0': TensorCategory(0.7667),\n",
        " 'seed_1': TensorCategory(0.6843),\n",
        " 'seed_2': TensorCategory(0.8468),\n",
        " 'seed_3': TensorCategory(0.7687),\n",
        " 'seed_4': TensorCategory(0.7288)}\n",
        "print(torch.mean(tensor(list(BT_42_10_run1.values()))))#tensor(0.7591)\n",
        "\n",
        "MBT_42_10_run1 = {'seed_0': TensorCategory(0.8238),\n",
        " 'seed_1': TensorCategory(0.7834),\n",
        " 'seed_2': TensorCategory(0.8999),\n",
        " 'seed_3': TensorCategory(0.8601),\n",
        " 'seed_4': TensorCategory(0.8221)}\n",
        "\n",
        "print(torch.mean(tensor(list(MBT_42_10_run1.values()))))#tensor(0.8379)\n",
        "\n",
        "BT_420_100_run1 = {'seed_0': TensorCategory(0.7532),\n",
        " 'seed_1': TensorCategory(0.7501),\n",
        " 'seed_2': TensorCategory(0.7808),\n",
        " 'seed_3': TensorCategory(0.7804),\n",
        " 'seed_4': TensorCategory(0.7461)}\n",
        "\n",
        "print(torch.mean(tensor(list(BT_420_100_run1.values()))))#tensor(0.7621)\n",
        "\n",
        "MBT_420_100_run1={'seed_0': TensorCategory(0.8016),\n",
        " 'seed_1': TensorCategory(0.7849),\n",
        " 'seed_2': TensorCategory(0.8416),\n",
        " 'seed_3': TensorCategory(0.8364),\n",
        " 'seed_4': TensorCategory(0.7986)}\n",
        "\n",
        "print(torch.mean(tensor(list(MBT_420_100_run1.values())))) #tensor(0.8126)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KMNc94mz5CJ",
        "outputId": "8b0b52ab-d817-4c04-b99c-8374c0d1e4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.7591)\n",
            "tensor(0.8379)\n",
            "tensor(0.7621)\n",
            "tensor(0.8126)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#100 learn_epochs, 420 and 100 random seeds used instead of 42 and 10.\n",
        "BT_1={'seed_0': TensorCategory(0.7614),\n",
        " 'seed_1': TensorCategory(0.7328),\n",
        " 'seed_2': TensorCategory(0.7640),\n",
        " 'seed_3': TensorCategory(0.8039),\n",
        " 'seed_4': TensorCategory(0.7309)}\n",
        "print(torch.mean(tensor(list(BT.values())))) #tensor(0.7586)\n",
        "\n",
        "BT_2={'seed_0': TensorCategory(0.7413),\n",
        " 'seed_1': TensorCategory(0.7645),\n",
        " 'seed_2': TensorCategory(0.7981),\n",
        " 'seed_3': TensorCategory(0.8010),\n",
        " 'seed_4': TensorCategory(0.7355)}\n",
        "print(torch.mean(tensor(list(BT_2.values())))) #tensor(0.7681)\n",
        "\n",
        "MBT_1 = {'seed_0': TensorCategory(0.7721),\n",
        " 'seed_1': TensorCategory(0.7693),\n",
        " 'seed_2': TensorCategory(0.8398),\n",
        " 'seed_3': TensorCategory(0.8129),\n",
        " 'seed_4': TensorCategory(0.7908)}\n",
        "print(torch.mean(tensor(list(MBT_1.values())))) #tensor(0.7970)\n",
        "\n",
        "MBT_2 = {'seed_0': TensorCategory(0.8126),\n",
        " 'seed_1': TensorCategory(0.7781),\n",
        " 'seed_2': TensorCategory(0.8234),\n",
        " 'seed_3': TensorCategory(0.8280),\n",
        " 'seed_4': TensorCategory(0.8084)}\n",
        "print(torch.mean(tensor(list(MBT_2.values())))) #tensor(0.8101)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yUHZSinCsrt4",
        "outputId": "9c8ed2a0-eb46-423f-a0a0-c6dbeee1d333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0f6bbb4458fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m  \u001b[0;34m'seed_3'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensorCategory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8039\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m  'seed_4': TensorCategory(0.7309)}\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#tensor(0.7586)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m BT_2={'seed_0': TensorCategory(0.7413),\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BT' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please see commit  e849943... 4/10/22 if needed. We have edited the base functions in that file to try and make them nicer, but we need to make sure we can reproduce results (i.e. the changes make things nicer but don't actually change anything).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zjR-EnBubM6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With: fc1,fc2,fc3,fc4 distinct. Indexmod=2, K=2 With Max_corr = (sigmoid,relu); \n",
        "$a\\sim b$ = 0.2 x N(0,1): 0.8576,0.8364,0.8393\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T4bY6DPjb8MY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below applies to prior implementation, we leave it here for now.\n"
      ],
      "metadata": {
        "id": "KjPF34iucYWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All of the below have sin and cos with constant coefficients. If we take `best so far` and give it random coefficients a and b with std=0.2 then get:\n",
        "\n",
        "With fc1,fc2,fc3,fc4 distinct. Indexmod=2, K=2 With Max_corr = (sigmoid,relu);a~b = 0.2 x N(0,1) **0.8390, 0.8553** Conclusion: We need to search over the a and b parameters (coefficients of sinusoids) when we do our big search. Or rather search over std the hps controlling how we sample a and b. "
      ],
      "metadata": {
        "id": "-Rmtuu4nxc13"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SYRjUytI8Fw"
      },
      "source": [
        "Results (continuing from prior commit):\n",
        "\n",
        "\n",
        "Note these are with 200 learn_epochs etc. Same random seed. See above for other details (and we mention when recording the results below that we varied)\n",
        "BT = 0.7581\n",
        "\n",
        "(These are with fc1,fc2,fc3,fc4 distinct). Indexmod=2, K=2 \n",
        "**With Max_corr = (sigmoid,relu): 0.8493,0.8332,0.8392. Best so far.**\n",
        "With Max_corr = (sigmoid,sigmoid): 0.3080,0.3219.   \n",
        "With Max_corr = (relu,relu): 0.8132,0.8142,0.8093.   \n",
        "\n",
        "(These are with fc1=fc3, fc2=fc4. i.e. just one NN applied)  \n",
        "With Max_corr = (sigmoid,relu): 0.8359,0.8258,0.8303\n",
        "\n",
        "Above are all with indexmod=2. Now we try removing the indexmod condition.\n",
        "Then get:\n",
        "Indexmod=0\n",
        "With Max_corr = (sigmoid,relu): 0.8264,0.8253 \n",
        "\n",
        "Try indexmod=4\n",
        "With Max_corr = (sigmoid,relu):0.8174\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snRrKfwCH6XD"
      },
      "source": [
        "Results on different MBT runs (see above for implementation): tensor(0.8493) (trying to reproduce now) (just changed y=self.sigmoid(self.fc3(y))\n",
        "from relu in MaxCorr). Performance went to tensor(0.3080)!!! Crazy. Let's change y back to relu and see what happens. result: tensor(0.8332)!! Wow. Similar\n",
        "to before (i.e. evidence in favour of reproducibility). All we changed was sigmoid to relu!\n",
        "\n",
        "To summarise: Max_Corr had (sigmoid,relu) and got great results (on two diff MBT runs 0.8493 and 0.8332. When we changed relu to sigmoid, got terrible results (0.3080). (As an aside the loss jumped around a lot)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPEDQE-rcKjL",
        "outputId": "8e127c85-873f-4a36-d76a-66a57c566f74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.8252)\n"
          ]
        }
      ],
      "source": [
        "#250 learn_epochs. To beat: (K=10,indexmod=4,std=0.1,convex=(0.2,0.8))\n",
        "tem={'seed_0': TensorCategory(0.8364),\n",
        " 'seed_1': TensorCategory(0.7424),\n",
        " 'seed_2': TensorCategory(0.8947),\n",
        " 'seed_3': TensorCategory(0.8392),\n",
        " 'seed_4': TensorCategory(0.8133)}\n",
        "print(torch.mean(tensor(list(tem.values())))) #tensor(0.8252)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZPO88U4XMgb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYI6BiCTXCmt",
        "outputId": "c99dc95d-0a71-4fc3-d791-16cee98bfa8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.8487)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "hw0VjYxRx1va",
        "outputId": "659f5266-600d-4b32-a834-a84fff7d18b3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-c8c02caa2cf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperformance_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'performance_dict' is not defined"
          ]
        }
      ],
      "source": [
        "performance_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COooHESjoNGb"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}