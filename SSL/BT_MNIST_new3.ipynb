{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamish-haggerty/AI-hacking/blob/master/SSL/BT_MNIST_new3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8jFsEXz_61O",
        "outputId": "3e2c58b1-01a5-4ef8-d0d2-9de13a4a7152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting self_supervised\n",
            "  Downloading self_supervised-1.0.4-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 306 kB/s \n",
            "\u001b[?25hRequirement already satisfied: fastai>=2.2.7 in /usr/local/lib/python3.7/dist-packages (from self_supervised) (2.7.9)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from self_supervised) (21.1.3)\n",
            "Collecting kornia>=0.5.0\n",
            "  Downloading kornia-0.6.8-py2.py3-none-any.whl (551 kB)\n",
            "\u001b[K     |████████████████████████████████| 551 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from self_supervised) (21.3)\n",
            "Collecting timm>=0.4.5\n",
            "  Downloading timm-0.6.11-py3-none-any.whl (548 kB)\n",
            "\u001b[K     |████████████████████████████████| 548 kB 80.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (0.13.1+cu113)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.0.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (0.0.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (3.2.2)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (7.1.2)\n",
            "Requirement already satisfied: torch<1.14,>=1.7 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.12.1+cu113)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.3.5)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (3.4.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.7.3)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.0.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (6.0)\n",
            "Requirement already satisfied: fastcore<1.6,>=1.4.5 in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (1.5.27)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai>=2.2.7->self_supervised) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.10.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (8.1.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.6.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.11.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.4.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.9.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.4.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.1.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (57.4.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.3.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4->fastai>=2.2.7->self_supervised) (3.9.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->self_supervised) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4->fastai>=2.2.7->self_supervised) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai>=2.2.7->self_supervised) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai>=2.2.7->self_supervised) (0.7.8)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai>=2.2.7->self_supervised) (0.0.3)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 89.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4->fastai>=2.2.7->self_supervised) (7.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm>=0.4.5->self_supervised) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm>=0.4.5->self_supervised) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4->fastai>=2.2.7->self_supervised) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai>=2.2.7->self_supervised) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->fastai>=2.2.7->self_supervised) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai>=2.2.7->self_supervised) (2022.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (3.1.0)\n",
            "Installing collected packages: huggingface-hub, timm, kornia, self-supervised\n",
            "Successfully installed huggingface-hub-0.10.1 kornia-0.6.8 self-supervised-1.0.4 timm-0.6.11\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (8.14.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (22.1.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.11.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest) (0.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest) (57.4.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest) (1.4.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipytest\n",
            "  Downloading ipytest-0.12.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ipytest) (7.9.0)\n",
            "Collecting pytest>=5.4\n",
            "  Downloading pytest-7.2.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ipytest) (21.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (22.1.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from pytest>=5.4->ipytest) (4.13.0)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.0.0rc9-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->pytest>=5.4->ipytest) (3.9.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 80.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (57.4.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->ipytest) (2.0.10)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->ipytest) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->ipytest) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->ipytest) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->ipytest) (0.7.0)\n",
            "Installing collected packages: pluggy, jedi, iniconfig, exceptiongroup, pytest, ipytest\n",
            "  Attempting uninstall: pluggy\n",
            "    Found existing installation: pluggy 0.7.1\n",
            "    Uninstalling pluggy-0.7.1:\n",
            "      Successfully uninstalled pluggy-0.7.1\n",
            "  Attempting uninstall: pytest\n",
            "    Found existing installation: pytest 3.6.4\n",
            "    Uninstalling pytest-3.6.4:\n",
            "      Successfully uninstalled pytest-3.6.4\n",
            "Successfully installed exceptiongroup-1.0.0rc9 iniconfig-1.1.1 ipytest-0.12.0 jedi-0.18.1 pluggy-1.0.0 pytest-7.2.0\n"
          ]
        }
      ],
      "source": [
        "#Install\n",
        "!pip install self_supervised\n",
        "\n",
        "!pip install pytest\n",
        "!pip install ipytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pk01WY_Dag8s"
      },
      "outputs": [],
      "source": [
        "#import \n",
        "import fastai\n",
        "import self_supervised\n",
        "import torch\n",
        "if torch.cuda.is_available():device='cuda'\n",
        "else:device='cpu'\n",
        "from fastai.vision.all import *\n",
        "from self_supervised.augmentations import *\n",
        "from self_supervised.layers import *\n",
        "from torchvision import transforms\n",
        "import inspect\n",
        "import warnings\n",
        "import random\n",
        "import math\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import ipytest\n",
        "ipytest.autoconfig()\n",
        "import pytest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "zU4GwLruU5AD"
      },
      "outputs": [],
      "source": [
        "#Base functions / classes we need to train a BT / RBT model.\n",
        "class BarlowTwinsModel(Module):\n",
        "    \"\"\"An encoder followed by a projector\n",
        "    \"\"\"\n",
        "    def __init__(self,encoder,projector):self.encoder,self.projector = encoder,projector\n",
        "        \n",
        "    def forward(self,x): \n",
        "        \n",
        "        return self.projector(self.encoder(x))\n",
        "\n",
        "#In the paper it's mentioned that MLP layer consists of 3 layers... following function will create a 3 layer\n",
        "#MLP projector with batchnorm and ReLU by default. Alternatively, you can change bn and nlayers. \n",
        "\n",
        "#Questions: Why torch.no_grad() when doing this?\n",
        "def create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n",
        "    \"Create Barlow Twins model\"\n",
        "    n_in  = in_channels(encoder)\n",
        "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
        "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n",
        "    apply_init(projector)\n",
        "    return BarlowTwinsModel(encoder, projector)\n",
        "\n",
        "@delegates(get_multi_aug_pipelines)\n",
        "def get_barlow_twins_aug_pipelines(size,**kwargs): return get_multi_aug_pipelines(n=2,size=size,**kwargs)\n",
        "\n",
        "def random_sinusoid(x,std=0.1,seed=0):\n",
        "    \n",
        "    seed_everything(seed=seed)    \n",
        "    t=(std) * torch.randn(1,500).to(device)\n",
        "    s=(std) * torch.randn(1,500).to(device)\n",
        "    \n",
        "    u=torch.randn(1,500).to(device)\n",
        "    v=torch.randn(1,500).to(device)\n",
        "\n",
        "    a=(0.2) * torch.randn(1,500).to(device)\n",
        "    b=(0.2) * torch.randn(1,500).to(device)\n",
        "\n",
        "    return a*torch.sin(t*x[:,]*math.pi+u) + b*torch.cos(s*x[:,]*math.pi+v)\n",
        "\n",
        "def C_z1z2(z1norm,z1norm_2,z2norm,z2norm_2,bs,indep=False):\n",
        "    \n",
        "    if indep == False:\n",
        "        C1 =  (z1norm.T @ z2norm_2) / bs\n",
        "        C2 = (z1norm_2.T @ z2norm) / bs\n",
        "        cdiff = (0.5*C1.pow(2) + 0.5*C2.pow(2))\n",
        "        \n",
        "    elif indep == True:\n",
        "        cdiff =  (z1norm_2.T @ z2norm_2) / bs\n",
        "        \n",
        "    return cdiff\n",
        "\n",
        "class Cdiff_Rand:\n",
        "    \n",
        "    def __init__(self,seed,bs,std=0.1,K=2):\n",
        "        self.seed=seed\n",
        "        self.std=std\n",
        "        self.K=K\n",
        "        self.bs=bs\n",
        "\n",
        "    def __call__(self,z1norm,z2norm):\n",
        "        \n",
        "        cdiff_rand=0\n",
        "        for i in range(self.K):\n",
        "\n",
        "            z1norm_2,z2norm_2 = random_sinusoid(z1norm,std=self.std,seed=self.seed+i), random_sinusoid(z2norm,std=self.std,seed=2*self.seed+i)\n",
        "            cdiff_rand = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs,indep=True)\n",
        "\n",
        "        cdiff_rand=(1/self.K)*cdiff_rand\n",
        "    \n",
        "        return cdiff_rand\n",
        "\n",
        "class Max_Corr(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(ps,ps)\n",
        "        self.fc2 = nn.Linear(ps,ps)\n",
        "\n",
        "        self.fc3 = nn.Linear(ps,ps)\n",
        "        self.fc4 = nn.Linear(ps,ps)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self,x,y):\n",
        "\n",
        "        x=self.sigmoid(self.fc1(x)) #when (sigmoid,relu) GREAT results, with (sigmoid,sigmoid) TERRIBLE. Currently testing (relu,relu)\n",
        "        x=self.fc2(x)\n",
        "       \n",
        "        y=self.relu(self.fc3(y)) #originally had relu and got really good results. If we can't reproduce those results, possible reasons:\n",
        "                                    #results were due to chance; or having relu on one branch (and sigmoid on the other) helps via breaking\n",
        "                                      #the symmetry! Other idea: set fc1=fc3, fc2=fc4. \n",
        "        y=self.fc4(y)\n",
        "\n",
        "        return x,y\n",
        "\n",
        "class Cdiff_Sup:\n",
        "    \n",
        "    def __init__(self,I,inner_steps,bs):\n",
        "        \n",
        "        self.I=I\n",
        "        self.inner_steps=inner_steps\n",
        "        self.bs=bs\n",
        "        self.max_corr = Max_Corr()\n",
        "        if device == 'cuda':\n",
        "            self.max_corr.cuda()\n",
        "        \n",
        "    def inner_step(self,z1norm,z2norm):\n",
        "    \n",
        "        max_corr=self.max_corr\n",
        "        I=self.I\n",
        "        bs=self.bs\n",
        "        inner_steps=self.inner_steps\n",
        "\n",
        "        z1norm=z1norm.detach()\n",
        "        z2norm=z2norm.detach()\n",
        "\n",
        "        # z1norm=z1norm[:,0]\n",
        "        # z2norm=z2norm[:,0]\n",
        "\n",
        "        max_corr = Max_Corr()\n",
        "        max_corr.cuda()\n",
        "    \n",
        "        # for p in max_corr.parameters():\n",
        "        #     p.requires_grad=True]\n",
        "\n",
        "        optimizer = torch.optim.Adam(list(max_corr.parameters()),lr=0.001)\n",
        "        for i in range(inner_steps):\n",
        "            z1norm_2,z2norm_2=max_corr(z1norm,z2norm)\n",
        "            #z1norm_2 = (z1norm_2 - z1norm_2.mean(0)) / z1norm_2.std(0, unbiased=False)\n",
        "        \n",
        "            assert (z1norm_2.shape,z2norm_2.shape) == (z1norm.shape,z2norm.shape)\n",
        "\n",
        "            # Ctem1 =  (z1norm.T @ z2norm_2) / bs\n",
        "            # Ctem2 = (z1norm_2.T @ z2norm) / bs\n",
        "            # cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2))\n",
        "\n",
        "            cdiff_2 = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs,indep=False)\n",
        "            #cdiff_2=Ctem.pow(2)\n",
        "\n",
        "            inner_loss=-1*(cdiff_2*(1-I)).mean()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            inner_loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        for p in max_corr.parameters():\n",
        "            p.requires_grad=False\n",
        "            \n",
        "        return max_corr\n",
        "    \n",
        "    def __call__(self,z1norm,z2norm):\n",
        "        \n",
        "            max_corr =  self.inner_step(z1norm,z2norm)\n",
        "            z1norm_2,z2norm_2 = max_corr(z1norm,z2norm)\n",
        "      \n",
        "            cdiff_sup = C_z1z2(z1norm=z1norm,z1norm_2=z1norm_2,z2norm=z2norm,z2norm_2=z2norm_2,bs=bs,indep=False)\n",
        "    \n",
        "            return cdiff_sup\n",
        "\n",
        "\n",
        "#export\n",
        "class BarlowTwins(Callback):\n",
        "    order,run_valid = 9,True\n",
        "    def __init__(self, aug_pipelines, lmb=5e-3, print_augs=False):\n",
        "        assert_aug_pipelines(aug_pipelines)\n",
        "        self.aug1, self.aug2 = aug_pipelines\n",
        "        if print_augs: print(self.aug1), print(self.aug2)\n",
        "        store_attr('lmb')\n",
        "        self.index=-1\n",
        "\n",
        "        self.inner_steps=5\n",
        "        \n",
        "    def before_fit(self): \n",
        "        self.learn.loss_func = self.lf\n",
        "        nf = self.learn.model.projector[-1].out_features\n",
        "        self.I = torch.eye(nf).to(self.dls.device)\n",
        "\n",
        "    def update_seed(self):\n",
        "        \n",
        "        indexmod=2\n",
        "        if self.index%indexmod == 0: #every `indexmod` index update the seed (best we have found so far)\n",
        "            self.seed = np.random.randint(0,10000)\n",
        "\n",
        "    def before_epoch(self):\n",
        "        self.index=-1\n",
        "\n",
        "            \n",
        "    def before_batch(self):\n",
        "    \n",
        "        xi,xj = self.aug1(TensorImageBW(self.x)), self.aug2(TensorImageBW(self.x))\n",
        "        self.learn.xb = (torch.cat([xi, xj]),)\n",
        "\n",
        "        self.index=self.index+1\n",
        "        self.update_seed()\n",
        "\n",
        "        #Uncomment to run standard BT\n",
        "    # def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
        "    #     bs,nf = pred.size(0)//2,pred.size(1)\n",
        "\n",
        "    #     z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
        "\n",
        "    #     z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
        "    #     z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
        "        \n",
        "    #     C = (z1norm.T @ z2norm) / bs \n",
        "    #     cdiff = (C - self.I)**2\n",
        "    #     loss = (cdiff*self.I + cdiff*(1-self.I)*self.lmb).sum() \n",
        "    #     return loss\n",
        "\n",
        "\n",
        "    def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
        "        \n",
        "        bs,nf = pred.size(0)//2,pred.size(1)\n",
        "\n",
        "        #All standard, from BT\n",
        "        z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
        "        z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
        "        z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
        "        \n",
        "        C = (z1norm.T @ z2norm) / bs \n",
        "        cdiff = (C - self.I)**2\n",
        "\n",
        "\n",
        "\n",
        "        # #Let's change this block to rewritten (should do same thing)\n",
        "        # max_corr = inner_step(z1norm,z2norm,I=self.I,inner_steps=5)#,inner_steps=self.inner_steps)\n",
        "        # z1norm_2,z2norm_2 = max_corr(z1norm,z2norm)\n",
        "        # Ctem1 =  (z1norm.T @ z2norm_2) / bs\n",
        "        # Ctem2 = (z1norm_2.T @ z2norm) / bs\n",
        "        # #Ctem = (z1norm_2.T @ z2norm_2) / bs\n",
        "        # #cdiff_2 = Ctem.pow(2)\n",
        "        # cdiff_2 = (0.5*Ctem1.pow(2) + 0.5*Ctem2.pow(2)) #+ 0.1*Ctem.pow(2)\n",
        "     \n",
        "\n",
        "        CdiffSup = Cdiff_Sup(I=self.I,inner_steps=self.inner_steps,bs=bs)\n",
        "        cdiff_2 = CdiffSup(z1norm,z2norm)\n",
        "\n",
        "        CdiffRand = Cdiff_Rand(seed=self.seed,bs=bs,std=0.2,K=4)\n",
        "        cdiff_2_2 = CdiffRand(z1norm,z2norm)\n",
        "\n",
        "        cdiff_2 = 0.8*cdiff_2_2 + 0.2*cdiff_2 #best so far 0.8,0.2 and K=4 above.\n",
        "            \n",
        "        l2 = cdiff_2*(1-self.I)*self.lmb #Is either the standard term - or not.\n",
        "\n",
        "        loss = (cdiff*self.I + l2).sum()\n",
        "        torch.cuda.empty_cache()\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def show(self, n=1):\n",
        " \n",
        "        bs = self.learn.x.size(0)//2\n",
        "        x1,x2  = self.learn.x[:bs], self.learn.x[bs:]\n",
        "        #x1 = TensorImageBW(x1)\n",
        "        #x2 = TensorImageBW(x2)\n",
        "        idxs = np.random.choice(range(bs),n,False)\n",
        "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n",
        "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n",
        "        images = []\n",
        "        for i in range(n): images += [x1[i],x2[i]]\n",
        "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#New idea \n",
        "\n",
        "#Like most other SSL algorithms BT's model consists of an encoder and projector (MLP) layer.\n",
        "#Definition is straightforward:\n",
        "#https://colab.research.google.com/github/KeremTurgutlu/self_supervised/blob/master/nbs/14%20-%20barlow_twins.ipynb#scrollTo=1M6QcUChcvpz\n",
        "class BarlowTwinsModel(Module):\n",
        "    \"\"\"An encoder followed by a projector\n",
        "    \"\"\"\n",
        "    def __init__(self,encoder,projector,projector2):self.encoder,self.projector,self.projector2 = encoder,projector,projector2\n",
        "        \n",
        "    def forward(self,x): \n",
        "        # print('BarlowTwinsModel...')\n",
        "        # print(x.shape)\n",
        "        # print('shape of x')\n",
        "        # input()\n",
        "\n",
        "        # print(self.encoder(x).shape)\n",
        "        # print('shape of encoder(x)')\n",
        "        # input()\n",
        "        # print(self.projector(self.encoder(x)).shape)\n",
        "        \n",
        "        # input('shape of z')\n",
        "        tem = self.encoder(x)\n",
        "        \n",
        "        return self.projector(tem),self.projector2(tem)\n",
        "    \n",
        "    \n",
        "#HOWEVER instead of directly using the above, by passing both an encoder and a projector, create_barlow_twins_model\n",
        "#function can be used by minimally passing a predefined encoder and the expected input channels.\n",
        "\n",
        "#In the paper it's mentioned that MLP layer consists of 3 layers... following function will create a 3 layer\n",
        "#MLP projector with batchnorm and ReLU by default. Alternatively, you can change bn and nlayers. \n",
        "\n",
        "#Questions: Why torch.no_grad() when doing this?\n",
        "def create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n",
        "    \"Create Barlow Twins model\"\n",
        "    n_in  = in_channels(encoder)\n",
        "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
        "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n",
        "    apply_init(projector)\n",
        "    \n",
        "    projector2 = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n",
        "    apply_init(projector2)\n",
        "    \n",
        "    return BarlowTwinsModel(encoder, projector,projector2)\n",
        "\n",
        "#Similar to above. Simple API to make the BT model:\n",
        "class BarlowTwins(Callback):\n",
        "    order,run_valid = 9,True\n",
        "    def __init__(self, aug_pipelines, lmb=5e-3, print_augs=False):\n",
        "        assert_aug_pipelines(aug_pipelines)\n",
        "        self.aug1, self.aug2 = aug_pipelines\n",
        "        if print_augs: print(self.aug1), print(self.aug2)\n",
        "        store_attr('lmb')\n",
        "        self.index=-1\n",
        "\n",
        "        self.inner_steps=5\n",
        "        \n",
        "    def before_fit(self): \n",
        "        self.learn.loss_func = self.lf\n",
        "        nf = self.learn.model.projector[-1].out_features\n",
        "        self.I = torch.eye(nf).to(self.dls.device)\n",
        "\n",
        "    def update_seed(self):\n",
        "        \n",
        "        indexmod=2\n",
        "        if self.index%indexmod == 0: #every `indexmod` index update the seed (best we have found so far)\n",
        "            self.seed = np.random.randint(0,10000)\n",
        "\n",
        "    def before_epoch(self):\n",
        "        self.index=-1\n",
        "\n",
        "            \n",
        "    def before_batch(self):\n",
        "    \n",
        "        xi,xj = self.aug1(TensorImageBW(self.x)), self.aug2(TensorImageBW(self.x))\n",
        "        self.learn.xb = (torch.cat([xi, xj]),)\n",
        "\n",
        "        self.index=self.index+1\n",
        "        self.update_seed()\n",
        "\n",
        "\n",
        "\n",
        "    #Uncomment to run standard BT\n",
        "    def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
        "        #bs,nf = pred.size(0)//2,pred.size(1)\n",
        "        bs,nf = pred[0].size(0)//2,pred[0].size(1)\n",
        "        seed=self.seed\n",
        "\n",
        "        pred1=pred[0]\n",
        "        pred2=pred[0]\n",
        "\n",
        "        z1, z2 = pred1[:bs],pred1[bs:] #so z1 is bs*projection_size, likewise for z2\n",
        "\n",
        "        #Used to encode, primarily invariance\n",
        "        z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
        "        z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
        "\n",
        "\n",
        "        #Used to encode, primarily redundancy-reduction\n",
        "        z1_two,z2_two = pred2[:bs],pred2[bs:]\n",
        "        z1norm_two = (z1_two - z1_two.mean(0)) / z1_two.std(0, unbiased=False)\n",
        "        z2norm_two = (z2_two - z2_two.mean(0)) / z2_two.std(0, unbiased=False)\n",
        "\n",
        "        #The invariance term\n",
        "        Invar = (z1norm-z2norm).pow(2) #add to loss\n",
        "\n",
        "\n",
        "        #The redundancy reduction term\n",
        "        CdiffRand = Cdiff_Rand(seed=self.seed,bs=bs,std=0.2,K=4)\n",
        "        cdiff = CdiffRand(z1norm_two,z2norm_two)\n",
        "\n",
        "        #New\n",
        "        CdiffSup = Cdiff_Sup(I=self.I,inner_steps=5,bs=bs)\n",
        "        cdiff_2 = CdiffSup(z1norm_two,z2norm_two)\n",
        "\n",
        "        cdiff = 0.8*cdiff + 0.2*cdiff_2\n",
        "\n",
        "        redun_reduc = self.lmb*cdiff #add to loss\n",
        "\n",
        "        # #The `make the reps different` term\n",
        "        # C1 = (z1norm.T @ z1norm_two) / bs\n",
        "        # C2 = (z2norm.T @ z2norm_two) / bs\n",
        "        \n",
        "        # cdiff1 = 0.5*C1.pow(2)\n",
        "        # cdiff2 = 0.5*C2.pow(2)\n",
        "\n",
        "        # cdiff = self.lmb*(cdiff1+cdiff2) #add to loss\n",
        "\n",
        "        CdiffRand = Cdiff_Rand(seed=self.seed,bs=bs,std=0.2,K=4)\n",
        "        cdiff1  = CdiffRand(z1norm,z1norm_two)\n",
        "        CdiffSup = Cdiff_Sup(I=self.I,inner_steps=5,bs=bs)\n",
        "        cdiff11 = CdiffSup(z1norm,z1norm_two)\n",
        "        cdiff1 = 0.8*cdiff1 + 0.2*cdiff11\n",
        "\n",
        "        # CdiffRand = Cdiff_Rand(seed=self.seed,bs=bs,std=0.2,K=2)\n",
        "        # cdiff2  = CdiffRand(z2norm,z2norm_two)\n",
        "        # CdiffSup = Cdiff_Sup(I=self.I,inner_steps=5,bs=bs)\n",
        "        # cdiff22 = CdiffSup(z2norm,z2norm_two)\n",
        "        # cdiff2 = 0.5*cdiff2 + 0.5*cdiff22\n",
        "        #cdiff = self.lmb*(0.5*cdiff1 + 0.5*cdiff2)\n",
        "        cdiff = self.lmb*cdiff1\n",
        "\n",
        "        # #The `prevent collapse to zero` term. Unnecessary given independence?\n",
        "        # relu = nn.ReLU()\n",
        "        # eps=1e-7\n",
        "        # #C = (z1norm.T @ z1norm) / bs\n",
        "\n",
        "        # cdiffa=relu(1-(z1.var(0,unbiased=False)+eps).pow(0.5))\n",
        "        # cdiffb=relu(1-(z1_two.var(0,unbiased=False)+eps).pow(0.5))\n",
        "\n",
        "        #loss = (1/nf)*Invar.sum() + 0.5*(cdiffa + cdiffb).sum()+ (redun_reduc + cdiff).sum()\n",
        "        loss = (1/nf)*Invar.sum() + (redun_reduc + cdiff).sum()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def show(self, n=1):\n",
        " \n",
        "        bs = self.learn.x.size(0)//2\n",
        "        x1,x2  = self.learn.x[:bs], self.learn.x[bs:]\n",
        "        #x1 = TensorImageBW(x1)\n",
        "        #x2 = TensorImageBW(x2)\n",
        "        idxs = np.random.choice(range(bs),n,False)\n",
        "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n",
        "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n",
        "        images = []\n",
        "        for i in range(n): images += [x1[i],x2[i]]\n",
        "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)"
      ],
      "metadata": {
        "id": "99KqmobO4Pir"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions / classes we need to evaluate the encoder - i.e. train linear head on frozen rep and evaluate. \n",
        "class LinearClassifier(nn.Module):\n",
        "    \n",
        "    def __init__(self,zdim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(zdim,10) #As 10 classes for mnist\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = cast(self.fc1(x),Tensor) #so we have to use cross entropy loss. cast is because using old version fastai \n",
        "        return x\n",
        "\n",
        "def turnoffgrad_model(fastai_encoder):\n",
        "    for p in fastai_encoder.parameters():\n",
        "        p.requires_grad=False\n",
        "        \n",
        "    return fastai_encoder\n",
        "\n",
        "def train_head(encoder_nograd,tune_seed=10,bs_tune=20): #The seed choses a different (20) samples for training the head. 2 of each class\n",
        "    \"\"\"Train head on a tune_set, chosen through given tune_seed for reproducibility if needed\n",
        "    \"\"\"\n",
        "                                    # of the tune_seed)\n",
        "    \n",
        "    dls_tune=tune_set(items0,seed=tune_seed,bs_tune=bs_tune) #different random tune set each time (but as a function of tune_seed)\n",
        " \n",
        "    N=len(dls_tune.valid)*bs_tune \n",
        "    assert N == len(dls_tune.valid_ds) #Check that the tune set (valid) is divided by the batch size\n",
        "    assert len(dls_tune.valid_ds) == bs_tune\n",
        "    \n",
        "    zdim=1024 \n",
        "    head = LinearClassifier(zdim=zdim)\n",
        "    head.to(device)\n",
        "    optimizer = torch.optim.Adam(head.parameters())\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(200):\n",
        "        #for x,y in dls_tune.valid: #Slows massively on colab but not on kaggle. Weird. \n",
        "        x,y=dls_tune.valid.one_batch() #Same every time since dataset only has length=batch size = 20.\n",
        "                                        #Will need to fix this for CIFAR10 etc\n",
        "\n",
        "        loss = criterion(head(encoder_nograd(x)),y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return head\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_head(head):\n",
        "    \"\"\"Evaluate the (typically trained) head on on the test set\n",
        "    \"\"\"\n",
        "    N=len(dls_test.train)*bs_test\n",
        "    assert N == len(dls_test.train_ds)\n",
        "\n",
        "    num_correct=0\n",
        "    for x,y in dls_test.train:\n",
        "\n",
        "        ypred = head(encoder_nograd(x))\n",
        "        correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n",
        "        num_correct += correct.sum()\n",
        "    \n",
        "    return num_correct/N\n",
        "\n",
        "def eval_encoder(encoder_nograd,tune_seed=10):\n",
        "    \"\"\"\"Evaluate the encoder, which means to train and evaluate the head - basically wrap functions train_head\n",
        "        and eval_head\n",
        "    \"\"\"\n",
        "    head=train_head(encoder_nograd,tune_seed=tune_seed)\n",
        "    pct_correct = eval_head(head)\n",
        "    return pct_correct\n",
        "    "
      ],
      "metadata": {
        "id": "G7pPw34mzPG7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In this cell we get the data for MNIST (including some helpful functions we can potentially use\n",
        "#to get CIFAR10 data etc)\n",
        "def seed_everything(seed=42):\n",
        "    \"\"\"\"\n",
        "    Seed everything.\n",
        "    \"\"\"   \n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def tune_set(items0,seed=None,bs_tune=20):\n",
        "    \n",
        "    seed_everything(seed=seed)\n",
        "    \n",
        "    items0=items0.shuffle()\n",
        "    d = {'0':0,'1':0,'2':0,'3':0,'4':0,'5':0,'6':0,'7':0,'8':0,'9':0}\n",
        "    ITEMS=[]\n",
        "    for i in items0:\n",
        "        s=str(i).split('/training/')[1][0]\n",
        "        if d[s] is 0 or d[s] is 1:\n",
        "            ITEMS.append(i)\n",
        "            d[s]+=1\n",
        "    #items0=ITEMS\n",
        "\n",
        "    for i in items0:\n",
        "        if i not in ITEMS:\n",
        "            ITEMS.append(i)\n",
        "            \n",
        "    split = IndexSplitter(list(range(bs_tune)))\n",
        "\n",
        "    tds_tune = Datasets(ITEMS, [PILImageBW.create, [parent_label, Categorize()]], splits=split(ITEMS)) #Or do we want this?\n",
        "    dls_tune = tds_tune.dataloaders(bs=bs_tune,num_workers=6, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "    \n",
        "    return dls_tune\n",
        "\n",
        "\n",
        "def shuffle_items(items,seed):\n",
        "    \"\"\"Helper function to sort a list according to given random seed\n",
        "    \"\"\"\n",
        "    items.sort()\n",
        "    \n",
        "    if seed !=None:\n",
        "        seed_everything(seed=seed)\n",
        "        items=items.shuffle()\n",
        "    \n",
        "    return items\n",
        "\n",
        "class BT_Data:\n",
        "    \n",
        "    def __init__(self,items,seed=42,ts=16384,bs=512,tune_s=2000,bs_tune=20,bs_test=578):\n",
        "        \n",
        "        self.ts=ts\n",
        "        self.bs=bs\n",
        "        self.tune_s=tune_s\n",
        "        self.bs_tune=bs_tune\n",
        "        self.bs_test=bs_test\n",
        "        \n",
        "        self._seed=seed\n",
        "        self.seed=seed\n",
        "        items=shuffle_items(items,seed)\n",
        "        self.items=items\n",
        "\n",
        "    @property\n",
        "    def seed(self):\n",
        "        return self._seed\n",
        "    \n",
        "    @seed.setter #When we update the seed, we update the datasets (so items and dls objects) accordingly\n",
        "    def seed(self,val):\n",
        "        self._seed=val\n",
        "        self.items = shuffle_items(items,val)\n",
        "        self.build_items_i()\n",
        "        self.build_dls()\n",
        "        \n",
        "    def build_items_i(self):\n",
        "        self.items1 = self.items[0:ts] #train BT on these guys\n",
        "        self.items0 = self.items[self.ts:self.ts+self.tune_s] #for fine tuning - just choose 2000 guys to extract 20 for fine tuning \n",
        "        self.items2 = self.items[self.ts+self.tune_s:] #test on remainder\n",
        "        \n",
        "    def build_dls(self):\n",
        "        \n",
        "        split = RandomSplitter(valid_pct=0.0)\n",
        "        tds = Datasets(self.items1, [PILImageBW.create, [parent_label, Categorize()]], splits=split(self.items1))\n",
        "        self.dls = tds.dataloaders(bs=self.bs,num_workers=6, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "\n",
        "        #Evaluate linear classifier on this guy\n",
        "        split = RandomSplitter(valid_pct=0.0) #randomly split training set into training and validation\n",
        "        tds_test = Datasets(self.items2, [PILImageBW.create, [parent_label, Categorize()]], splits=split(self.items2)) #Or do we want this?\n",
        "        self.dls_test = tds_test.dataloaders(bs=self.bs_test,num_workers=6, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n",
        "\n",
        "path = untar_data(URLs.MNIST)\n",
        "items = get_image_files(path/'training') #i.e. NOT testing!!!\n",
        "items.sort()\n",
        "\n",
        "seed=42\n",
        "tune_seed=10\n",
        "ts=16384\n",
        "bs=512\n",
        "tune_s=2000\n",
        "bs_tune=20\n",
        "bs_test=578\n",
        "\n",
        "bt_data = BT_Data(items=items,seed=seed,ts=ts,bs=bs,tune_s=tune_s,bs_tune=bs_tune,bs_test=bs_test)\n",
        "\n",
        "items1=bt_data.items1\n",
        "items0=bt_data.items0\n",
        "items2=bt_data.items2\n",
        "\n",
        "dls=bt_data.dls\n",
        "dls_test=bt_data.dls_test\n",
        "\n"
      ],
      "metadata": {
        "id": "YuBW-lc5s73g"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In this cell we train BT / RBT and then evaluate\n",
        "\n",
        "#Train BT / RBT\n",
        "ps=500\n",
        "hs=500\n",
        "fastai_encoder = create_fastai_encoder(xresnet18(),pretrained=False,n_in=1)\n",
        "model = create_barlow_twins_model(fastai_encoder, hidden_size=hs,projection_size=ps)# projection_size=1024)\n",
        "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=True)\n",
        "learn = Learner(dls,model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "learn.fit(200)\n",
        "\n",
        "#Test performance of encoder\n",
        "performance_dict={}\n",
        "fastai_encoder.eval()\n",
        "encoder_nograd = turnoffgrad_model(fastai_encoder) \n",
        "for num in range(5):\n",
        "    \n",
        "    pct_correct = eval_encoder(encoder_nograd,tune_seed=tune_seed+num)\n",
        "    performance_dict[f'seed_{num}'] = pct_correct \n",
        "\n",
        "print(torch.mean(tensor(list(performance_dict.values()))))\n",
        "performance_dict"
      ],
      "metadata": {
        "id": "D8xzAoBCYoKx",
        "outputId": "992374f5-57fd-4f27-ad89-b871724bb50e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>315.213165</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>212.428345</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>163.163559</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>139.380478</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>122.047928</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>108.903351</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>95.779144</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>90.597458</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>84.810570</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>76.280937</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>72.367180</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>66.632553</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>59.746250</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>57.507549</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>57.555191</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>55.468166</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>52.988747</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>49.694698</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>46.379036</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>45.157974</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>43.194309</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>41.621021</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>42.459286</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>40.348881</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>37.446358</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>36.110622</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>34.949467</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>33.101444</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>32.374668</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>31.980122</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>30.458792</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>29.795177</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>29.859591</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>29.627115</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>28.155195</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>26.385517</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>26.295473</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>27.603548</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>26.110250</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>25.746012</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>25.437973</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>23.351797</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>22.890467</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>22.396324</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>21.387173</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>22.532835</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>22.488590</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>21.298262</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>20.224504</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>19.537514</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>19.393217</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>20.199366</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>20.357260</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>20.212786</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>21.498629</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>19.893124</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>19.546007</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>18.606668</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>17.222744</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>17.298714</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>17.195433</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>16.484940</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>17.437002</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>17.815428</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>17.295431</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>17.372942</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>17.102299</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>17.016455</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>17.419521</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>16.670675</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>15.801195</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>15.478311</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>15.063912</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>14.808991</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>14.900729</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>15.309538</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>14.794662</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>14.935511</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>15.084785</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>15.029571</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>14.922977</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>14.968266</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>15.180882</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>14.965628</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>14.120830</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>13.841693</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>13.644332</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>13.517421</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>14.701640</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>15.645259</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>14.426538</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>13.841462</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>13.530118</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>13.759733</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>14.123858</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>13.930632</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>13.159128</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>13.068036</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>13.771625</td>\n",
              "      <td>None</td>\n",
              "      <td>00:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>13.228563</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>12.642108</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>12.377795</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>12.327703</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>12.911693</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>12.149336</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>12.240286</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>11.956239</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>11.606602</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>11.669775</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>12.010477</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>11.233412</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>11.660822</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>12.394113</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>12.004641</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>11.691591</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>11.459340</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>11.725735</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>12.150042</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>11.831613</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>11.433379</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>11.776400</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>11.366779</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>10.977694</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>10.999821</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>10.925634</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>10.495159</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>10.513984</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>10.556334</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>10.207754</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>10.607828</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>10.860964</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>10.663840</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>10.821610</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>10.650681</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>10.415852</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>10.498394</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>10.962426</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>11.048903</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>10.678147</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>9.892398</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>9.753616</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>9.762632</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>9.701035</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>10.610844</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>10.717106</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>10.619227</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>10.207530</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>10.735379</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>10.650376</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>10.195023</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>9.874522</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>10.091575</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>10.290292</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>9.614181</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>9.549645</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>9.515810</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>9.132244</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>9.491624</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>9.553582</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>9.424575</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>9.664693</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>9.636703</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>9.166614</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>9.420025</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>9.482842</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>8.974715</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>9.142964</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>9.195036</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>9.614235</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>10.156965</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>9.649114</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>9.637926</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>9.276249</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>9.284497</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>9.118446</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>9.081065</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>9.240181</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>9.492611</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>9.242279</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>8.905148</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>8.968238</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>9.027113</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>8.712192</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>8.690726</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>8.852246</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>8.880070</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>9.323677</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>9.485859</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>8.788319</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>8.596568</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>8.673689</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>8.688624</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>9.236845</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>8.937562</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>8.508750</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>8.328794</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>8.426717</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>8.241940</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>8.590079</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>8.657460</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8949)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'seed_0': TensorCategory(0.8872),\n",
              " 'seed_1': TensorCategory(0.8518),\n",
              " 'seed_2': TensorCategory(0.9181),\n",
              " 'seed_3': TensorCategory(0.9068),\n",
              " 'seed_4': TensorCategory(0.9106)}"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here we are applying random functions \"as in the theorem\", but using our old method on the sup component. Also increased K to 4. Also note that convexity is 0.8,0.2. \n"
      ],
      "metadata": {
        "id": "7ik5qCcGq53t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Current: just removed `index` condition\n",
        "\n",
        "#All with seed=42,tune_seed=10\n",
        "\n",
        "#200 epochs no indexmod (made it slightly worse)\n",
        "RBT={'seed_0': TensorCategory(0.8439),\n",
        " 'seed_1': TensorCategory(0.8016),\n",
        " 'seed_2': TensorCategory(0.9042),\n",
        " 'seed_3': TensorCategory(0.8454),\n",
        " 'seed_4': TensorCategory(0.8323)}\n",
        "\n",
        "print(torch.mean(tensor(list(RBT.values())))) #tensor(0.8455)\n",
        "\n",
        "#200 epochs, indexmod=2\n",
        "RBT={'seed_0': TensorCategory(0.8507),\n",
        " 'seed_1': TensorCategory(0.8151),\n",
        " 'seed_2': TensorCategory(0.9106),\n",
        " 'seed_3': TensorCategory(0.8896),\n",
        " 'seed_4': TensorCategory(0.8663)}\n",
        "print(torch.mean(tensor(list(RBT.values())))) #tensor(0.8665)\n",
        "\n",
        "#200 epochs, indexmod=2\n",
        "RAT = {'seed_0': TensorCategory(0.8872),\n",
        " 'seed_1': TensorCategory(0.8518),\n",
        " 'seed_2': TensorCategory(0.9181),\n",
        " 'seed_3': TensorCategory(0.9068),\n",
        " 'seed_4': TensorCategory(0.9106)}\n",
        "print(torch.mean(tensor(list(RAT.values())))) #tensor(0.8949) \n",
        "\n",
        "#200 epochs, indexmod=8\n",
        "RBT={'seed_0': TensorCategory(0.8247),\n",
        " 'seed_1': TensorCategory(0.7691),\n",
        " 'seed_2': TensorCategory(0.8869),\n",
        " 'seed_3': TensorCategory(0.8334),\n",
        " 'seed_4': TensorCategory(0.8444)}\n",
        "print(torch.mean(tensor(list(RBT.values())))) #tensor(0.8317)\n",
        "\n",
        "\n",
        "#To beat (100 epochs)\n",
        "{'seed_0': TensorCategory(0.8426)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuOl_HeiAOGm",
        "outputId": "63ded59c-2f8d-4347-d4c8-509cc3d9ada2"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.8455)\n",
            "tensor(0.8665)\n",
            "tensor(0.8949)\n",
            "tensor(0.8317)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'seed_0': TensorCategory(0.8426)}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comments"
      ],
      "metadata": {
        "id": "pbepNL0JxTmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we have some tests and example of augmentations used:"
      ],
      "metadata": {
        "id": "Go-6_SkNxZDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%ipytest -qq\n",
        "#Tests\n",
        "\n",
        "labeller = using_attr(RegexLabeller(pat = r'(\\d+).png$'), 'name')\n",
        "convert_tensor = transforms.ToTensor()\n",
        "Expected_first_item = {42:{'items1':'19825','items0':'40684','items2':'43064'},420:{'items1':'44942','items0':'23821','items2':'908'}}\n",
        "Expected_first_dls = {42:{'dls':0.085169,'dls_test':0.099924},420:{'dls':0.183678,'dls_test':0.162825}}\n",
        "Expected_first_dls_tune = {(42,55):0.093707,(420,55):0.1513355}\n",
        "\n",
        "def build_BT_Data(items,seed,tune_seed):\n",
        "    \"\"\" Helper function to get dictionary of data we need for testing purposes\n",
        "    \"\"\"\n",
        "    ts=16384\n",
        "    bs=512\n",
        "    tune_s=2000\n",
        "    bs_tune=20\n",
        "    bs_test=578\n",
        "    \n",
        "    k=dict(seed=seed,ts=ts,bs=bs,tune_s=tune_s,bs_tune=bs_tune,bs_test=bs_test)\n",
        "    bt_dataset = BT_Data(items=items,**k)\n",
        "\n",
        "    items = bt_dataset.items\n",
        "    items1 = bt_dataset.items1\n",
        "    items0 = bt_dataset.items0\n",
        "    items2 = bt_dataset.items2\n",
        "\n",
        "    dls = bt_dataset.dls\n",
        "    dls_test = bt_dataset.dls_test\n",
        "\n",
        "    dls_tune=tune_set(items0=items0,seed=seed,bs_tune=bs_tune)\n",
        "    \n",
        "    return dict(seed=seed,tune_seed=tune_seed,items=items,items1=items1,items0=items0,items2=items2,dls=dls,dls_test=dls_test,dls_tune=dls_tune)\n",
        "\n",
        "bt_data_42 = build_BT_Data(items=items,seed=42,tune_seed=10)\n",
        "bt_data_420 = build_BT_Data(items=items,seed=420,tune_seed=100)\n",
        "\n",
        "def verify_DatasetShape(dls_obj,batch_size,ds_settype='train'):\n",
        "    \"\"\"\"Helper function to verify shape of a dls object given the batch size; ds_settype is either `train` or \n",
        "        `valid`. The idea is we want the batch_size to divide the length of the dlsobj.\n",
        "    \"\"\"\n",
        "    \n",
        "    tem = len(getattr(dls_obj,ds_settype)) #length of dlsobj.train or dlsobj.valid depending on settpe\n",
        "    return tem*batch_size == len(getattr(dls_obj,ds_settype+'_ds'))\n",
        "\n",
        "def verify_first_item(items,expected):\n",
        "    \"\"\"Helper function to verify first element of items is as expected, given random seed of 42\n",
        "    \"\"\"\n",
        "        \n",
        "    return labeller(items[0]) == expected\n",
        "        \n",
        "def verify_first_dls(dls_obj,expected,ds_settype='train'):\n",
        "    \"\"\"Helper function to verify first element of the given dls object is as expected, given random seed of 42.\n",
        "        Note that ds_settype is either `train` or `valid\n",
        "    \"\"\"\n",
        "    \n",
        "    #All we are doing here is getting the first tensor in, for example e.g. dls_obj.train_ds and computing\n",
        "    #the mean of all the elements. If random seed is same, then it should give the same results\n",
        "    z=convert_tensor(next(iter(getattr(dls_obj,ds_settype+'_ds')))[0]).mean().item()\n",
        "    \n",
        "    #logging.debug(f'with {ds_settype} has: {z}')\n",
        "    assert z-expected < 0.0001\n",
        "\n",
        "@pytest.mark.parametrize('bt_dataset',[bt_data_42,bt_data_420])\n",
        "class Test_shapes:\n",
        "    \n",
        "    def test_shape_dlsobjects(self,bt_dataset):\n",
        "        \"\"\"\"Test the shape of each dlsobj\n",
        "        \"\"\"\n",
        "    \n",
        "        assert verify_DatasetShape(bt_dataset['dls'],batch_size=bs,ds_settype='train')\n",
        "\n",
        "        assert verify_DatasetShape(bt_dataset['dls_tune'],batch_size=bs_tune,ds_settype='valid')\n",
        "\n",
        "        assert verify_DatasetShape(bt_dataset['dls_test'],batch_size=bs_test,ds_settype='train')\n",
        "    \n",
        "    def test_length_dlsobjects(self,bt_dataset):\n",
        "        \"\"\"\"Test the length of each dlsobj that we use\n",
        "        \"\"\"\n",
        "        assert len(bt_dataset['dls'].train_ds) == ts and len(bt_dataset['dls_tune'].valid_ds) == bs_tune and len(bt_dataset['dls_test'].train_ds)==41616\n",
        "    \n",
        "    \n",
        "@pytest.mark.parametrize('bt_dataset',[bt_data_42,bt_data_420])   \n",
        "class Test_first:\n",
        "    \n",
        "    def test_first_item(self,bt_dataset):\n",
        "        \"\"\"\"Verify that the first item of each items is as expected\n",
        "        \"\"\"\n",
        "\n",
        "        seed = bt_dataset['seed']\n",
        "\n",
        "        assert verify_first_item(bt_dataset['items1'],Expected_first_item[seed]['items1'])\n",
        "\n",
        "        assert verify_first_item(bt_dataset['items0'],Expected_first_item[seed]['items0'])\n",
        "\n",
        "        assert verify_first_item(bt_dataset['items2'],Expected_first_item[seed]['items2'])\n",
        "\n",
        "    def test_first_dlsobj(self,bt_dataset):\n",
        "        \"\"\"Verify that the first item of each dlsobj is as expected\n",
        "        \"\"\"\n",
        "        seed = bt_dataset['seed']\n",
        "        dls = bt_dataset['dls']\n",
        "        dls_test = bt_dataset['dls_test']\n",
        "        items0 = bt_dataset['items0']\n",
        "\n",
        "        verify_first_dls(dls,ds_settype='train',expected=Expected_first_dls[seed]['dls'])\n",
        "        verify_first_dls(dls_test,ds_settype='train',expected=Expected_first_dls[seed]['dls_test'])\n",
        "\n",
        "        tune_seed=55\n",
        "        dls_tune=tune_set(items0,seed=tune_seed,bs_tune=bs_tune)\n",
        "\n",
        "        verify_first_dls(dls_tune,ds_settype='valid',expected=Expected_first_dls_tune[(seed,tune_seed)])\n",
        "\n",
        "\n",
        "@pytest.mark.parametrize('bt_dataset',[bt_data_42,bt_data_420])\n",
        "def test1_tune_set(bt_dataset):\n",
        "    \"\"\"Check whether the function `tune_set` gives us the expected values\"\"\"\n",
        "    \n",
        "\n",
        "    seed=bt_dataset['seed']\n",
        "    tune_seed=bt_dataset['tune_seed']\n",
        "    items0 = bt_dataset['items0']\n",
        "    \n",
        "    if tune_seed==10 and seed==42:\n",
        "        expected = {10:0.12255,11:0.153564,12:0.12781,13:0.129523,14:0.13019}\n",
        "    \n",
        "    elif tune_seed==100 and seed==420:\n",
        "        expected={100:0.136104,101:0.120989,102:0.1381390,103:0.1380412,104:0.14285138}\n",
        "        \n",
        "    for i in range(5):\n",
        "        #seed_everything(seed=seed)\n",
        "        dls_tune=tune_set(items0,seed=tune_seed+i,bs_tune=20)\n",
        "        x_mean=0\n",
        "        for x,y in dls_tune.valid:\n",
        "            x_mean += x.mean()\n",
        "\n",
        "        assert abs(x_mean-expected[tune_seed+i])<0.0001\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z3pyrAB8rpN",
        "outputId": "fdbfda5d-9d3d-49b6-90d9-4c0655164b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                   [100%]\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "AKbw2pxMag82",
        "outputId": "f902e81a-66b9-4301-f6ad-dc21b95c72e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
            "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#In this cell we display the augmentations used. \n",
        "fastai_encoder = create_fastai_encoder(xresnet18(),pretrained=False,n_in=1)\n",
        "model = create_barlow_twins_model(fastai_encoder, hidden_size=10,projection_size=10)# projection_size=1024)\n",
        "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=False)\n",
        "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
        "b = dls.one_batch()\n",
        "learn._split(b)\n",
        "learn('before_batch')\n",
        "axes = learn.barlow_twins.show(n=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}