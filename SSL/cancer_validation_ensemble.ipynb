{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamish-haggerty/AI-hacking/blob/master/SSL/cancer_validation_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqZPF94637Fk"
      },
      "source": [
        "# cancer_validation_ensemble\n",
        "\n",
        "> Purpose of this notebook is to explore whether interspersing some BT pretraining makes an ensemble better. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gFWev-_Q37Fm"
      },
      "outputs": [],
      "source": [
        "#| default_exp cancer_validation_ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cat536_37Fn"
      },
      "source": [
        "Setup: Surely there is a way to get rid of having to put this cell everywhere. hmmm.\n",
        "\n",
        "Or we can just copy paste / delete this in and out when needed. Either way, getting close to a decent workable workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fO8vhi6b37Fn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4537f78f-6fcb-4e95-a6df-d52df7032baa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#| hide\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "def colab_is_true():\n",
        "\n",
        "    try: \n",
        "        from google.colab import drive\n",
        "\n",
        "        return True \n",
        "    except ModuleNotFoundError:\n",
        "        return False\n",
        "\n",
        "def setup_colab():\n",
        "\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    #os.system('unzip -q \"/content/drive/My Drive/archive (1).zip\"')\n",
        "    os.system('git clone https://github.com/hamish-haggerty/cancer-proj.git')\n",
        "\n",
        "    os.chdir('cancer-proj')\n",
        "    \n",
        "    os.system('pip install .')\n",
        "    os.system('pip install -qU nbdev')\n",
        "    os.system('nbdev_install_quarto')\n",
        "\n",
        "    os.system('unzip -q \"/content/drive/My Drive/archive (1).zip\"') #does this work?\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    on_colab = colab_is_true()\n",
        "    if on_colab:\n",
        "        setup_colab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VNjMrD_W37Fo"
      },
      "outputs": [],
      "source": [
        "#| hide\n",
        "from nbdev.showdoc import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1RZVlbHF37Fo"
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "from fastai.vision.all import *\n",
        "from base_rbt.all import *\n",
        "from cancer_proj.cancer_dataloading import *\n",
        "from cancer_proj.cancer_metrics import *\n",
        "from cancer_proj.cancer_maintrain import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def predict_model(xval,yval,model,aug_pipelines_test,numavg=3):\n",
        "    \"Note that this assumes xval is entire validation set. If it doesn't fit in memory, can't use this guy\"\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "    N=xval.shape[0]\n",
        "\n",
        "    scores=0\n",
        "    for _ in range(numavg):\n",
        "\n",
        "        scores += model(aug_pipelines_test[0](xval)) #test time augmentation. This also gets around issue of randomness in the dataloader in each session...\n",
        "\n",
        "    scores *= 1/numavg\n",
        "\n",
        "    ypred = cast(torch.argmax(scores, dim=1),TensorCategory)\n",
        "\n",
        "    correct = (ypred == yval)#.type(torch.FloatTensor)\n",
        "\n",
        "    #correct = (torch.argmax(ypred,dim=1) == yval).type(torch.FloatTensor)\n",
        "    num_correct = correct.sum()\n",
        "    accuracy = num_correct/N\n",
        "    \n",
        "    return scores,ypred,accuracy.item()"
      ],
      "metadata": {
        "id": "zRYDiC-ze-rH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eN4Js6uo37Fq"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pp3x3wbL37Fq"
      },
      "outputs": [],
      "source": [
        "#| hide\n",
        "\n",
        "#Since we have cloned repository and cd'd into it (and the data itself is not stored in the\n",
        "#repo) we need cd out of it, get the data, then cd back into the repo `cancer-proj`.\n",
        "#This is a bit annoying, can maybe remove this later\n",
        "if on_colab:\n",
        "    #os.chdir('..') #assumes we are currently in cancer-proj directory\n",
        "    train_dir = colab_train_dir\n",
        "    test_dir = colab_test_dir\n",
        "else:\n",
        "    train_dir = local_train_dir\n",
        "    test_dir = local_test_dir\n",
        "\n",
        "#define general hps\n",
        "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#bs=256\n",
        "#bs=698\n",
        "bs=256\n",
        "bs_tune=256\n",
        "size=128\n",
        "bs_val=174\n",
        "\n",
        "#get the data dictionary\n",
        "data_dict = get_fnames_dls_dict(train_dir=train_dir,test_dir=test_dir,\n",
        "                    device=device,bs_val=bs_val,bs=bs,bs_tune=bs_tune,size=size,n_in=3)\n",
        "\n",
        "#get the dataloaders\n",
        "dls_train,dls_tune,dls_valid = data_dict['dls_train'],data_dict['dls_tune'],data_dict['dls_valid']\n",
        "x,y = data_dict['x'],data_dict['y']\n",
        "xval,yval = data_dict['xval'],data_dict['yval']\n",
        "xtune,ytune = data_dict['xtune'],data_dict['ytune']\n",
        "vocab = data_dict['vocab']\n",
        "\n",
        "#If we want to write some tests (make sure the data is same every time etc):\n",
        "fnames,fnames_train,fnames_tune,fnames_valid,fnames_test = data_dict['fnames'],data_dict['fnames_train'],data_dict['fnames_tune'],data_dict['fnames_valid'],data_dict['fnames_test']\n",
        "\n",
        "test_eq(x.shape,xtune.shape)\n",
        "\n",
        "# if on_colab:\n",
        "#     os.chdir('cancer-proj')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PpIwqfu37Fr"
      },
      "source": [
        "## Load aug pipelines here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ntc4hMMj37Fr"
      },
      "outputs": [],
      "source": [
        "#| hide\n",
        "\n",
        "aug_dict = create_aug_pipelines(size=size,device=device,Augs=BYOL_Augs,TUNE_Augs=TUNE_Augs,Val_Augs=Val_Augs)\n",
        "aug_pipelines = aug_dict['aug_pipelines']\n",
        "aug_pipelines_tune = aug_dict['aug_pipelines_tune']\n",
        "aug_pipelines_test = aug_dict['aug_pipelines_test'] "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optionally, display:"
      ],
      "metadata": {
        "id": "zlZuNHTW34iy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PP7Sp4P837Fr"
      },
      "outputs": [],
      "source": [
        "#| hide\n",
        "#show_bt_batch(dls=dls_train,aug=aug_pipelines,n_in=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "foWFxkv_37Fr"
      },
      "outputs": [],
      "source": [
        "#| hide\n",
        "\n",
        "#show_linear_batch(dls=dls_tune,n_in=3,aug=aug_pipelines_tune,n=2,print_augs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KAW1Mwwa37Fs"
      },
      "outputs": [],
      "source": [
        "#| export\n",
        "\n",
        "@patch\n",
        "def lf(self:BarlowTwins, pred,*yb): return lf_bt(pred,I=self.I,lmb=self.lmb)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to run a few exploratory experiments. Based on the results, next is to run some systematic experiments, probably with W and B... Or final results..."
      ],
      "metadata": {
        "id": "uA_B-8T8-NpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\n",
        "\n",
        "@patch\n",
        "@delegates(Learner.fit_one_cycle)\n",
        "def encoder_fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n",
        "              pct_start=0.3, div=5.0, **kwargs):\n",
        "    \"Fine tuner to use with bt initial weights\"\n",
        "    \n",
        "    self.freeze() #freeze the resnet\n",
        "    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n",
        "    base_lr /= 2\n",
        "    #self.unfreeze() #don't unfreeze the resnet. We are fitting training the encoder head + projector\n",
        "    #self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\n",
        "    self.fit_one_cycle(epochs, slice(base_lr, base_lr), pct_start=pct_start, div=div, **kwargs)\n",
        "\n",
        "    self.unfreeze() #We can unfreeze at the end"
      ],
      "metadata": {
        "id": "0q915iOR-gFj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory experiment: BT initial weights, with a small amount of pretraining. First, let's try updating all of the weights (i.e. the resnet gets updated with BT pretraining). Remember, we need to freeze the pretrained resnet first, and align the encoder-head + projector head."
      ],
      "metadata": {
        "id": "CKsB_xyHjcLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We need to edit several of our base functions: Since we have to align the head of the encoder with the projector, we need to edit `create_model`, and define a new bt_splitter: i.e. the splitter needs to freeze the pretrained resnet, and leave the new head_encoder + projector unfrozen."
      ],
      "metadata": {
        "id": "yPIrdfaiFvqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\n",
        "\n",
        "class HeadEncoder(nn.Module):\n",
        "    \"Basic nonlinear \"\n",
        "    def __init__(self,resnet_encoder,device='cuda'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.resnet_encoder=resnet_encoder\n",
        "\n",
        "        self.head_encoder = sequential(nn.Linear(2048,2048),nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "                               nn.ReLU(inplace=True))\n",
        "        \n",
        "        self.device = torch.device(device)\n",
        "        self.to(self.device)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.resnet_encoder(x)\n",
        "        x=self.head_encoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def create_model(which_model,device,ps=8192,n_in=3):\n",
        "    print('inside create_model')\n",
        "\n",
        "    #pretrained=True if 'which_model' in ['bt_pretrain', 'supervised_pretrain'] else False\n",
        "\n",
        "    if which_model == 'bt_pretrain': model = torch.hub.load('facebookresearch/barlowtwins:main', 'resnet50')\n",
        "    \n",
        "    elif which_model == 'no_pretrain': model = resnet50()\n",
        "\n",
        "    elif which_model == 'supervised_pretrain': model = resnet50(weights='IMAGENET1K_V2')\n",
        "\n",
        "    #ignore the 'pretrained=False' argument here. Just means we use the weights above \n",
        "    #(which themselves are either pretrained or not)\n",
        "    encoder = get_resnet_encoder(model)\n",
        "    encoder = HeadEncoder(encoder,device='cpu')\n",
        "\n",
        "    model = create_barlow_twins_model(encoder, hidden_size=ps,projection_size=ps,nlayers=3)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        model.cuda()\n",
        "        encoder.cuda()\n",
        "\n",
        "\n",
        "    return model,encoder\n",
        "\n",
        "bt_model,encoder = create_model(which_model='bt_pretrain',ps=8192,device=device)\n",
        "\n",
        "def my_splitter_bt(m):\n",
        "\n",
        "    return L(sequential(*m.encoder.resnet_encoder),sequential(m.encoder.head_encoder,m.projector)).map(params)\n",
        "\n",
        "test_eq(len(my_splitter_bt(bt_model)),2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "0b06ebefe34345d69c57763be586ca18",
            "ef4f87f3a87d41cabb9139e3ab3d7c01",
            "a18742cfa8e44462833d74f3faf1142b",
            "fd3150c258314091801a71dbf69cd086",
            "55a4fd3d3f2c402e84b62fc9d9380f0d",
            "e60962ee47184bdf8136d9a0c94713c3",
            "ef5b0fad74534d59b7959b417b2b2a9b",
            "4f1cff7d79ee437980f0cad0357d3df7",
            "91f10e6cdaeb4ad18eeee8879e5ae9e6",
            "57b3a7cc04604bd291de227dcbc6f846",
            "d00b9ce27702491990aa7b4df0ee8517"
          ]
        },
        "id": "1g2RtinCF8uh",
        "outputId": "f6d76780-9896-45e2-e50a-7dd83e2bfdaa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside create_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/barlowtwins/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/barlowtwins/ep1000_bs2048_lrw0.2_lrb0.0048_lambd0.0051/resnet50.pth\" to /root/.cache/torch/hub/checkpoints/resnet50.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/90.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b06ebefe34345d69c57763be586ca18"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Verify that splitter freezes expected part of model:\n",
        "\n",
        "# #test : manual. BT\n",
        "\n",
        "learn = Learner(dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(aug_pipelines,n_in=3,lmb=1/8192,print_augs=False)])\n",
        "learn.freeze()\n",
        "print('resnet should be frozen, encoder head + projector unfrozen')\n",
        "learn.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7dujr1ggO0-m",
        "outputId": "4cb181cc-f306-47a4-8349-1fe4b07c3295"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resnet should be frozen, encoder head + projector unfrozen\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BarlowTwinsModel (Input shape: 256 x 3 x 128 x 128)\n",
              "============================================================================\n",
              "Layer (type)         Output Shape         Param #    Trainable \n",
              "============================================================================\n",
              "                     256 x 64 x 64 x 64  \n",
              "Conv2d                                    9408       False     \n",
              "BatchNorm2d                               128        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "MaxPool2d                                                      \n",
              "Conv2d                                    4096       False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 32 x 32 \n",
              "Conv2d                                    32768      False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    131072     False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 16 x 16 \n",
              "Conv2d                                    131072     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    524288     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 8 x 8   \n",
              "Conv2d                                    524288     False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    2097152    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 1 x 1  \n",
              "AdaptiveAvgPool2d                                              \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048          \n",
              "Flatten                                                        \n",
              "Linear                                    4196352    True      \n",
              "BatchNorm1d                               4096       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 8192          \n",
              "Linear                                    16785408   True      \n",
              "BatchNorm1d                               16384      True      \n",
              "ReLU                                                           \n",
              "Linear                                    67117056   True      \n",
              "BatchNorm1d                               16384      True      \n",
              "ReLU                                                           \n",
              "Linear                                    67117056   True      \n",
              "____________________________________________________________________________\n",
              "\n",
              "Total params: 178,760,768\n",
              "Total trainable params: 155,305,856\n",
              "Total non-trainable params: 23,454,912\n",
              "\n",
              "Optimizer used: <function Adam at 0x7fbc7740a0d0>\n",
              "Loss function: <bound method BarlowTwins.lf of BarlowTwins>\n",
              "\n",
              "Model frozen up to parameter group #1\n",
              "\n",
              "Callbacks:\n",
              "  - TrainEvalCallback\n",
              "  - CastToTensor\n",
              "  - BarlowTwins\n",
              "  - Recorder\n",
              "  - ProgressCallback"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We also need to edit main_train. Now the model is: resnet_encoder -> head_encoder -> projector or linear layer. Also need to edit the splitter fuction for fine tuning."
      ],
      "metadata": {
        "id": "-RdYwIMrIIhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All that changes here is the definition of our model in `fine_tune`, and we need a new splitter function.\n",
        "\n",
        "We tried just patching in new def of fine_tune, but since `create_model` defintion changed it fucked things up. \n",
        "\n",
        "Feels like this should be some kind of callback, written extensibly enough that you just just \"patch\" functions etc in..."
      ],
      "metadata": {
        "id": "0Qo0TxLpTWI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\n",
        "\n",
        "class main_train:\n",
        "    \"\"\"Instantiate and (optionally) train the encoder. Then fine-tune the supervised model. \n",
        "    Outputs metrics on validation data\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dls_train, #used for training BT (if pretrain=True)\n",
        "                 dls_tune , #used for tuning\n",
        "                 dls_valid, #used to compute metrics / evaluate results. \n",
        "                 xval, #currently `predict_model` below assumes this is entire validation / test data\n",
        "                 yval,\n",
        "                 aug_pipelines, #the aug pipeline for self-supervised learning\n",
        "                 aug_pipelines_tune, #the aug pipeline for supervised learning\n",
        "                 aug_pipelines_test, #test (or valid) time augmentations \n",
        "                 initial_weights, #Which initial weights to use\n",
        "                 pretrain, #Whether to fit BT\n",
        "                 num_epochs, #number of BT fit epochs\n",
        "                 numfit, #number of tune_fit epochs\n",
        "                 freeze_num_epochs, #How many epochs to freeze body for when training BT\n",
        "                 freeze_numfit, #How many epochs to freeze body for when fine tuning\n",
        "                 ps=8192, #projection size\n",
        "                 n_in=3, #color channels\n",
        "                 indim=2048, #dimension output of encoder (2048 for resnet50)\n",
        "                 outdim=9, #number of classes\n",
        "                 print_report=False, #F1 metrics etc\n",
        "                 print_plot=False, #ROC curve\n",
        "                 ):\n",
        "        store_attr()\n",
        "        self.vocab = self.dls_valid.vocab\n",
        "        self.device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "\n",
        "                \n",
        "                 \n",
        "\n",
        "                 #Soon we might want to save some models here:\n",
        "\n",
        "                 #if self.model_type == 'res_proj': test_eq(self.fit_policy,'resnet_fine_tune') #I THINK this is only viable option?\n",
        "                 #self.encoder_path = f'/content/drive/My Drive/models/baselineencoder_initial_weights={self.initial_weights}_pretrain={self.pretrain}.pth'\n",
        "                 #self.tuned_model_path = f'/content/drive/My Drive/models/baselinefinetuned_initial_weights={self.initial_weights}_pretrain={self.pretrain}.pth'\n",
        "\n",
        "    @staticmethod\n",
        "    def fit(learn,fit_type,epochs,freeze_epochs,initial_weights):\n",
        "        \"\"\"We can patch in a modification, e.g. if we want subtype of fine_tune:supervised_pretrain to be different\n",
        "        to fine_tune:bt_pretrain\"\"\"\n",
        "\n",
        "        if fit_type == 'encoder_fine_tune': #i.e. barlow twins\n",
        "\n",
        "            learn.encoder_fine_tune(epochs,freeze_epochs=freeze_epochs) \n",
        "\n",
        "        elif fit_type == 'fine_tune':\n",
        "            \n",
        "            #elif initial_weights == 'supervised_pretrain':\n",
        "            learn.linear_fine_tune(epochs,freeze_epochs=freeze_epochs) \n",
        "\n",
        "        else: raise Exception('Fit policy not of expected form')\n",
        "\n",
        "    def train_encoder(self):\n",
        "        \"create encoder and (optionally, if pretrain=True) train with BT algorithm, according to fit_policy\"\n",
        "\n",
        "        try: #get existing encoder and plonk on new projector\n",
        "            encoder = self.encoder\n",
        "            encoder.cpu()\n",
        "            bt_model = create_barlow_twins_model(encoder, hidden_size=self.ps,projection_size=self.ps,nlayers=3)\n",
        "            bt_model.cuda()\n",
        "\n",
        "        except AttributeError: #otherwise, create\n",
        "            bt_model,encoder = create_model(which_model=self.initial_weights,ps=self.ps,device=self.device)\n",
        "\n",
        "        if self.pretrain: #train encoder according to fit policy\n",
        "\n",
        "            learn = Learner(self.dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(self.aug_pipelines,n_in=self.n_in,lmb=1/self.ps,print_augs=False)])\n",
        "            main_train.fit(learn,fit_type='encoder_fine_tune',\n",
        "                           epochs=self.num_epochs,freeze_epochs=self.freeze_num_epochs,\n",
        "                           initial_weights=self.initial_weights\n",
        "                          )\n",
        "            \n",
        "        self.encoder = bt_model.encoder\n",
        "\n",
        "    def fine_tune(self):\n",
        "        \"fine tune in supervised fashion, according to tune_fit_policy, and get metrics\"\n",
        "\n",
        "        #encoder = pickle.loads(pickle.dumps(self.encoder)) #We might want to pretrain once and fine tune several times (varying e.g. tune augs)\n",
        "\n",
        "        try: \n",
        "            encoder = self.encoder\n",
        "        \n",
        "        except AttributeError:\n",
        "            _,self.encoder = create_model(which_model=self.initial_weights,ps=self.ps,device=device)\n",
        "\n",
        "        #model = LM(self.encoder)\n",
        "        model = sequential(self.encoder,nn.Linear(2048,9))\n",
        "        \n",
        "        learn = Learner(self.dls_tune,model,splitter=my_splitter,cbs = [LinearBt(aug_pipelines=self.aug_pipelines_tune,n_in=self.n_in)],wd=0.0)\n",
        "\n",
        "        #debugging\n",
        "        #learn = Learner(self.dls_tune,model,cbs = [LinearBt(aug_pipelines=self.aug_pipelines_tune,n_in=self.n_in)],wd=0.0)\n",
        "\n",
        "        main_train.fit(learn,fit_type='fine_tune',\n",
        "                       epochs=self.numfit,freeze_epochs=self.freeze_numfit,\n",
        "                       initial_weights=self.initial_weights\n",
        "                      ) #fine tuning (don't confuse this with fit policy!)\n",
        "        \n",
        "        #model.encoder=encoder\n",
        "        scores,preds, acc = predict_model(self.xval,self.yval,model=model,aug_pipelines_test=self.aug_pipelines_test,numavg=3)\n",
        "        #metrics dict will have f1 score, auc etc etc\n",
        "        metrics = classification_report_wrapper(preds, self.yval, self.vocab, print_report=self.print_report)\n",
        "        auc_dict = plot_roc(self.yval,preds,self.vocab,print_plot=self.print_plot)\n",
        "        metrics['acc'],metrics['auc_dict'],metrics['scores'],metrics['preds'],metrics['xval'],metrics['yval'] = acc,auc_dict,scores,preds,self.xval,self.yval\n",
        "  \n",
        "        #torch.save(model.state_dict(), self.tuned_model_path)\n",
        "        return metrics #\n",
        "\n",
        "    def __call__(self):\n",
        "\n",
        "        self.train_encoder() #train (or extract) the encoder\n",
        "        metrics = self.fine_tune()\n",
        "        \n",
        "        return metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "1C9RO65sIK7X"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to define the splitter function for the fine_tune part of main differently as well:"
      ],
      "metadata": {
        "id": "JEHA6-14QiXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_splitter(m):\n",
        "    print('inside new my_splitter')\n",
        "    return L(sequential(*m[0].resnet_encoder),sequential(m[0].head_encoder,m[1])).map(params)"
      ],
      "metadata": {
        "id": "FMwr-MngRaYP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # #Verify that splitter freezes expected part of model, from linear point of view:\n",
        "\n",
        "bt_model,encoder = create_model(which_model='bt_pretrain',ps=8192,device=device)\n",
        "model = sequential(encoder,nn.Linear(2048,9))\n",
        "test_eq(len(my_splitter(model)),2)\n",
        "test_eq(len(my_splitter_bt(bt_model)),2)\n",
        "\n",
        "learn = Learner(dls_tune,model,splitter=my_splitter,cbs = [LinearBt(aug_pipelines=aug_pipelines_tune,n_in=3)],wd=0.0)\n",
        "learn.freeze()\n",
        "print('resnet should be frozen, encoder_head + linear layer unfrozen')\n",
        "learn.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gW5SiWd0PS9s",
        "outputId": "813c0f15-613d-4813-bc5a-ce359ccbfbee"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside create_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_barlowtwins_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside new my_splitter\n",
            "inside new my_splitter\n",
            "resnet should be frozen, encoder_head + linear layer unfrozen\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential (Input shape: 256 x 3 x 128 x 128)\n",
              "============================================================================\n",
              "Layer (type)         Output Shape         Param #    Trainable \n",
              "============================================================================\n",
              "                     256 x 64 x 64 x 64  \n",
              "Conv2d                                    9408       False     \n",
              "BatchNorm2d                               128        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "MaxPool2d                                                      \n",
              "Conv2d                                    4096       False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 32 x 32 \n",
              "Conv2d                                    32768      False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    131072     False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 16 x 16 \n",
              "Conv2d                                    131072     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    524288     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 8 x 8   \n",
              "Conv2d                                    524288     False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    2097152    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 1 x 1  \n",
              "AdaptiveAvgPool2d                                              \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048          \n",
              "Flatten                                                        \n",
              "Linear                                    4196352    True      \n",
              "BatchNorm1d                               4096       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 9             \n",
              "Linear                                    18441      True      \n",
              "____________________________________________________________________________\n",
              "\n",
              "Total params: 27,726,921\n",
              "Total trainable params: 4,272,009\n",
              "Total non-trainable params: 23,454,912\n",
              "\n",
              "Optimizer used: <function Adam at 0x7fbc7740a0d0>\n",
              "Loss function: <bound method LinearBt.lf of LinearBt>\n",
              "\n",
              "Model frozen up to parameter group #1\n",
              "\n",
              "Callbacks:\n",
              "  - TrainEvalCallback\n",
              "  - CastToTensor\n",
              "  - LinearBt\n",
              "  - Recorder\n",
              "  - ProgressCallback"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First we need to verify that the head still gives good performance:"
      ],
      "metadata": {
        "id": "d24rHCszX5ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Non default inputs\n",
        "# initial_weights = 'supervised_pretrain'\n",
        "# pretrain=False\n",
        "# numfit=50\n",
        "# num_epochs='na'\n",
        "# freeze_num_epochs = 'na'\n",
        "# freeze_numfit=3\n",
        "\n",
        "# main = main_train(dls_train=dls_train,dls_tune=dls_tune,dls_valid=dls_valid, xval=xval, yval=yval,\n",
        "#         aug_pipelines=aug_pipelines, aug_pipelines_tune=aug_pipelines_tune, aug_pipelines_test=aug_pipelines_test, \n",
        "#         initial_weights=initial_weights,pretrain=pretrain,\n",
        "#         num_epochs=num_epochs,numfit=numfit,freeze_num_epochs=freeze_num_epochs,freeze_numfit=freeze_numfit,\n",
        "#         print_report=True,\n",
        "#                  )\n",
        "\n",
        "# metrics = main()"
      ],
      "metadata": {
        "id": "Pwb66YCzYB_L"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, great."
      ],
      "metadata": {
        "id": "hKDFSQN6tpcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check for BT as well:"
      ],
      "metadata": {
        "id": "m1eGQ1wCsW3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This hasn't been working. Hypothesis: need to freeze backbone for longer."
      ],
      "metadata": {
        "id": "Y9AqSQdZrjGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Non default inputs\n",
        "initial_weights = 'bt_pretrain'\n",
        "pretrain=False\n",
        "numfit=50\n",
        "num_epochs='na'\n",
        "freeze_num_epochs = 'na'\n",
        "freeze_numfit=6\n",
        "\n",
        "main = main_train(dls_train=dls_train,dls_tune=dls_tune,dls_valid=dls_valid, xval=xval, yval=yval,\n",
        "        aug_pipelines=aug_pipelines, aug_pipelines_tune=aug_pipelines_tune, aug_pipelines_test=aug_pipelines_test, \n",
        "        initial_weights=initial_weights,pretrain=pretrain,\n",
        "        num_epochs=num_epochs,numfit=numfit,freeze_num_epochs=freeze_num_epochs,freeze_numfit=freeze_numfit,\n",
        "        print_report=True,\n",
        "                 )\n",
        "\n",
        "metrics = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TGwTUBfjsY8-",
        "outputId": "ab1fc65e-692b-4293-8944-a10851e4fa32"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside create_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_barlowtwins_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside new my_splitter\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.226256</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.073040</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.875264</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.674538</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.505282</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.353871</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/fastprogress/fastprogress.py:73: UserWarning: Your generator is empty.\n",
            "  warn(\"Your generator is empty.\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.597909</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.593243</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.556044</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.509185</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.493918</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.461221</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.438027</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.417411</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.402489</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.381574</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.365571</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.350935</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.337110</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.323093</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.312969</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.299931</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.290464</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.282761</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.275003</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.266513</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.258244</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.252754</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.245273</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.237631</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.232575</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.226561</td>\n",
              "      <td>None</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.220863</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.215827</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.209395</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.205046</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.201187</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.197489</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.193192</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.189222</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.186560</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.182678</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.178166</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.174686</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.170490</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.167457</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.163255</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.160897</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.158485</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.155057</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.152685</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.149356</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.147234</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.144764</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.142426</td>\n",
              "      <td>None</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.140578</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "         actinic keratosis       0.59      0.65      0.62        20\n",
            "      basal cell carcinoma       0.61      0.70      0.65        20\n",
            "            dermatofibroma       0.80      0.84      0.82        19\n",
            "                  melanoma       0.33      0.25      0.29        20\n",
            "                     nevus       0.38      0.50      0.43        20\n",
            "pigmented benign keratosis       0.62      0.40      0.48        20\n",
            "      seborrheic keratosis       0.47      0.47      0.47        15\n",
            "   squamous cell carcinoma       0.53      0.50      0.51        20\n",
            "           vascular lesion       0.86      0.90      0.88        20\n",
            "\n",
            "                  accuracy                           0.58       174\n",
            "                 macro avg       0.58      0.58      0.57       174\n",
            "              weighted avg       0.58      0.58      0.57       174\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try freezing the resnet for longer"
      ],
      "metadata": {
        "id": "fzCHEf3ortZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Non default inputs\n",
        "initial_weights = 'bt_pretrain'\n",
        "pretrain=False\n",
        "numfit=30\n",
        "num_epochs='na'\n",
        "freeze_num_epochs = 'na'\n",
        "freeze_numfit=20\n",
        "\n",
        "main = main_train(dls_train=dls_train,dls_tune=dls_tune,dls_valid=dls_valid, xval=xval, yval=yval,\n",
        "        aug_pipelines=aug_pipelines, aug_pipelines_tune=aug_pipelines_tune, aug_pipelines_test=aug_pipelines_test, \n",
        "        initial_weights=initial_weights,pretrain=pretrain,\n",
        "        num_epochs=num_epochs,numfit=numfit,freeze_num_epochs=freeze_num_epochs,freeze_numfit=freeze_numfit,\n",
        "        print_report=True,\n",
        "                 )\n",
        "\n",
        "metrics = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SgU_vhxIrsfl",
        "outputId": "7c97a029-d526-49d0-cdf2-9f8e8c6c56cc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside create_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_barlowtwins_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside new my_splitter\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.231801</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.133175</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.024523</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.909593</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.797583</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.687701</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.578967</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.478047</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.384773</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.301351</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.224089</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.145226</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.079109</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.013243</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.953687</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.899877</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.854214</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.808105</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.778414</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.738356</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/fastprogress/fastprogress.py:73: UserWarning: Your generator is empty.\n",
            "  warn(\"Your generator is empty.\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.287917</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.275527</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.244412</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.236292</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.236868</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.229989</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.222742</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.212479</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.203615</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.205114</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.206001</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.206740</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.202951</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.199960</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.197122</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.193945</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.190946</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.186074</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.183553</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.181393</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.178316</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.175299</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.174035</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.171822</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.168773</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.167231</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.165066</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.162566</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.159780</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.157456</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "         actinic keratosis       0.50      0.60      0.55        20\n",
            "      basal cell carcinoma       0.68      0.65      0.67        20\n",
            "            dermatofibroma       0.74      0.74      0.74        19\n",
            "                  melanoma       0.28      0.25      0.26        20\n",
            "                     nevus       0.53      0.50      0.51        20\n",
            "pigmented benign keratosis       0.50      0.55      0.52        20\n",
            "      seborrheic keratosis       0.25      0.20      0.22        15\n",
            "   squamous cell carcinoma       0.50      0.45      0.47        20\n",
            "           vascular lesion       0.83      0.95      0.88        20\n",
            "\n",
            "                  accuracy                           0.55       174\n",
            "                 macro avg       0.53      0.54      0.54       174\n",
            "              weighted avg       0.54      0.55      0.54       174\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow! Still really bad performance. Adding a nonlinear head seems to really harm BT, so far. All I can think of is pretraining it really well."
      ],
      "metadata": {
        "id": "hR7GXmgktd63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## It appears the nonlinear head is harming BT performance. Weird. "
      ],
      "metadata": {
        "id": "bGRIQ8kwgpM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "Lmn2HaqbeXV1",
        "outputId": "054a1731-6331-4f39-db4a-374270b3edf2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a871fdc9ebee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ok, let's just try adding linear layer to BT backbone"
      ],
      "metadata": {
        "id": "y3HAbvLpg-3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All we do is uncomment one line in `create_model`, and edited the splitter:"
      ],
      "metadata": {
        "id": "vOQroyjKhfze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\n",
        "\n",
        "\n",
        "def create_model(which_model,device,ps=8192,n_in=3):\n",
        "    print('inside create_model')\n",
        "\n",
        "    #pretrained=True if 'which_model' in ['bt_pretrain', 'supervised_pretrain'] else False\n",
        "\n",
        "    if which_model == 'bt_pretrain': model = torch.hub.load('facebookresearch/barlowtwins:main', 'resnet50')\n",
        "    \n",
        "    elif which_model == 'no_pretrain': model = resnet50()\n",
        "\n",
        "    elif which_model == 'supervised_pretrain': model = resnet50(weights='IMAGENET1K_V2')\n",
        "\n",
        "    #ignore the 'pretrained=False' argument here. Just means we use the weights above \n",
        "    #(which themselves are either pretrained or not)\n",
        "    encoder = get_resnet_encoder(model)\n",
        "    #encoder = HeadEncoder(encoder,device='cpu')\n",
        "\n",
        "    model = create_barlow_twins_model(encoder, hidden_size=ps,projection_size=ps,nlayers=3)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        model.cuda()\n",
        "        encoder.cuda()\n",
        "\n",
        "\n",
        "    return model,encoder\n",
        "\n",
        "\n",
        "def my_splitter(m):\n",
        "    print('inside new my_splitter')\n",
        "    return L(sequential(*m[0]),sequential(m[1])).map(params)"
      ],
      "metadata": {
        "id": "AkRDnzD7hDEt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # #Verify that splitter freezes expected part of model, from linear point of view:\n",
        "\n",
        "bt_model,encoder = create_model(which_model='bt_pretrain',ps=8192,device=device)\n",
        "model = sequential(encoder,nn.Linear(2048,9))\n",
        "learn = Learner(dls_tune,model,splitter=my_splitter,cbs = [LinearBt(aug_pipelines=aug_pipelines_tune,n_in=3)],wd=0.0)\n",
        "learn.freeze()\n",
        "print('resnet should be frozen, then should just have unfrozen linear layer')\n",
        "learn.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7Q0__sZwhvMX",
        "outputId": "e1cf2681-fb65-4003-b414-02e8b2b7a4b6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside create_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_barlowtwins_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside new my_splitter\n",
            "resnet should be frozen, encoder_head + linear layer unfrozen\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential (Input shape: 256 x 3 x 128 x 128)\n",
              "============================================================================\n",
              "Layer (type)         Output Shape         Param #    Trainable \n",
              "============================================================================\n",
              "                     256 x 64 x 64 x 64  \n",
              "Conv2d                                    9408       False     \n",
              "BatchNorm2d                               128        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "MaxPool2d                                                      \n",
              "Conv2d                                    4096       False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 32 x 32 \n",
              "Conv2d                                    32768      False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    131072     False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 16 x 16 \n",
              "Conv2d                                    131072     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    524288     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 8 x 8   \n",
              "Conv2d                                    524288     False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    2097152    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 1 x 1  \n",
              "AdaptiveAvgPool2d                                              \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048          \n",
              "Flatten                                                        \n",
              "____________________________________________________________________________\n",
              "                     256 x 9             \n",
              "Linear                                    18441      True      \n",
              "____________________________________________________________________________\n",
              "\n",
              "Total params: 23,526,473\n",
              "Total trainable params: 71,561\n",
              "Total non-trainable params: 23,454,912\n",
              "\n",
              "Optimizer used: <function Adam at 0x7fbc7740a0d0>\n",
              "Loss function: <bound method LinearBt.lf of LinearBt>\n",
              "\n",
              "Model frozen up to parameter group #1\n",
              "\n",
              "Callbacks:\n",
              "  - TrainEvalCallback\n",
              "  - CastToTensor\n",
              "  - LinearBt\n",
              "  - Recorder\n",
              "  - ProgressCallback"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Experiment: Linear layer. Comparing to with nonlinear head..."
      ],
      "metadata": {
        "id": "SsFB8zmalENv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Non default inputs\n",
        "initial_weights = 'bt_pretrain'\n",
        "pretrain=False\n",
        "numfit=50\n",
        "num_epochs='na'\n",
        "freeze_num_epochs = 'na'\n",
        "freeze_numfit=6\n",
        "\n",
        "avg=0\n",
        "for _ in range(3):\n",
        "\n",
        "    main = main_train(dls_train=dls_train,dls_tune=dls_tune,dls_valid=dls_valid, xval=xval, yval=yval,\n",
        "            aug_pipelines=aug_pipelines, aug_pipelines_tune=aug_pipelines_tune, aug_pipelines_test=aug_pipelines_test, \n",
        "            initial_weights=initial_weights,pretrain=pretrain,\n",
        "            num_epochs=num_epochs,numfit=numfit,freeze_num_epochs=freeze_num_epochs,freeze_numfit=freeze_numfit,\n",
        "            print_report=True,\n",
        "                    )\n",
        "\n",
        "    metrics = main()\n",
        "    avg += metrics['acc']\n",
        "\n",
        "avg/3\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o7cYpTNwioAb",
        "outputId": "1d8e3e66-0529-40f3-f03d-e61c7d4b3529"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside create_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_barlowtwins_main\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside new my_splitter\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.198341</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.196352</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.192322</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.186077</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.176804</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.163485</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/fastprogress/fastprogress.py:73: UserWarning: Your generator is empty.\n",
            "  warn(\"Your generator is empty.\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.069430</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.068809</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.059758</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.053328</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.044665</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.034600</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.023101</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.011719</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.996386</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.976805</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.953595</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.927138</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.894686</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.859201</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.817305</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.770916</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.722516</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.674248</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.622990</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.571304</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.517367</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.462131</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.408615</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.356084</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.305149</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.257388</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.208990</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.163944</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.117596</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.075497</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.033147</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.992616</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.954866</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.918212</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.884951</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.852918</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.821603</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.791009</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.762083</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.735640</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.710138</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.687064</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.663541</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.641257</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.619402</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.599954</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.581083</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.563153</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.545914</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.529574</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "         actinic keratosis       0.64      0.70      0.67        20\n",
            "      basal cell carcinoma       0.68      0.65      0.67        20\n",
            "            dermatofibroma       0.80      0.84      0.82        19\n",
            "                  melanoma       0.50      0.40      0.44        20\n",
            "                     nevus       0.59      0.65      0.62        20\n",
            "pigmented benign keratosis       0.56      0.70      0.62        20\n",
            "      seborrheic keratosis       0.50      0.40      0.44        15\n",
            "   squamous cell carcinoma       0.71      0.50      0.59        20\n",
            "           vascular lesion       0.79      0.95      0.86        20\n",
            "\n",
            "                  accuracy                           0.65       174\n",
            "                 macro avg       0.64      0.64      0.64       174\n",
            "              weighted avg       0.65      0.65      0.64       174\n",
            "\n",
            "inside create_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_barlowtwins_main\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside new my_splitter\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.202211</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.199894</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.196281</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.189226</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.179641</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.165967</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/fastprogress/fastprogress.py:73: UserWarning: Your generator is empty.\n",
            "  warn(\"Your generator is empty.\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.065496</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.065332</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.058458</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.051941</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.044210</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.034340</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.024710</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.011851</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.997306</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.978611</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.957904</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.932869</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.902417</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.866896</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.825991</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.786438</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.738470</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.689090</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.637896</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.583727</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.531953</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.481654</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.431111</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.379689</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.330266</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.280401</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.231820</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.184975</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.139744</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.095591</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.053545</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.013233</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.974759</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.937454</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.902206</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.869110</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.836766</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.807037</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.777451</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.750637</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.724277</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.699332</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.676720</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.654189</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.633398</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.612627</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.593680</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.575676</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.559913</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.543637</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "         actinic keratosis       0.70      0.70      0.70        20\n",
            "      basal cell carcinoma       0.69      0.55      0.61        20\n",
            "            dermatofibroma       0.80      0.84      0.82        19\n",
            "                  melanoma       0.44      0.35      0.39        20\n",
            "                     nevus       0.55      0.55      0.55        20\n",
            "pigmented benign keratosis       0.48      0.50      0.49        20\n",
            "      seborrheic keratosis       0.47      0.53      0.50        15\n",
            "   squamous cell carcinoma       0.63      0.60      0.62        20\n",
            "           vascular lesion       0.76      0.95      0.84        20\n",
            "\n",
            "                  accuracy                           0.62       174\n",
            "                 macro avg       0.61      0.62      0.61       174\n",
            "              weighted avg       0.62      0.62      0.62       174\n",
            "\n",
            "inside create_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_barlowtwins_main\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside new my_splitter\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.197782</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.196333</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.192182</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.186058</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.176118</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.162399</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/fastprogress/fastprogress.py:73: UserWarning: Your generator is empty.\n",
            "  warn(\"Your generator is empty.\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.069100</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.061539</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.052295</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.048429</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.040670</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.032983</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.023321</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.009928</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.995080</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.974495</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.952185</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.925520</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.893992</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.854636</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.814561</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.768071</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.723325</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.673268</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.622761</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.572439</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.520319</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.467811</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.416576</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.366465</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.314230</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.263600</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.214913</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.166810</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.120296</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.077868</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.036856</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.996838</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.958666</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.921926</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.887448</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.853289</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.820489</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.789365</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.760423</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.732963</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.706414</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.681582</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.659004</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.637929</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.616540</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.596444</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.577089</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.559685</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.544365</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.528249</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "         actinic keratosis       0.59      0.65      0.62        20\n",
            "      basal cell carcinoma       0.83      0.75      0.79        20\n",
            "            dermatofibroma       0.84      0.84      0.84        19\n",
            "                  melanoma       0.57      0.40      0.47        20\n",
            "                     nevus       0.58      0.70      0.64        20\n",
            "pigmented benign keratosis       0.48      0.60      0.53        20\n",
            "      seborrheic keratosis       0.60      0.60      0.60        15\n",
            "   squamous cell carcinoma       0.65      0.55      0.59        20\n",
            "           vascular lesion       0.90      0.90      0.90        20\n",
            "\n",
            "                  accuracy                           0.67       174\n",
            "                 macro avg       0.67      0.67      0.67       174\n",
            "              weighted avg       0.67      0.67      0.67       174\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6455938617388407"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So just adding linear layer: ~ 0.65 accuracy, vs. ~ 0.6 for training a nonlinear head. "
      ],
      "metadata": {
        "id": "BAUiK-6xp4mL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, maybe. But might need to compare to just linear head later."
      ],
      "metadata": {
        "id": "hRmVsvoYvlES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Warning: we added new stuff above... i.e. linear probe. "
      ],
      "metadata": {
        "id": "klhB5xE0hrGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory baseline ensembling with a nonlinear head, without pretraining:"
      ],
      "metadata": {
        "id": "TcWo9NGxwxt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to make sure the nonlinear head model is still around..."
      ],
      "metadata": {
        "id": "TsonTn0ht9hV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\n",
        "\n",
        "#| export\n",
        "\n",
        "class HeadEncoder(nn.Module):\n",
        "    \"Basic nonlinear \"\n",
        "    def __init__(self,resnet_encoder,device='cuda'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.resnet_encoder=resnet_encoder\n",
        "\n",
        "        self.head_encoder = sequential(nn.Linear(2048,2048),nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "                               nn.ReLU(inplace=True))\n",
        "        \n",
        "        self.device = torch.device(device)\n",
        "        self.to(self.device)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.resnet_encoder(x)\n",
        "        x=self.head_encoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def create_model(which_model,device,ps=8192,n_in=3):\n",
        "    print('inside create_model')\n",
        "\n",
        "    #pretrained=True if 'which_model' in ['bt_pretrain', 'supervised_pretrain'] else False\n",
        "\n",
        "    if which_model == 'bt_pretrain': model = torch.hub.load('facebookresearch/barlowtwins:main', 'resnet50')\n",
        "    \n",
        "    elif which_model == 'no_pretrain': model = resnet50()\n",
        "\n",
        "    elif which_model == 'supervised_pretrain': model = resnet50(weights='IMAGENET1K_V2')\n",
        "\n",
        "    #ignore the 'pretrained=False' argument here. Just means we use the weights above \n",
        "    #(which themselves are either pretrained or not)\n",
        "    encoder = get_resnet_encoder(model)\n",
        "    encoder = HeadEncoder(encoder,device='cpu')\n",
        "\n",
        "    model = create_barlow_twins_model(encoder, hidden_size=ps,projection_size=ps,nlayers=3)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        model.cuda()\n",
        "        encoder.cuda()\n",
        "\n",
        "\n",
        "    return model,encoder\n",
        "\n",
        "bt_model,encoder = create_model(which_model='bt_pretrain',ps=8192,device=device)\n",
        "\n",
        "def my_splitter_bt(m):\n",
        "\n",
        "    return L(sequential(*m.encoder.resnet_encoder),sequential(m.encoder.head_encoder,m.projector)).map(params)\n",
        "\n",
        "test_eq(len(my_splitter_bt(bt_model)),2)\n",
        "\n",
        "class main_train:\n",
        "    \"\"\"Instantiate and (optionally) train the encoder. Then fine-tune the supervised model. \n",
        "    Outputs metrics on validation data\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dls_train, #used for training BT (if pretrain=True)\n",
        "                 dls_tune , #used for tuning\n",
        "                 dls_valid, #used to compute metrics / evaluate results. \n",
        "                 xval, #currently `predict_model` below assumes this is entire validation / test data\n",
        "                 yval,\n",
        "                 aug_pipelines, #the aug pipeline for self-supervised learning\n",
        "                 aug_pipelines_tune, #the aug pipeline for supervised learning\n",
        "                 aug_pipelines_test, #test (or valid) time augmentations \n",
        "                 initial_weights, #Which initial weights to use\n",
        "                 pretrain, #Whether to fit BT\n",
        "                 num_epochs, #number of BT fit epochs\n",
        "                 numfit, #number of tune_fit epochs\n",
        "                 freeze_num_epochs, #How many epochs to freeze body for when training BT\n",
        "                 freeze_numfit, #How many epochs to freeze body for when fine tuning\n",
        "                 ps=8192, #projection size\n",
        "                 n_in=3, #color channels\n",
        "                 indim=2048, #dimension output of encoder (2048 for resnet50)\n",
        "                 outdim=9, #number of classes\n",
        "                 print_report=False, #F1 metrics etc\n",
        "                 print_plot=False, #ROC curve\n",
        "                 ):\n",
        "        store_attr()\n",
        "        self.vocab = self.dls_valid.vocab\n",
        "        self.device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "\n",
        "                \n",
        "                 \n",
        "\n",
        "                 #Soon we might want to save some models here:\n",
        "\n",
        "                 #if self.model_type == 'res_proj': test_eq(self.fit_policy,'resnet_fine_tune') #I THINK this is only viable option?\n",
        "                 #self.encoder_path = f'/content/drive/My Drive/models/baselineencoder_initial_weights={self.initial_weights}_pretrain={self.pretrain}.pth'\n",
        "                 #self.tuned_model_path = f'/content/drive/My Drive/models/baselinefinetuned_initial_weights={self.initial_weights}_pretrain={self.pretrain}.pth'\n",
        "\n",
        "    @staticmethod\n",
        "    def fit(learn,fit_type,epochs,freeze_epochs,initial_weights):\n",
        "        \"\"\"We can patch in a modification, e.g. if we want subtype of fine_tune:supervised_pretrain to be different\n",
        "        to fine_tune:bt_pretrain\"\"\"\n",
        "\n",
        "        if fit_type == 'encoder_fine_tune': #i.e. barlow twins\n",
        "\n",
        "            learn.encoder_fine_tune(epochs,freeze_epochs=freeze_epochs) \n",
        "\n",
        "        elif fit_type == 'fine_tune':\n",
        "            \n",
        "            #elif initial_weights == 'supervised_pretrain':\n",
        "            learn.linear_fine_tune(epochs,freeze_epochs=freeze_epochs) \n",
        "\n",
        "        else: raise Exception('Fit policy not of expected form')\n",
        "\n",
        "    def train_encoder(self):\n",
        "        \"create encoder and (optionally, if pretrain=True) train with BT algorithm, according to fit_policy\"\n",
        "\n",
        "        try: #get existing encoder and plonk on new projector\n",
        "            encoder = self.encoder\n",
        "            encoder.cpu()\n",
        "            bt_model = create_barlow_twins_model(encoder, hidden_size=self.ps,projection_size=self.ps,nlayers=3)\n",
        "            bt_model.cuda()\n",
        "\n",
        "        except AttributeError: #otherwise, create\n",
        "            bt_model,encoder = create_model(which_model=self.initial_weights,ps=self.ps,device=self.device)\n",
        "\n",
        "        if self.pretrain: #train encoder according to fit policy\n",
        "\n",
        "            learn = Learner(self.dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(self.aug_pipelines,n_in=self.n_in,lmb=1/self.ps,print_augs=False)])\n",
        "            main_train.fit(learn,fit_type='encoder_fine_tune',\n",
        "                           epochs=self.num_epochs,freeze_epochs=self.freeze_num_epochs,\n",
        "                           initial_weights=self.initial_weights\n",
        "                          )\n",
        "            \n",
        "        self.encoder = bt_model.encoder\n",
        "\n",
        "    def fine_tune(self):\n",
        "        \"fine tune in supervised fashion, according to tune_fit_policy, and get metrics\"\n",
        "\n",
        "        #encoder = pickle.loads(pickle.dumps(self.encoder)) #We might want to pretrain once and fine tune several times (varying e.g. tune augs)\n",
        "\n",
        "        try: \n",
        "            encoder = self.encoder\n",
        "        \n",
        "        except AttributeError:\n",
        "            _,self.encoder = create_model(which_model=self.initial_weights,ps=self.ps,device=device)\n",
        "\n",
        "        #model = LM(self.encoder)\n",
        "        model = sequential(self.encoder,nn.Linear(2048,9))\n",
        "        \n",
        "        learn = Learner(self.dls_tune,model,splitter=my_splitter,cbs = [LinearBt(aug_pipelines=self.aug_pipelines_tune,n_in=self.n_in)],wd=0.0)\n",
        "\n",
        "        #debugging\n",
        "        #learn = Learner(self.dls_tune,model,cbs = [LinearBt(aug_pipelines=self.aug_pipelines_tune,n_in=self.n_in)],wd=0.0)\n",
        "\n",
        "        main_train.fit(learn,fit_type='fine_tune',\n",
        "                       epochs=self.numfit,freeze_epochs=self.freeze_numfit,\n",
        "                       initial_weights=self.initial_weights\n",
        "                      ) #fine tuning (don't confuse this with fit policy!)\n",
        "        \n",
        "        #model.encoder=encoder\n",
        "        scores,preds, acc = predict_model(self.xval,self.yval,model=model,aug_pipelines_test=self.aug_pipelines_test,numavg=3)\n",
        "        #metrics dict will have f1 score, auc etc etc\n",
        "        metrics = classification_report_wrapper(preds, self.yval, self.vocab, print_report=self.print_report)\n",
        "        auc_dict = plot_roc(self.yval,preds,self.vocab,print_plot=self.print_plot)\n",
        "        metrics['acc'],metrics['auc_dict'],metrics['scores'],metrics['preds'],metrics['xval'],metrics['yval'] = acc,auc_dict,scores,preds,self.xval,self.yval\n",
        "  \n",
        "        #torch.save(model.state_dict(), self.tuned_model_path)\n",
        "        return metrics #\n",
        "\n",
        "    def __call__(self):\n",
        "\n",
        "        self.train_encoder() #train (or extract) the encoder\n",
        "        metrics = self.fine_tune()\n",
        "        \n",
        "        return metrics\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tc2NWokt8-K",
        "outputId": "b670d9b1-6f09-415e-9e6a-06acefb75c80"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside create_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_barlowtwins_main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Verify that splitter freezes expected part of model:\n",
        "\n",
        "# #test : manual. BT\n",
        "\n",
        "learn = Learner(dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(aug_pipelines,n_in=3,lmb=1/8192,print_augs=False)])\n",
        "learn.freeze()\n",
        "print('resnet should be frozen, encoder head + projector unfrozen')\n",
        "learn.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2V9iSL4CupFj",
        "outputId": "9c865909-62df-412b-a940-f1b364d0bbf9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resnet should be frozen, encoder head + projector unfrozen\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BarlowTwinsModel (Input shape: 256 x 3 x 128 x 128)\n",
              "============================================================================\n",
              "Layer (type)         Output Shape         Param #    Trainable \n",
              "============================================================================\n",
              "                     256 x 64 x 64 x 64  \n",
              "Conv2d                                    9408       False     \n",
              "BatchNorm2d                               128        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "MaxPool2d                                                      \n",
              "Conv2d                                    4096       False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 32 x 32 \n",
              "Conv2d                                    32768      False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    131072     False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 16 x 16 \n",
              "Conv2d                                    131072     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    524288     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 8 x 8   \n",
              "Conv2d                                    524288     False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    2097152    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 1 x 1  \n",
              "AdaptiveAvgPool2d                                              \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048          \n",
              "Flatten                                                        \n",
              "Linear                                    4196352    True      \n",
              "BatchNorm1d                               4096       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 8192          \n",
              "Linear                                    16785408   True      \n",
              "BatchNorm1d                               16384      True      \n",
              "ReLU                                                           \n",
              "Linear                                    67117056   True      \n",
              "BatchNorm1d                               16384      True      \n",
              "ReLU                                                           \n",
              "Linear                                    67117056   True      \n",
              "____________________________________________________________________________\n",
              "\n",
              "Total params: 178,760,768\n",
              "Total trainable params: 155,305,856\n",
              "Total non-trainable params: 23,454,912\n",
              "\n",
              "Optimizer used: <function Adam at 0x7fa2826b0160>\n",
              "Loss function: <bound method BarlowTwins.lf of BarlowTwins>\n",
              "\n",
              "Model frozen up to parameter group #1\n",
              "\n",
              "Callbacks:\n",
              "  - TrainEvalCallback\n",
              "  - CastToTensor\n",
              "  - BarlowTwins\n",
              "  - Recorder\n",
              "  - ProgressCallback"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # #Verify that splitter freezes expected part of model, from linear point of view:\n",
        "\n",
        "bt_model,encoder = create_model(which_model='bt_pretrain',ps=8192,device=device)\n",
        "model = sequential(encoder,nn.Linear(2048,9))\n",
        "learn = Learner(dls_tune,model,splitter=my_splitter,cbs = [LinearBt(aug_pipelines=aug_pipelines_tune,n_in=3)],wd=0.0)\n",
        "learn.freeze()\n",
        "print('resnet should be frozen, then should just have unfrozen linear layer')\n",
        "learn.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OMc8zE1Kup1y",
        "outputId": "a8052ce3-7d4b-405b-82e7-a205657acf7c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside create_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_barlowtwins_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside new my_splitter\n",
            "resnet should be frozen, then should just have unfrozen linear layer\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential (Input shape: 256 x 3 x 128 x 128)\n",
              "============================================================================\n",
              "Layer (type)         Output Shape         Param #    Trainable \n",
              "============================================================================\n",
              "                     256 x 64 x 64 x 64  \n",
              "Conv2d                                    9408       False     \n",
              "BatchNorm2d                               128        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "MaxPool2d                                                      \n",
              "Conv2d                                    4096       False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 64 x 32 x 32  \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               128        True      \n",
              "Conv2d                                    36864      False     \n",
              "BatchNorm2d                               128        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 32 x 32 \n",
              "Conv2d                                    16384      False     \n",
              "BatchNorm2d                               512        True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 32 x 32 \n",
              "Conv2d                                    32768      False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    131072     False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 128 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               256        True      \n",
              "Conv2d                                    147456     False     \n",
              "BatchNorm2d                               256        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 16 x 16 \n",
              "Conv2d                                    65536      False     \n",
              "BatchNorm2d                               1024       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 16 x 16 \n",
              "Conv2d                                    131072     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    524288     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 256 x 8 x 8   \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               512        True      \n",
              "Conv2d                                    589824     False     \n",
              "BatchNorm2d                               512        True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 1024 x 8 x 8  \n",
              "Conv2d                                    262144     False     \n",
              "BatchNorm2d                               2048       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 8 x 8   \n",
              "Conv2d                                    524288     False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "Conv2d                                    2097152    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 512 x 4 x 4   \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "Conv2d                                    2359296    False     \n",
              "BatchNorm2d                               1024       True      \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 4 x 4  \n",
              "Conv2d                                    1048576    False     \n",
              "BatchNorm2d                               4096       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048 x 1 x 1  \n",
              "AdaptiveAvgPool2d                                              \n",
              "____________________________________________________________________________\n",
              "                     256 x 2048          \n",
              "Flatten                                                        \n",
              "Linear                                    4196352    True      \n",
              "BatchNorm1d                               4096       True      \n",
              "ReLU                                                           \n",
              "____________________________________________________________________________\n",
              "                     256 x 9             \n",
              "Linear                                    18441      True      \n",
              "____________________________________________________________________________\n",
              "\n",
              "Total params: 27,726,921\n",
              "Total trainable params: 4,272,009\n",
              "Total non-trainable params: 23,454,912\n",
              "\n",
              "Optimizer used: <function Adam at 0x7fa2826b0160>\n",
              "Loss function: <bound method LinearBt.lf of LinearBt>\n",
              "\n",
              "Model frozen up to parameter group #1\n",
              "\n",
              "Callbacks:\n",
              "  - TrainEvalCallback\n",
              "  - CastToTensor\n",
              "  - LinearBt\n",
              "  - Recorder\n",
              "  - ProgressCallback"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experiment: pretrain the head for a really long time, with resnet frozen."
      ],
      "metadata": {
        "id": "B2rZbGpgvFgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Non default inputs\n",
        "initial_weights = 'bt_pretrain'\n",
        "pretrain=True\n",
        "numfit=50\n",
        "num_epochs=200\n",
        "freeze_num_epochs = 1\n",
        "freeze_numfit=6\n",
        "\n",
        "\n",
        "main = main_train(dls_train=dls_train,dls_tune=dls_tune,dls_valid=dls_valid, xval=xval, yval=yval,\n",
        "        aug_pipelines=aug_pipelines, aug_pipelines_tune=aug_pipelines_tune, aug_pipelines_test=aug_pipelines_test, \n",
        "        initial_weights=initial_weights,pretrain=pretrain,\n",
        "        num_epochs=num_epochs,numfit=numfit,freeze_num_epochs=freeze_num_epochs,freeze_numfit=freeze_numfit,\n",
        "        print_report=True,\n",
        "                )\n",
        "\n",
        "metrics = main()\n",
        "print(metrics['acc'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xKSSL4WOvV07",
        "outputId": "80bc08e4-5c89-416b-95fd-0bdb6bcb4e24"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside create_model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_barlowtwins_main\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>5618.505859</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/fastprogress/fastprogress.py:73: UserWarning: Your generator is empty.\n",
            "  warn(\"Your generator is empty.\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2225.252686</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2220.608398</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2020.624756</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1898.112427</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1812.766479</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1726.943970</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1662.019775</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1600.636841</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1549.371338</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1498.420288</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1453.601440</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1413.298218</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1375.709717</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1344.151855</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1308.969604</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1279.468750</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1247.379761</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1219.535400</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1190.677612</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1162.957031</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1138.811279</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1114.262451</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1091.691650</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1068.595093</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1045.490356</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1023.732422</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1003.405457</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>985.059509</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>968.948181</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>951.169250</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>935.535522</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>917.849365</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>903.588379</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>886.190125</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>871.231445</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>855.438904</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>840.972046</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>824.798035</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>811.757507</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>797.824707</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>784.760803</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>770.136536</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>756.785706</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>746.121582</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>733.413879</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>720.408020</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>706.812622</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>695.247803</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>684.849792</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>675.100708</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>665.224487</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>654.560791</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>644.721191</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>634.485535</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>625.267700</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>614.698486</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>604.850952</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>595.540894</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>588.320496</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>580.664429</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>571.778015</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>562.908997</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>555.578064</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>547.894165</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>539.096191</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>532.278809</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>524.385498</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>517.931641</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>510.660126</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>504.080048</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>496.859589</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>490.100128</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>483.727020</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>477.412323</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>470.285339</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>463.915100</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>457.534546</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>451.636963</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>445.503815</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>440.073944</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>434.222992</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>427.632721</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>422.399841</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>417.396088</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>412.992737</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>408.235474</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>403.750275</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>399.050690</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>394.780426</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>391.299805</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>386.701935</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>382.587250</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>377.476715</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>372.980438</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>369.737823</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>365.564423</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>361.682007</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>359.053253</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>355.582581</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>351.779938</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>347.775513</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>344.059906</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>340.558289</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>336.531189</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>334.191193</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>330.795990</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>327.979431</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>324.633972</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>322.454895</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>319.834198</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>316.641479</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>313.861969</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>311.251770</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>308.223450</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>305.610260</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>303.399628</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>301.086548</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>297.995087</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>295.670471</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>293.200226</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>290.569641</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>287.518921</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>285.104919</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>282.567139</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>279.759155</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>278.337769</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>275.934479</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>274.091492</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>272.255585</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>270.010925</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>268.165558</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>265.940277</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>263.649414</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>261.329590</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>259.465302</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>257.614258</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>256.129333</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>254.718704</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>252.911896</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>251.271332</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>249.524307</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>248.013641</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>247.095459</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>245.353455</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>244.123032</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>242.337189</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>241.039566</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>239.753983</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>238.253494</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>236.933548</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>235.454041</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>234.353973</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>232.862747</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>232.514114</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>231.578979</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>230.821915</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>230.353165</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>229.124847</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>227.675568</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>226.363342</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>225.140091</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>224.029037</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>222.698639</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>221.614761</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>220.620117</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>219.540741</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>218.437134</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>217.848373</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>217.137909</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>216.341766</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>215.482407</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>214.689087</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>214.476151</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>213.694885</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>213.001495</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>212.010315</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>211.059860</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>210.554810</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>209.788284</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>209.240829</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>209.052917</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>208.206985</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>207.598938</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>206.985321</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>206.404099</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>205.800568</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>205.147598</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>204.680145</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>204.534210</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>204.186234</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>203.357986</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>202.844528</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>202.317352</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>201.740326</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>201.145660</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>201.520203</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>201.879837</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>201.190002</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>200.703949</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>200.221161</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inside new my_splitter\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.209545</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.074716</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.903065</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.718835</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.545133</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.394172</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.575437</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.549418</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.523801</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.496486</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.469012</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.446456</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.421263</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.398532</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.371554</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.352936</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.332398</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.316469</td>\n",
              "      <td>None</td>\n",
              "      <td>00:07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.298481</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.284461</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.273237</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.262671</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.253104</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.244441</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.236579</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.228177</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.220256</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.216390</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.211926</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.206482</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.201531</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.196515</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.192127</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.185784</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.181490</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.177575</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.173571</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.170171</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.165984</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.163052</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.160043</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.157163</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.153340</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.150026</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.147056</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.144447</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.141952</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.139521</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.136280</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.133803</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.131742</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.129971</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.128727</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.127117</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.124934</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.123012</td>\n",
              "      <td>None</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "         actinic keratosis       0.57      0.65      0.60        20\n",
            "      basal cell carcinoma       0.50      0.50      0.50        20\n",
            "            dermatofibroma       0.74      0.74      0.74        19\n",
            "                  melanoma       0.32      0.30      0.31        20\n",
            "                     nevus       0.48      0.50      0.49        20\n",
            "pigmented benign keratosis       0.46      0.55      0.50        20\n",
            "      seborrheic keratosis       0.30      0.20      0.24        15\n",
            "   squamous cell carcinoma       0.56      0.45      0.50        20\n",
            "           vascular lesion       0.82      0.90      0.86        20\n",
            "\n",
            "                  accuracy                           0.54       174\n",
            "                 macro avg       0.53      0.53      0.53       174\n",
            "              weighted avg       0.53      0.54      0.53       174\n",
            "\n",
            "0.540229856967926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wow, it didn't work at all."
      ],
      "metadata": {
        "id": "5RtgCHENIIFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "\n",
        "Depending on above results, we need to:\n",
        "\n",
        "- Explore the effect of using `fine_tune` instead of `linear_fine_tune`. i.e. using different learning rates.\n",
        "- Perhaps try using a nice FastAI head, via `create_head` magic. Perhaps we can pretrain this guy with BT instead?\n",
        "- Run head-probing experiments, where head may be linear or non linear. That is, freeze the resnet the whole way. Not sure whether to bother whether this. Probably not actually. Paper sugges LP-FT works best, after all.\n"
      ],
      "metadata": {
        "id": "i59x4iICFQTc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MI4qEtmgAB5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just look at bt weights first:"
      ],
      "metadata": {
        "id": "L9GD7DUuxFGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_main_train(initial_weights,num_epochs,freeze_numfit,freeze_num_epochs,numfit=100,pretrain=False,num=5):\n",
        "    \"run main_train num times.\"\n",
        "\n",
        "    main_dict = {}\n",
        "    for i in range(num):\n",
        "\n",
        "        main = main_train(dls_train=dls_train,dls_tune=dls_tune,dls_valid=dls_valid, xval=xval, yval=yval,\n",
        "                aug_pipelines=aug_pipelines, aug_pipelines_tune=aug_pipelines_tune, aug_pipelines_test=aug_pipelines_test, \n",
        "                initial_weights=initial_weights,pretrain=pretrain,\n",
        "                num_epochs=num_epochs,numfit=numfit,freeze_num_epochs=freeze_num_epochs,freeze_numfit=freeze_numfit,\n",
        "                print_report=True,\n",
        "                        )\n",
        "        \n",
        "        metrics = main()\n",
        "        main_dict[i] = metrics\n",
        "\n",
        "    return main_dict\n",
        "        "
      ],
      "metadata": {
        "id": "a4ux74eoiMkh"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment: Baseline, non linear head with no pretraining. Ensemble."
      ],
      "metadata": {
        "id": "MLYZ2Iplf8L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_weights='bt_pretrain'\n",
        "main_dict = run_main_train(initial_weights=initial_weights,numfit=50,num_epochs='na',freeze_num_epochs='na',pretrain=False,freeze_numfit=3,num=3)\n",
        "\n",
        "from itertools import combinations\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "print('Results for ensembling within bt weights:')\n",
        "\n",
        "bt_results = list(main_dict.values())\n",
        "\n",
        "_bt_results = [k['acc'] for k in bt_results]\n",
        "print(_bt_results)\n",
        "print(mean(_bt_results))\n",
        "print(stdev(_bt_results))\n",
        "\n",
        "bt_results = list(combinations(bt_results,2)) #all pairs of results. So for num=3, will be 3\n",
        "for v in bt_results:\n",
        "\n",
        "    print(f\"\\nAcc of first guy in ensemble is: {v[0]['acc']}\")\n",
        "    print(f\"Acc of second guy in ensemble is: {v[1]['acc']}\")\n",
        "    _,acc = predict_ensemble(yval=yval,scores1=v[0]['scores'],scores2=v[1]['scores'])\n",
        "    print(f'Acc of ensemble is:{acc}\\n')"
      ],
      "metadata": {
        "id": "GKxhWwlLxCTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_weights='bt_pretrain'\n",
        "main_dict = run_main_train(initial_weights=initial_weights,numfit=30,num_epochs='na',freeze_num_epochs='na',pretrain=False,freeze_numfit=3,num=3)\n",
        "\n",
        "from itertools import combinations\n",
        "from statistics import mean\n",
        "from statistics import stdev\n",
        "\n",
        "print('Results for ensembling within bt weights:')\n",
        "\n",
        "bt_results = list(main_dict.values())\n",
        "\n",
        "_bt_results = [k['acc'] for k in bt_results]\n",
        "print(_bt_results)\n",
        "print(mean(_bt_results))\n",
        "print(stdev(_bt_results))\n",
        "\n",
        "bt_results = list(combinations(bt_results,2)) #all pairs of results. So for num=3, will be 3\n",
        "for v in bt_results:\n",
        "\n",
        "    print(f\"\\nAcc of first guy in ensemble is: {v[0]['acc']}\")\n",
        "    print(f\"Acc of second guy in ensemble is: {v[1]['acc']}\")\n",
        "    _,acc = predict_ensemble(yval=yval,scores1=v[0]['scores'],scores2=v[1]['scores'])\n",
        "    print(f'Acc of ensemble is:{acc}\\n')"
      ],
      "metadata": {
        "id": "kQLm7lpIDVbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert False"
      ],
      "metadata": {
        "id": "zMfncqPYNTPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's look at doing some pretraining. For now, say BT pretrain for 10 epochs. Freezing the projector doesn't make sense remember: we are aligning the random head and projector. But, check above: the backbone encoder is frozen. \n",
        "\n",
        "Thinking about this more: training the encoder_head on a frozen backbone will make them less variable."
      ],
      "metadata": {
        "id": "tBADYAKmxxZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's try an ensemble with the resnet frozen the whole way through.\n",
        "\n",
        "Notice that the resnet is kept frozen the whole way through (you can look up above to verify the freeze is working as expected):"
      ],
      "metadata": {
        "id": "BVP8NHY004hO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment: pretrain the nonlinear head with BT"
      ],
      "metadata": {
        "id": "AhZN_wuNgfAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\n",
        "@patch\n",
        "@delegates(Learner.fit_one_cycle)\n",
        "def encoder_fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n",
        "              pct_start=0.3, div=5.0, **kwargs):\n",
        "    \"Fine tuner to use with bt initial weights\"\n",
        "    \n",
        "    self.freeze() #freeze the resnet\n",
        "    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n",
        "    base_lr /= 2\n",
        "    #self.unfreeze() #don't unfreeze the resnet. We are fitting training the encoder head + projector\n",
        "    #self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\n",
        "    self.fit_one_cycle(epochs, slice(base_lr, base_lr), pct_start=pct_start, div=div, **kwargs)\n",
        "\n",
        "    self.unfreeze() #We can unfreeze at the end\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    initial_weights='bt_pretrain'\n",
        "    num_epochs=100\n",
        "    freeze_num_epochs=1\n",
        "    freeze_numfit=3\n",
        "    pretrain=True\n",
        "    main_dict = run_main_train(initial_weights,num_epochs=50,freeze_numfit,freeze_num_epochs,pretrain=pretrain,num=3)\n",
        "\n",
        "\n",
        "\n",
        "    print('Results for ensembling with bt weights, where we just trained the head')\n",
        "    bt_results = list(main_dict.values())\n",
        "    print([bt_results[i]['acc'] for i in range(len(bt_results))])\n",
        "    bt_results = list(combinations(bt_results,2)) #all pairs of results. So for num=3, will be 3\n",
        "    for v in bt_results:\n",
        "\n",
        "        print(f\"\\nAcc of first guy in ensemble is: {v[0]['acc']}\")\n",
        "        print(f\"Acc of second guy in ensemble is: {v[1]['acc']}\")\n",
        "        _,acc = predict_ensemble(yval=yval,scores1=v[0]['scores'],scores2=v[1]['scores'])\n",
        "        print(f'Acc of ensemble is:{acc}\\n')\n",
        "        \n"
      ],
      "metadata": {
        "id": "qZvejZRq1POY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A natural thing to try is to just plonk a projector on the end, train BT as usual (unfrozen encoder). Then plonk a random head on the end and fine tune as usual. The idea here is we will create some variation in the BT weights without destroying them: should increase ensemble performance slightly. Note that training just random heads with BT (as above) will DECREASE the variation in the heads: they have gone from random, to all trained on the same objective. \n"
      ],
      "metadata": {
        "id": "3tH--pmT7bD2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do this, we will need to edit everything again (gah!)\n",
        "\n",
        "Main points in edit in below cell(s): \n",
        "\n",
        "- create_model is as original: resnet + projector\n",
        "- We need to check bt_splitter and verify is working in cell below\n",
        "- We need to check the linear splitter as well...\n",
        "- We need to include encoder_fine_tune, to work as before.\n",
        "- fine_tune now needs a random encoder_head + linear layer"
      ],
      "metadata": {
        "id": "fpS6BVtk9dcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('We ran two experiments above: basline with head (no pretraining) and with BT pretraining just the head')"
      ],
      "metadata": {
        "id": "5TQfBJ8piosS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We don't want to run any of this stuff:"
      ],
      "metadata": {
        "id": "rgvtvdDQhP2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert False"
      ],
      "metadata": {
        "id": "Chc7-t3AhMlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\n",
        "\n",
        "def create_model(which_model,device,ps=8192,n_in=3):\n",
        "    print('inside create_model')\n",
        "\n",
        "    #pretrained=True if 'which_model' in ['bt_pretrain', 'supervised_pretrain'] else False\n",
        "\n",
        "    if which_model == 'bt_pretrain': model = torch.hub.load('facebookresearch/barlowtwins:main', 'resnet50')\n",
        "    \n",
        "    elif which_model == 'no_pretrain': model = resnet50()\n",
        "\n",
        "    elif which_model == 'supervised_pretrain': model = resnet50(weights='IMAGENET1K_V2')\n",
        "\n",
        "    #ignore the 'pretrained=False' argument here. Just means we use the weights above \n",
        "    #(which themselves are either pretrained or not)\n",
        "    encoder = get_resnet_encoder(model)\n",
        "    #encoder = HeadEncoder(encoder,device='cpu')\n",
        "\n",
        "    model = create_barlow_twins_model(encoder, hidden_size=ps,projection_size=ps,nlayers=3)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        model.cuda()\n",
        "        encoder.cuda()\n",
        "\n",
        "\n",
        "    return model,encoder\n",
        "\n",
        "class main_train:\n",
        "    \"\"\"Instantiate and (optionally) train the encoder. Then fine-tune the supervised model. \n",
        "    Outputs metrics on validation data\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dls_train, #used for training BT (if pretrain=True)\n",
        "                 dls_tune , #used for tuning\n",
        "                 dls_valid, #used to compute metrics / evaluate results. \n",
        "                 xval, #currently `predict_model` below assumes this is entire validation / test data\n",
        "                 yval,\n",
        "                 aug_pipelines, #the aug pipeline for self-supervised learning\n",
        "                 aug_pipelines_tune, #the aug pipeline for supervised learning\n",
        "                 aug_pipelines_test, #test (or valid) time augmentations \n",
        "                 initial_weights, #Which initial weights to use\n",
        "                 pretrain, #Whether to fit BT\n",
        "                 num_epochs, #number of BT fit epochs\n",
        "                 numfit, #number of tune_fit epochs\n",
        "                 freeze_num_epochs, #How many epochs to freeze body for when training BT\n",
        "                 freeze_numfit, #How many epochs to freeze body for when fine tuning\n",
        "                 ps=8192, #projection size\n",
        "                 n_in=3, #color channels\n",
        "                 indim=2048, #dimension output of encoder (2048 for resnet50)\n",
        "                 outdim=9, #number of classes\n",
        "                 print_report=False, #F1 metrics etc\n",
        "                 print_plot=False, #ROC curve\n",
        "                 ):\n",
        "        store_attr()\n",
        "        self.vocab = self.dls_valid.vocab\n",
        "        self.device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
        "\n",
        "                \n",
        "                 \n",
        "\n",
        "                 #Soon we might want to save some models here:\n",
        "\n",
        "                 #if self.model_type == 'res_proj': test_eq(self.fit_policy,'resnet_fine_tune') #I THINK this is only viable option?\n",
        "                 #self.encoder_path = f'/content/drive/My Drive/models/baselineencoder_initial_weights={self.initial_weights}_pretrain={self.pretrain}.pth'\n",
        "                 #self.tuned_model_path = f'/content/drive/My Drive/models/baselinefinetuned_initial_weights={self.initial_weights}_pretrain={self.pretrain}.pth'\n",
        "\n",
        "    @staticmethod\n",
        "    def fit(learn,fit_type,epochs,freeze_epochs,initial_weights):\n",
        "        \"\"\"We can patch in a modification, e.g. if we want subtype of fine_tune:supervised_pretrain to be different\n",
        "        to fine_tune:bt_pretrain\"\"\"\n",
        "\n",
        "        if fit_type == 'encoder_fine_tune': #i.e. barlow twins\n",
        "\n",
        "            learn.encoder_fine_tune(epochs,freeze_epochs=freeze_epochs) \n",
        "\n",
        "        elif fit_type == 'fine_tune':\n",
        "            \n",
        "            #elif initial_weights == 'supervised_pretrain':\n",
        "            learn.linear_fine_tune(epochs,freeze_epochs=freeze_epochs) \n",
        "\n",
        "        else: raise Exception('Fit policy not of expected form')\n",
        "\n",
        "    def train_encoder(self):\n",
        "        \"create encoder and (optionally, if pretrain=True) train with BT algorithm, according to fit_policy\"\n",
        "\n",
        "        try: #get existing encoder and plonk on new projector\n",
        "            encoder = self.encoder\n",
        "            encoder.cpu()\n",
        "            bt_model = create_barlow_twins_model(encoder, hidden_size=self.ps,projection_size=self.ps,nlayers=3)\n",
        "            bt_model.cuda()\n",
        "\n",
        "        except AttributeError: #otherwise, create\n",
        "            bt_model,encoder = create_model(which_model=self.initial_weights,ps=self.ps,device=self.device)\n",
        "\n",
        "        if self.pretrain: #train encoder according to fit policy\n",
        "\n",
        "            learn = Learner(self.dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(self.aug_pipelines,n_in=self.n_in,lmb=1/self.ps,print_augs=False)])\n",
        "            main_train.fit(learn,fit_type='encoder_fine_tune',\n",
        "                           epochs=self.num_epochs,freeze_epochs=self.freeze_num_epochs,\n",
        "                           initial_weights=self.initial_weights\n",
        "                          )\n",
        "            \n",
        "        self.encoder = bt_model.encoder\n",
        "\n",
        "    def fine_tune(self):\n",
        "        \"fine tune in supervised fashion, according to tune_fit_policy, and get metrics\"\n",
        "\n",
        "        #encoder = pickle.loads(pickle.dumps(self.encoder)) #We might want to pretrain once and fine tune several times (varying e.g. tune augs)\n",
        "\n",
        "        try: \n",
        "            encoder = self.encoder\n",
        "        \n",
        "        except AttributeError:\n",
        "            _,self.encoder = create_model(which_model=self.initial_weights,ps=self.ps,device=device)\n",
        "\n",
        "        #model = LM(self.encoder)\n",
        "        encoder = HeadEncoder(self.encoder,device='cuda') #resnet + nonlinear head\n",
        "        model = sequential(encoder,nn.Linear(2048,9)) #+ linear layer. \n",
        "        \n",
        "        learn = Learner(self.dls_tune,model,splitter=my_splitter,cbs = [LinearBt(aug_pipelines=self.aug_pipelines_tune,n_in=self.n_in)],wd=0.0)\n",
        "\n",
        "        #debugging\n",
        "        #learn = Learner(self.dls_tune,model,cbs = [LinearBt(aug_pipelines=self.aug_pipelines_tune,n_in=self.n_in)],wd=0.0)\n",
        "\n",
        "        main_train.fit(learn,fit_type='fine_tune',\n",
        "                       epochs=self.numfit,freeze_epochs=self.freeze_numfit,\n",
        "                       initial_weights=self.initial_weights\n",
        "                      ) #fine tuning (don't confuse this with fit policy!)\n",
        "        \n",
        "        #model.encoder=encoder\n",
        "        scores,preds, acc = predict_model(self.xval,self.yval,model=model,aug_pipelines_test=self.aug_pipelines_test,numavg=3)\n",
        "        #metrics dict will have f1 score, auc etc etc\n",
        "        metrics = classification_report_wrapper(preds, self.yval, self.vocab, print_report=self.print_report)\n",
        "        auc_dict = plot_roc(self.yval,preds,self.vocab,print_plot=self.print_plot)\n",
        "        metrics['acc'],metrics['auc_dict'],metrics['scores'],metrics['preds'],metrics['xval'],metrics['yval'] = acc,auc_dict,scores,preds,self.xval,self.yval\n",
        "  \n",
        "        #torch.save(model.state_dict(), self.tuned_model_path)\n",
        "        return metrics #\n",
        "\n",
        "    def __call__(self):\n",
        "\n",
        "        self.train_encoder() #train (or extract) the encoder\n",
        "        metrics = self.fine_tune()\n",
        "        \n",
        "        return metrics\n",
        "\n",
        "#The model is now resnet->nonlinear head -> linear layer\n",
        "def my_splitter(m):\n",
        "\n",
        "    return L(sequential(*m[0].resnet_encoder),sequential(m[0].head_encoder,m[1])).map(params)\n",
        "\n",
        "\n",
        "#The model is now just a resnet encoder + a projector\n",
        "def my_splitter_bt(m):\n",
        "\n",
        "    return L(sequential(*m.encoder),sequential(m.projector)).map(params)\n",
        "\n"
      ],
      "metadata": {
        "id": "hK50y0eF9hDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#| export\n",
        "\n",
        "@patch\n",
        "@delegates(Learner.fit_one_cycle)\n",
        "def encoder_fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n",
        "              pct_start=0.3, div=5.0, **kwargs):\n",
        "    \"Fine tuner to use with bt initial weights\"\n",
        "    \n",
        "    self.freeze() #freeze the resnet\n",
        "    print('froze resnet')\n",
        "    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n",
        "    base_lr /= 2\n",
        "    self.unfreeze() #Now we want to unfreeze the resnet!\n",
        "    print('unfroze resnet')\n",
        "    #self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\n",
        "    self.fit_one_cycle(epochs, slice(base_lr, base_lr), pct_start=pct_start, div=div, **kwargs)\n",
        "\n",
        "    self.unfreeze() #We can unfreeze at the end"
      ],
      "metadata": {
        "id": "qNwZQz-OCzNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test bt split\n",
        "bt_model,encoder = create_model(which_model='bt_pretrain',ps=8192,device=device)\n",
        "\n",
        "#| hide\n",
        "\n",
        "#test : manual. BT\n",
        "\n",
        "learn = Learner(dls_train,bt_model,splitter=my_splitter_bt,cbs=[BarlowTwins(aug_pipelines,n_in=3,lmb=1/8192,print_augs=False)])\n",
        "learn.freeze()\n",
        "print('resnet (frozen) + projector')\n",
        "learn.summary()"
      ],
      "metadata": {
        "id": "1QLc8OzFBoEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test linear split\n",
        "bt_model,encoder = create_model(which_model='bt_pretrain',ps=8192,device=device)\n",
        "encoder = HeadEncoder(encoder,device='cuda') #resnet + nonlinear head\n",
        "model = sequential(encoder,nn.Linear(2048,9)) #+ linear layer. \n",
        "model.cuda()\n",
        "\n",
        "learn = Learner(dls_tune,model,splitter=my_splitter,cbs = [LinearBt(aug_pipelines=aug_pipelines_tune,n_in=3)],wd=0.0)\n",
        "learn.freeze()\n",
        "print('resnet (frozen) + unfrozen head and linear layer')\n",
        "learn.summary()"
      ],
      "metadata": {
        "id": "YzMPF75WApT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go back over your checklist!"
      ],
      "metadata": {
        "id": "X9LMUOAQC-KW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Alright, now our thesis is that pretraining with BT, so long as we don't do it for too long and destroy the representations, will cause diversity and improve ensembling:\n",
        "\n",
        "(as an aside, we could add a callback/model that implements our ensembling idea IN PROJECTOR SPACE. So the idea is, basically, to push the projectors apart (while the resnet is frozen) as while as aligning it with the resnet, then just train as usual. Make sense!"
      ],
      "metadata": {
        "id": "kg_JPeCNEwxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_weights='bt_pretrain'\n",
        "num_epochs=10\n",
        "freeze_num_epochs=10\n",
        "freeze_numfit=3\n",
        "pretrain=True\n",
        "\n",
        "main_dict = run_main_train(initial_weights,num_epochs,freeze_numfit,freeze_num_epochs,pretrain=pretrain,num=3)"
      ],
      "metadata": {
        "id": "Kjc6vXQeBk_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "\n",
        "print('Results for ensembling with bt weights, where we trained the usual way (freeze resnet, then unfreeeze)')\n",
        "bt_results = list(main_dict.values())\n",
        "print([bt_results[i]['acc'] for i in range(len(bt_results))])\n",
        "bt_results = list(combinations(bt_results,2)) #all pairs of results. So for num=3, will be 3\n",
        "for v in bt_results:\n",
        "\n",
        "    print(f\"\\nAcc of first guy in ensemble is: {v[0]['acc']}\")\n",
        "    print(f\"Acc of second guy in ensemble is: {v[1]['acc']}\")\n",
        "    _,acc = predict_ensemble(yval=yval,scores1=v[0]['scores'],scores2=v[1]['scores'])\n",
        "    print(f'Acc of ensemble is:{acc}\\n')\n",
        "    "
      ],
      "metadata": {
        "id": "YLhQMXLVMDzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ok, didn't do much. Seems performance is (at least potentially) similar to before. So, a natural next thing to try is to just do the same experiment, but for longer. Also, it makes sense to use a lower base learning rate. Also, it would have been better to make things more extensible..."
      ],
      "metadata": {
        "id": "TpAgZgrMMnIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In this experiment we edited encoder_fine_tune (essentially lowered the learning rate), and trained for larger number of epochs.\n",
        "\n",
        "##Lesson: perhaps should have made main_train even more extensible: all hyperparameters it depends on should be passable (even as e.g. dictionaries). Anyway:"
      ],
      "metadata": {
        "id": "YhfSVmgWM5-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@patch\n",
        "@delegates(Learner.fit_one_cycle)\n",
        "def encoder_fine_tune(self:Learner, epochs, base_lr=1e-3, freeze_epochs=1, lr_mult=100,\n",
        "              pct_start=0.3, div=5.0, **kwargs):\n",
        "    \"Fine tuner to use with bt initial weights\"\n",
        "    \n",
        "    self.freeze() #freeze the resnet\n",
        "    print('froze resnet')\n",
        "    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n",
        "    base_lr /= 2\n",
        "    self.unfreeze() #Now we want to unfreeze the resnet!\n",
        "    print('unfroze resnet')\n",
        "    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\n",
        "    #self.fit_one_cycle(epochs, slice(base_lr, base_lr), pct_start=pct_start, div=div, **kwargs)\n",
        "\n",
        "    self.unfreeze() #We can unfreeze at the end\n",
        "\n",
        "\n",
        "initial_weights='bt_pretrain'\n",
        "num_epochs=50\n",
        "freeze_num_epochs=10\n",
        "freeze_numfit=3\n",
        "pretrain=True\n",
        "\n",
        "main_dict = run_main_train(initial_weights,num_epochs,freeze_numfit,freeze_num_epochs,pretrain=pretrain,num=3)\n",
        "\n",
        "\n",
        "from itertools import combinations\n",
        "\n",
        "print('Results for ensembling with bt weights, where we trained the usual way (freeze resnet, then unfreeeze)')\n",
        "bt_results = list(main_dict.values())\n",
        "print([bt_results[i]['acc'] for i in range(len(bt_results))])\n",
        "bt_results = list(combinations(bt_results,2)) #all pairs of results. So for num=3, will be 3\n",
        "for v in bt_results:\n",
        "\n",
        "    print(f\"\\nAcc of first guy in ensemble is: {v[0]['acc']}\")\n",
        "    print(f\"Acc of second guy in ensemble is: {v[1]['acc']}\")\n",
        "    _,acc = predict_ensemble(yval=yval,scores1=v[0]['scores'],scores2=v[1]['scores'])\n",
        "    print(f'Acc of ensemble is:{acc}\\n')\n",
        "    "
      ],
      "metadata": {
        "id": "7AWzPjBdMykB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An alternative approach could be to just train head with BT, but add a decorrelation penalty term: this really only probably makes sense if pretraining the head really does help (which we really think it should)."
      ],
      "metadata": {
        "id": "4fs_eePSaasz"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "premium",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b06ebefe34345d69c57763be586ca18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef4f87f3a87d41cabb9139e3ab3d7c01",
              "IPY_MODEL_a18742cfa8e44462833d74f3faf1142b",
              "IPY_MODEL_fd3150c258314091801a71dbf69cd086"
            ],
            "layout": "IPY_MODEL_55a4fd3d3f2c402e84b62fc9d9380f0d"
          }
        },
        "ef4f87f3a87d41cabb9139e3ab3d7c01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e60962ee47184bdf8136d9a0c94713c3",
            "placeholder": "​",
            "style": "IPY_MODEL_ef5b0fad74534d59b7959b417b2b2a9b",
            "value": "100%"
          }
        },
        "a18742cfa8e44462833d74f3faf1142b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f1cff7d79ee437980f0cad0357d3df7",
            "max": 94355933,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91f10e6cdaeb4ad18eeee8879e5ae9e6",
            "value": 94355933
          }
        },
        "fd3150c258314091801a71dbf69cd086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57b3a7cc04604bd291de227dcbc6f846",
            "placeholder": "​",
            "style": "IPY_MODEL_d00b9ce27702491990aa7b4df0ee8517",
            "value": " 90.0M/90.0M [00:01&lt;00:00, 60.0MB/s]"
          }
        },
        "55a4fd3d3f2c402e84b62fc9d9380f0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e60962ee47184bdf8136d9a0c94713c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef5b0fad74534d59b7959b417b2b2a9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f1cff7d79ee437980f0cad0357d3df7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91f10e6cdaeb4ad18eeee8879e5ae9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57b3a7cc04604bd291de227dcbc6f846": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d00b9ce27702491990aa7b4df0ee8517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}