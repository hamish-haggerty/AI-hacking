{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e41dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initially copied from BT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514cae59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastai\n",
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a893d809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamishhaggerty/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4abb754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from self_supervised.augmentations import *\n",
    "from self_supervised.layers import *\n",
    "import inspect\n",
    "\n",
    "#These are imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df738458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Base_Stein.SVGD_classes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40bd1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Like most other SSL algorithms BT's model consists of an encoder and projector (MLP) layer.\n",
    "#Definition is straightforward:\n",
    "#https://colab.research.google.com/github/KeremTurgutlu/self_supervised/blob/master/nbs/14%20-%20barlow_twins.ipynb#scrollTo=1M6QcUChcvpz\n",
    "\n",
    "class BarlowTwinsModel(Module):\n",
    "    \"\"\"An encoder followed by a projector\n",
    "    \"\"\"\n",
    "    def __init__(self,encoder,projector):self.encoder,self.projector = encoder,projector\n",
    "        \n",
    "    def forward(self,x): return self.projector(self.encoder(x))\n",
    "    \n",
    "    \n",
    "#Nothing much to this: Just a simple API for the BT model, with inputs encoder and projector. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f629c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOWEVER instead of directly using the above, by passing both an encoder and a projector, create_barlow_twins_model\n",
    "#function can be used by minimally passing a predefined encoder and the expected input channels.\n",
    "\n",
    "#In the paper it's mentioned that MLP layer consists of 3 layers... following function will create a 3 layer\n",
    "#MLP projector with batchnorm and ReLU by default. Alternatively, you can change bn and nlayers. \n",
    "\n",
    "\n",
    "#Questions: Why torch.no_grad() when doing this?\n",
    "def create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n",
    "    \"Create Barlow Twins model\"\n",
    "    n_in  = in_channels(encoder)\n",
    "    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n",
    "    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n",
    "    apply_init(projector)\n",
    "    return BarlowTwinsModel(encoder, projector)\n",
    "\n",
    "#Similar to above. Simple API to make the BT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f420c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can use self_supervised.layers module to create an encoder. \n",
    "\n",
    "encoder = create_encoder(\"tf_efficientnet_b0_ns\",n_in=3,pretrained=False,pool_type=PoolingType.CatAvgMax)\n",
    "model = create_barlow_twins_model(encoder,hidden_size=2048,projection_size=128,nlayers=2)\n",
    "out = model(torch.randn((2,3,224,224))); out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a158cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function self_supervised.augmentations.get_multi_aug_pipelines(n, size, rotate=True, jitter=True, bw=True, blur=True, resize_scale=(0.2, 1.0), resize_ratio=(0.75, 1.3333333333333333), rotate_deg=30, jitter_s=0.6, blur_s=(4, 32), same_on_batch=False, flip_p=0.5, rotate_p=0.3, jitter_p=0.3, bw_p=0.3, blur_p=0.3, stats=([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), cuda=False, xtra_tfms=[])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_multi_aug_pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37acc15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BarlowTwins Callback\n",
    "#The following parameters can be passed:\n",
    "# - aug_pipelines\n",
    "# Imb lambda is the weight for redundancy reduction term in the loss function\n",
    "\n",
    "@delegates(get_multi_aug_pipelines)\n",
    "def get_barlow_twins_aug_pipelines(size,**kwargs): return get_multi_aug_pipelines(n=2,size=size,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "126b5cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(size, rotate=True, jitter=True, bw=True, blur=True, resize_scale=(0.2, 1.0), resize_ratio=(0.75, 1.3333333333333333), rotate_deg=30, jitter_s=0.6, blur_s=(4, 32), same_on_batch=False, flip_p=0.5, rotate_p=0.3, jitter_p=0.3, bw_p=0.3, blur_p=0.3, stats=([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), cuda=False, xtra_tfms=[])\n"
     ]
    }
   ],
   "source": [
    "aug_pipelines = get_barlow_twins_aug_pipelines(size=28,rotate=False,jitter=False,bw=False,blur=False,stats=None,cuda=False)\n",
    "aug_pipelines\n",
    "\n",
    "print(inspect.signature(get_barlow_twins_aug_pipelines)) #If we comment out @delegates above, then only size is printed\n",
    "                                                         #here; i.e. just prints **kwargs instead of all the actual \n",
    "                                                         #keyword arguments!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61abfd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BarlowTwins(Callback):\n",
    "    order,run_valid = 9,True\n",
    "    def __init__(self, aug_pipelines, lmb=5e-3, print_augs=False):\n",
    "        assert_aug_pipelines(aug_pipelines)\n",
    "        self.aug1, self.aug2 = aug_pipelines\n",
    "        if print_augs: print(self.aug1), print(self.aug2)\n",
    "        store_attr('lmb')\n",
    "        \n",
    "        \n",
    "    def before_fit(self): \n",
    "        self.learn.loss_func = self.lf\n",
    "        nf = self.learn.model.projector[-1].out_features\n",
    "        self.I = torch.eye(nf).to(self.dls.device)\n",
    "                    \n",
    "            \n",
    "    def before_batch(self):\n",
    "        xi,xj = self.aug1(self.x), self.aug2(self.x)\n",
    "        self.learn.xb = (torch.cat([xi, xj]),)\n",
    "        \n",
    "    \n",
    "    #Check BT.pynb for original lf. Here we make our edits to try out SBT\n",
    "    def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n",
    "        bs,nf = pred.size(0)//2,pred.size(1)\n",
    "            \n",
    "        z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n",
    "        \n",
    "        z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n",
    "        z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n",
    "        \n",
    "        OffDiagLoss=0\n",
    "        for i in range(nf):\n",
    "            for j in range(nf):\n",
    "                if j==i:\n",
    "                    continue\n",
    "\n",
    "                Particles = torch.stack((z1norm[:,i],z2norm[:,j])).T #set of initial particles for SVGD\n",
    "                svgd = trainable_SVGD(N=bs,zdim=2,eta=0.01) #N particles and 2 dimensional\n",
    "                \n",
    "                svgd.Particles = Particles.detach()\n",
    "                \n",
    "                #put inner loop here if doing more than one SVGD step...\n",
    "                g = svgd.get_gradient() # \"Like forward\"\n",
    "                svgd.AdamStep(gradient=g) #\"Like step\"\n",
    "                \n",
    "                #QUESTION: Does this do what I think it does...\n",
    "                OffDiagLoss += (Particles-svgd.Particles).pow(2).sum()\n",
    "\n",
    "        C = (z1norm.T @ z2norm) / bs \n",
    "        cdiff = (C - self.I)**2\n",
    "        #loss = (cdiff*self.I + cdiff*(1-self.I)*self.lmb).sum() \n",
    "        loss = (cdiff*self.I).sum() + self.lmb*OffDiagLoss\n",
    "\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def show(self, n=1):\n",
    "        bs = self.learn.x.size(0)//2\n",
    "        x1,x2  = self.learn.x[:bs], self.learn.x[bs:] \n",
    "        idxs = np.random.choice(range(bs),n,False)\n",
    "        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n",
    "        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n",
    "        images = []\n",
    "        for i in range(n): images += [x1[i],x2[i]] \n",
    "        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "500d4e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.089093</td>\n",
       "      <td>0.085310</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.079891</td>\n",
       "      <td>0.075348</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.063264</td>\n",
       "      <td>0.071951</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.067533</td>\n",
       "      <td>0.177168</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.072721</td>\n",
       "      <td>0.107661</td>\n",
       "      <td>00:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.072457</td>\n",
       "      <td>0.080421</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.072219</td>\n",
       "      <td>0.094365</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.069305</td>\n",
       "      <td>0.111430</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.067954</td>\n",
       "      <td>0.095800</td>\n",
       "      <td>00:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.064716</td>\n",
       "      <td>0.105506</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
    "learn.fit(10) #delete this cell later\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ae54f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=64\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "items = get_image_files(path)\n",
    "tds = Datasets(items, [PILImageBW.create, [parent_label, Categorize()]], splits=GrandparentSplitter()(items))\n",
    "dls = tds.dataloaders(bs=bs, after_item=[ToTensor(), IntToFloatTensor()], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a17ce95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function self_supervised.layers.create_encoder(arch: str, pretrained=True, n_in=3, pool_type='catavgmax')>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76cd6de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n",
      "Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x864 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Let's build a reasonable composite augmentation: initially copy pasted from aug_pipe_exploration\n",
    "\n",
    "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
    "model = create_barlow_twins_model(fastai_encoder, hidden_size=1024, projection_size=1024)\n",
    "#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n",
    "#values for these which is tantamount to doing nothing\n",
    "\n",
    "#So if we choose resize_scale=(1,1) then the images look the same.\n",
    "#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\n",
    "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=False)\n",
    "\n",
    "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n",
    "b = dls.one_batch()\n",
    "learn._split(b)\n",
    "learn('before_batch')\n",
    "axes = learn.barlow_twins.show(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8250ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.recorder.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d893cfa3",
   "metadata": {},
   "source": [
    "So, in the above we train Barlow Twins on `MNIST_TINY`. \n",
    "\n",
    "Next step: Figure out what exactly we are training on. Then we want to \"fine tune\" on validation set and examine \n",
    "performance. \n",
    "\n",
    "Ok, for now let's just train as usual. We WILL improve this later. Iterate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50e24aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaba7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, train a classifier on the embedding!\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,zdim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(zdim,2) #As only 2 classes in TINY_MNIST\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = cast(self.fc1(x),Tensor) #so we have to use cross entropy loss. cast is because using old version fastai \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53c5de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=128\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "items = get_image_files(path)\n",
    "tds = Datasets(items, [PILImageBW.create, [parent_label, Categorize()]], splits=GrandparentSplitter()(items))\n",
    "dls = tds.dataloaders(bs=bs,num_workers=0, after_item=[ToTensor(), IntToFloatTensor()], device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2b4b2428",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [164]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m fastai_encoder \u001b[38;5;241m=\u001b[39m create_encoder(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxresnet18\u001b[39m\u001b[38;5;124m'\u001b[39m, n_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m create_barlow_twins_model(fastai_encoder, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, projection_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Use aug pipeline from above\u001b[39;00m\n\u001b[1;32m      5\u001b[0m aug_pipelines \u001b[38;5;241m=\u001b[39m get_barlow_twins_aug_pipelines(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m28\u001b[39m, rotate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,flip_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,resize_scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.7\u001b[39m,\u001b[38;5;241m1\u001b[39m), jitter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, bw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,blur\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,blur_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,blur_s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/ipykernel/kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1074\u001b[0m     )\n\u001b[0;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/old_fastai/lib/python3.10/site-packages/ipykernel/kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "fastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\n",
    "model = create_barlow_twins_model(fastai_encoder, hidden_size=100, projection_size=10)\n",
    "#Use aug pipeline from above\n",
    "aug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=False)\n",
    "learn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])#,ShortEpochCallback(0.001)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d374c6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls.valid.bs = len(list(iter(dls.valid_ds))) #Set the validation dataloader batch size to be the length of the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "97bc22ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.810843</td>\n",
       "      <td>1.508676</td>\n",
       "      <td>00:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.079975</td>\n",
       "      <td>0.270545</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.437366</td>\n",
       "      <td>0.373931</td>\n",
       "      <td>00:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.070069</td>\n",
       "      <td>0.245965</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.846498</td>\n",
       "      <td>0.270651</td>\n",
       "      <td>00:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.691370</td>\n",
       "      <td>0.350234</td>\n",
       "      <td>00:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.573768</td>\n",
       "      <td>0.155162</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.492096</td>\n",
       "      <td>0.265692</td>\n",
       "      <td>00:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.425368</td>\n",
       "      <td>0.124471</td>\n",
       "      <td>00:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.372595</td>\n",
       "      <td>0.155469</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "learn.fit(10)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "eb3bb43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorCategory(0.1293, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0836, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1042, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0839, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0964, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1063, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1432, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0773, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0850, grad_fn=<AliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Train Classifier on encoder(tiny_mnist)\n",
    "\n",
    "zdim=1024 #see above\n",
    "head = LinearClassifier(zdim=zdim)\n",
    "optimizer = torch.optim.Adam(head.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "EPOCHS=100\n",
    "for epoch in range(EPOCHS):\n",
    "    #break \n",
    "    b = dls.train.one_batch() #Seems need dls[0] or dls.train for training ... dls[1] is validation see here https://docs.fast.ai/data.core.html#DataLoaders.__getitem__\n",
    "    x,y = b[0],b[1]\n",
    "\n",
    "    loss = criterion(head(fastai_encoder(x)),y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch>90:\n",
    "        print(loss)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "dde7d022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9756795167922974\n"
     ]
    }
   ],
   "source": [
    "#Test result of above cell on the validation set - assumes that batch size of valid-dataloader is = number of valid samples                                        \n",
    "b = dls.valid.one_batch()\n",
    "x,y = b[0],b[1]\n",
    "ypred = head(fastai_encoder(x))\n",
    "correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n",
    "print(correct.mean().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "79237ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorCategory(0.0962, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1417, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0836, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1354, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0906, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.0815, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1050, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1309, grad_fn=<AliasBackward0>)\n",
      "TensorCategory(0.1284, grad_fn=<AliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Just train a linear classifier (no encoder)\n",
    "#Basically cell above but remove encoder and some re-shaping\n",
    "\n",
    "zdim=28*28 #see above\n",
    "head = LinearClassifier(zdim=zdim)\n",
    "optimizer = torch.optim.Adam(head.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "EPOCHS=100\n",
    "for epoch in range(EPOCHS):\n",
    "    #break\n",
    "    b = dls.train.one_batch() #see here https://docs.fast.ai/data.core.html#DataLoaders.__getitem__\n",
    "    x,y = b[0],b[1]\n",
    "\n",
    "    x=x.view(bs,zdim)\n",
    "    x=cast(x, Tensor) #Have to do this when using old version of fastai for some reason...\n",
    "\n",
    "    out = head(x)\n",
    "    loss = criterion(out,y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch>90:\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f7bd5db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9828326106071472\n"
     ]
    }
   ],
   "source": [
    "#Test result of above cell, (i.e. just a linear classifier), on the validation set - assumes that batch size of valid-dataloader is = number of valid samples                                        \n",
    "b = dls.valid.one_batch()\n",
    "x,y = b[0],b[1]\n",
    "x = x.view(-1,zdim)\n",
    "ypred = head(x)\n",
    "\n",
    "correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n",
    "print(correct.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402215e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
