{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch==1.11.0 torchvision==0.12.0 torchaudio==0.11.0\n!pip install fastai==2.6.3 --no-deps\n!pip install self_supervised","metadata":{"execution":{"iopub.status.busy":"2022-09-14T11:47:22.717449Z","iopub.execute_input":"2022-09-14T11:47:22.718486Z","iopub.status.idle":"2022-09-14T11:47:51.654431Z","shell.execute_reply.started":"2022-09-14T11:47:22.718376Z","shell.execute_reply":"2022-09-14T11:47:51.653290Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch==1.11.0 in /opt/conda/lib/python3.7/site-packages (1.11.0+cpu)\nRequirement already satisfied: torchvision==0.12.0 in /opt/conda/lib/python3.7/site-packages (0.12.0+cpu)\nRequirement already satisfied: torchaudio==0.11.0 in /opt/conda/lib/python3.7/site-packages (0.11.0+cpu)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.11.0) (4.1.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.12.0) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision==0.12.0) (2.27.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.12.0) (9.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0) (1.26.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0) (2022.6.15)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision==0.12.0) (2.0.12)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting fastai==2.6.3\n  Downloading fastai-2.6.3-py3-none-any.whl (197 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.9/197.9 kB\u001b[0m \u001b[31m826.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fastai\n  Attempting uninstall: fastai\n    Found existing installation: fastai 2.7.4\n    Uninstalling fastai-2.7.4:\n      Successfully uninstalled fastai-2.7.4\nSuccessfully installed fastai-2.6.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting self_supervised\n  Downloading self_supervised-1.0.4-py3-none-any.whl (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m348.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from self_supervised) (22.1.1)\nRequirement already satisfied: fastai>=2.2.7 in /opt/conda/lib/python3.7/site-packages (from self_supervised) (2.6.3)\nRequirement already satisfied: kornia>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from self_supervised) (0.5.8)\nCollecting timm>=0.4.5\n  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from self_supervised) (21.3)\nRequirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (1.0.2)\nRequirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (0.12.0+cpu)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (3.5.2)\nRequirement already satisfied: torch<1.12,>=1.7.0 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (1.11.0+cpu)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (6.0)\nRequirement already satisfied: fastdownload<2,>=0.0.5 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (0.0.6)\nRequirement already satisfied: fastcore<1.5,>=1.3.27 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (1.4.5)\nRequirement already satisfied: spacy<4 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (2.3.7)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (1.7.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (2.27.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (1.3.5)\nRequirement already satisfied: pillow>6.0.0 in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (9.1.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from fastai>=2.2.7->self_supervised) (1.0.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->self_supervised) (3.0.9)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.9.1)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (4.64.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (3.0.6)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (59.8.0)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.5)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (2.0.6)\nRequirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.21.6)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.0.7)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (1.1.3)\nRequirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (7.4.5)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai>=2.2.7->self_supervised) (0.7.8)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->fastai>=2.2.7->self_supervised) (2.0.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fastai>=2.2.7->self_supervised) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fastai>=2.2.7->self_supervised) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fastai>=2.2.7->self_supervised) (2022.6.15)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch<1.12,>=1.7.0->fastai>=2.2.7->self_supervised) (4.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai>=2.2.7->self_supervised) (2.8.2)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai>=2.2.7->self_supervised) (1.4.2)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai>=2.2.7->self_supervised) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai>=2.2.7->self_supervised) (4.33.3)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->fastai>=2.2.7->self_supervised) (2022.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->fastai>=2.2.7->self_supervised) (1.1.0)\nRequirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai>=2.2.7->self_supervised) (4.12.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->fastai>=2.2.7->self_supervised) (1.16.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai>=2.2.7->self_supervised) (3.8.0)\nInstalling collected packages: timm, self_supervised\nSuccessfully installed self_supervised-1.0.4 timm-0.6.7\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import fastai\nimport self_supervised\nimport torch\nassert(fastai.__version__ == '2.6.3') #Check that version is 2.6.3","metadata":{"execution":{"iopub.status.busy":"2022-09-14T11:47:51.657162Z","iopub.execute_input":"2022-09-14T11:47:51.657530Z","iopub.status.idle":"2022-09-14T11:47:53.294234Z","shell.execute_reply.started":"2022-09-14T11:47:51.657493Z","shell.execute_reply":"2022-09-14T11:47:53.293257Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from fastai.vision.all import *\nfrom self_supervised.augmentations import *\nfrom self_supervised.layers import *\nimport inspect\nimport warnings\nimport random\nimport math\nwarnings.filterwarnings(\"ignore\")\n#from Base_Stein.SVGD_classes import *","metadata":{"execution":{"iopub.status.busy":"2022-09-14T11:47:53.297374Z","iopub.execute_input":"2022-09-14T11:47:53.297991Z","iopub.status.idle":"2022-09-14T11:47:55.349139Z","shell.execute_reply.started":"2022-09-14T11:47:53.297956Z","shell.execute_reply":"2022-09-14T11:47:55.347759Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Like most other SSL algorithms BT's model consists of an encoder and projector (MLP) layer.\n#Definition is straightforward:\n#https://colab.research.google.com/github/KeremTurgutlu/self_supervised/blob/master/nbs/14%20-%20barlow_twins.ipynb#scrollTo=1M6QcUChcvpz\nclass BarlowTwinsModel(Module):\n    \"\"\"An encoder followed by a projector\n    \"\"\"\n    def __init__(self,encoder,projector):self.encoder,self.projector = encoder,projector\n        \n    def forward(self,x): return self.projector(self.encoder(x))","metadata":{"execution":{"iopub.status.busy":"2022-09-14T11:47:55.351481Z","iopub.execute_input":"2022-09-14T11:47:55.352702Z","iopub.status.idle":"2022-09-14T11:47:55.359173Z","shell.execute_reply.started":"2022-09-14T11:47:55.352664Z","shell.execute_reply":"2022-09-14T11:47:55.357602Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#HOWEVER instead of directly using the above, by passing both an encoder and a projector, create_barlow_twins_model\n#function can be used by minimally passing a predefined encoder and the expected input channels.\n\n#In the paper it's mentioned that MLP layer consists of 3 layers... following function will create a 3 layer\n#MLP projector with batchnorm and ReLU by default. Alternatively, you can change bn and nlayers. \n\n#Questions: Why torch.no_grad() when doing this?\ndef create_barlow_twins_model(encoder, hidden_size=256, projection_size=128, bn=True, nlayers=3):\n    \"Create Barlow Twins model\"\n    n_in  = in_channels(encoder)\n    with torch.no_grad(): representation = encoder(torch.randn((2,n_in,128,128)))\n    projector = create_mlp_module(representation.size(1), hidden_size, projection_size, bn=bn, nlayers=nlayers) \n    apply_init(projector)\n    return BarlowTwinsModel(encoder, projector)\n\n#Similar to above. Simple API to make the BT model:","metadata":{"execution":{"iopub.status.busy":"2022-09-14T11:47:57.610951Z","iopub.execute_input":"2022-09-14T11:47:57.611394Z","iopub.status.idle":"2022-09-14T11:47:57.619541Z","shell.execute_reply.started":"2022-09-14T11:47:57.611353Z","shell.execute_reply":"2022-09-14T11:47:57.618302Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#BarlowTwins Callback\n#The following parameters can be passed:\n# - aug_pipelines\n# Imb lambda is the weight for redundancy reduction term in the loss function\n\n@delegates(get_multi_aug_pipelines)\ndef get_barlow_twins_aug_pipelines(size,**kwargs): return get_multi_aug_pipelines(n=2,size=size,**kwargs)","metadata":{"execution":{"iopub.status.busy":"2022-09-14T11:48:00.198485Z","iopub.execute_input":"2022-09-14T11:48:00.199411Z","iopub.status.idle":"2022-09-14T11:48:00.206880Z","shell.execute_reply.started":"2022-09-14T11:48:00.199360Z","shell.execute_reply":"2022-09-14T11:48:00.205091Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Uniform random number between a and b\ndef Unif(a,b):\n    return (b-a)*torch.rand(1).item()+a","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Some random polynomial hacking\n\ndef random_polynomial(A):\n    \n    B=torch.normal(mean=0, std=0.025, size=(1, 1)).item() #First Horner term (and is coefficient of x^4)\n    \n    Btem = torch.normal(mean=0,std=0.05, size=(1, 1)).item() #Sample coefficient of x^3\n    B = Btem + B*A #Third Horner term\n    \n    Btem = torch.normal(mean=0,std=0.5, size=(1, 1)).item() #Sample coefficient of x^2\n    B = Btem + B*A #Third Horner term\n    \n    Btem = random.choice([-1,1])*torch.normal(mean=1,std=2, size=(1, 1)).item() #Sample coefficient of x\n    B = Btem + B*A #Fourth Horner term\n    \n    Btem = torch.normal(mean=0,std=1, size=(1, 1)).item() #Sample coefficient of x^0\n    B = Btem + B*A #Fifth Horner term\n    \n    \n    return B","metadata":{"execution":{"iopub.status.busy":"2022-09-14T11:48:03.869811Z","iopub.execute_input":"2022-09-14T11:48:03.870368Z","iopub.status.idle":"2022-09-14T11:48:03.882754Z","shell.execute_reply.started":"2022-09-14T11:48:03.870317Z","shell.execute_reply":"2022-09-14T11:48:03.881252Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def random_quintic(A):\n\n    \n    B=torch.normal(mean=0, std=0.125, size=(1, 1)).item() #First Horner term (and is coefficient of x^4)\n    \n    Btem=torch.normal(mean=0, std=0.25, size=(1, 1)).item()#Sample coefficient of x^3\n    B = Btem + B*A #Second Horner term\n    \n    Btem = torch.normal(mean=0,std=0.5, size=(1, 1)).item() #Sample coefficient of x^2\n    B = Btem + B*A #Third Horner term\n    \n    Btem = random.choice([-1,1])*torch.normal(mean=1,std=2, size=(1, 1)).item() #Sample coefficient of x\n    B = Btem + B*A #Fourth Horner term\n    \n    Btem = torch.normal(mean=0,std=1, size=(1, 1)).item() #Sample coefficient of x^0\n    B = Btem + B*A #Fifth Horner term\n    \n    \n    return B","metadata":{"execution":{"iopub.status.busy":"2022-09-14T05:43:35.688179Z","iopub.execute_input":"2022-09-14T05:43:35.688649Z","iopub.status.idle":"2022-09-14T05:43:35.703620Z","shell.execute_reply.started":"2022-09-14T05:43:35.688609Z","shell.execute_reply":"2022-09-14T05:43:35.702413Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def random_sinusoid(x,std=2.0):\n    \n    t=torch.normal(mean=1,std=std,size=(1,1)).item()\n    s=torch.normal(mean=1,std=std,size=(1,1)).item()\n    \n    u=torch.normal(mean=0,std=std,size=(1,1)).item()\n    v=torch.normal(mean=0,std=std,size=(1,1)).item()\n    \n    a=torch.normal(mean=0,std=2,size=(1,1)).item()\n    b=torch.normal(mean=0,std=2,size=(1,1)).item()\n    \n    return a*torch.sin(t*x+s) + b*torch.cos(u*x + v)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def poly_sinusoid(x):\n    \n    return 0.7*(random_polynomial(0.7*x) + 0.3*random_sinusoid(x,std=(1/1.71)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#export\nclass BarlowTwins(Callback):\n    order,run_valid = 9,True\n    def __init__(self, aug_pipelines, lmb=5e-3, print_augs=False):\n        assert_aug_pipelines(aug_pipelines)\n        self.aug1, self.aug2 = aug_pipelines\n        if print_augs: print(self.aug1), print(self.aug2)\n        store_attr('lmb')\n        \n    def before_fit(self): \n        self.learn.loss_func = self.lf\n        nf = self.learn.model.projector[-1].out_features\n        self.I = torch.eye(nf).to(self.dls.device)\n            \n    def before_batch(self):\n        xi,xj = self.aug1(self.x), self.aug2(self.x)\n        self.learn.xb = (torch.cat([xi, xj]),)\n        \n        #Uncomment to run standard BT\n    \n#     def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n#         bs,nf = pred.size(0)//2,pred.size(1)\n\n#         z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n\n#         z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n#         z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n        \n#         C = (z1norm.T @ z2norm) / bs \n#         cdiff = (C - self.I)**2\n#         loss = (cdiff*self.I + cdiff*(1-self.I)*self.lmb).sum() \n#         return loss\n\n\n    def lf(self, pred, *yb): #pred is (bs+bs)*projection_size\n        bs,nf = pred.size(0)//2,pred.size(1)\n\n        #All standard, from BT\n        z1, z2 = pred[:bs],pred[bs:] #so z1 is bs*projection_size, likewise for z2\n        z1norm = (z1 - z1.mean(0)) / z1.std(0, unbiased=False)\n        z2norm = (z2 - z2.mean(0)) / z2.std(0, unbiased=False)\n        \n        \n        C = (z1norm.T @ z2norm) / bs \n        cdiff = (C - self.I)**2\n    \n        polyprob=1\n        #polyprob=0.5\n        temrand = random.random()\n        if temrand < polyprob: #With some probability we want off diag terms to be (quadratic) say.\n\n            z1norm_2 = random_polynomial(z1norm)\n            z2norm_2 = z2norm\n                \n            C_2 = (z1norm_2.T @ z2norm_2) / bs\n            \n            cdiff_2 = (C_2)**2 #don't need to subtract I as only looking at off diag terms\n            \n        else:\n            cdiff_2 = cdiff\n            \n        l2 = cdiff_2*(1-self.I)*self.lmb #Is either the standard term - or not.\n\n        loss = (cdiff*self.I + l2).sum() \n        return loss\n\n    \n    @torch.no_grad()\n    def show(self, n=1):\n        bs = self.learn.x.size(0)//2\n        x1,x2  = self.learn.x[:bs], self.learn.x[bs:] \n        idxs = np.random.choice(range(bs),n,False)\n        x1 = self.aug1.decode(x1[idxs].to('cpu').clone()).clamp(0,1)\n        x2 = self.aug2.decode(x2[idxs].to('cpu').clone()).clamp(0,1)\n        images = []\n        for i in range(n): images += [x1[i],x2[i]] \n        return show_batch(x1[0], None, images, max_n=len(images), nrows=n)","metadata":{"execution":{"iopub.status.busy":"2022-09-14T13:30:30.657007Z","iopub.execute_input":"2022-09-14T13:30:30.657739Z","iopub.status.idle":"2022-09-14T13:30:30.675927Z","shell.execute_reply.started":"2022-09-14T13:30:30.657695Z","shell.execute_reply":"2022-09-14T13:30:30.674711Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"#Debugging cell - delete later (similar to cell below)\nps=500\nhs=500\nfastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\nmodel = create_barlow_twins_model(fastai_encoder, hidden_size=hs,projection_size=ps)# projection_size=1024)\n#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n#values for these which is tantamount to doing nothing\n#So if we choose resize_scale=(1,1) then the images look the same.\n#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\naug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=True)\n#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\nlearn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\nlearn.fit(100)#300          ","metadata":{"execution":{"iopub.status.busy":"2022-09-14T13:30:36.045843Z","iopub.execute_input":"2022-09-14T13:30:36.046271Z","iopub.status.idle":"2022-09-14T13:43:27.854796Z","shell.execute_reply.started":"2022-09-14T13:30:36.046210Z","shell.execute_reply":"2022-09-14T13:43:27.853379Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\nPipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>669.233032</td>\n      <td>3529.542480</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>560.833191</td>\n      <td>408.078735</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>493.887634</td>\n      <td>1558.884033</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>604.239075</td>\n      <td>66.614578</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>549.479797</td>\n      <td>3501.609863</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>513.572449</td>\n      <td>2062.845703</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>493.602539</td>\n      <td>851.089600</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>439.548828</td>\n      <td>553.963135</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>419.945404</td>\n      <td>73.202278</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>419.831268</td>\n      <td>399.272644</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>404.073273</td>\n      <td>3595.108887</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>406.685699</td>\n      <td>335.677429</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>441.555573</td>\n      <td>1181.977295</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>451.771698</td>\n      <td>632.795776</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>460.346863</td>\n      <td>311.840759</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>449.209015</td>\n      <td>315.667358</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>437.816071</td>\n      <td>2095.645508</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>442.105133</td>\n      <td>2155.185303</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>425.535034</td>\n      <td>315.979126</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>413.741791</td>\n      <td>172.486908</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>404.452209</td>\n      <td>229.060791</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>399.357391</td>\n      <td>49.339943</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>387.302551</td>\n      <td>278.964417</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>374.884247</td>\n      <td>497.872406</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>364.093567</td>\n      <td>359.918610</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>357.615265</td>\n      <td>1112.467041</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>407.537933</td>\n      <td>79.744949</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>400.739624</td>\n      <td>1872.095215</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>385.038147</td>\n      <td>1322.559814</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>374.977783</td>\n      <td>174.529007</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>380.239319</td>\n      <td>1398.310913</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>383.093842</td>\n      <td>186.226746</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>376.953705</td>\n      <td>345.726685</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>365.553589</td>\n      <td>446.640015</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>371.724213</td>\n      <td>263.841248</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>360.773285</td>\n      <td>223.032288</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>351.160889</td>\n      <td>1340.438843</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>346.415955</td>\n      <td>69.138550</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>340.545563</td>\n      <td>359.271698</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>332.413330</td>\n      <td>371.504761</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>322.243042</td>\n      <td>176.084488</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>314.885101</td>\n      <td>106.428673</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>320.203827</td>\n      <td>289.009460</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>311.358704</td>\n      <td>335.616638</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>306.034485</td>\n      <td>184.407242</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>317.930878</td>\n      <td>668.440125</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>310.818207</td>\n      <td>92.784027</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>306.057526</td>\n      <td>374.083008</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>299.232452</td>\n      <td>540.115234</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>295.053680</td>\n      <td>199.619995</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>289.314636</td>\n      <td>135.513397</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>306.508148</td>\n      <td>159.323853</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>300.945374</td>\n      <td>156.125488</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>300.598450</td>\n      <td>278.004761</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>294.232880</td>\n      <td>218.901398</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>290.647888</td>\n      <td>287.638092</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>292.670532</td>\n      <td>253.539413</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>290.272430</td>\n      <td>179.251190</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>285.186859</td>\n      <td>147.687607</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>280.817474</td>\n      <td>162.229370</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>277.958527</td>\n      <td>95.797806</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>273.435486</td>\n      <td>594.839722</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>279.394440</td>\n      <td>219.881500</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>275.345947</td>\n      <td>119.079330</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>270.256042</td>\n      <td>117.387512</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>266.746094</td>\n      <td>967.001953</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>278.760345</td>\n      <td>276.412537</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>297.882965</td>\n      <td>256.680725</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>292.637604</td>\n      <td>232.873093</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>288.845062</td>\n      <td>1173.149414</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>284.283020</td>\n      <td>123.252937</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>279.502716</td>\n      <td>694.312866</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>285.987946</td>\n      <td>299.625854</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>281.871490</td>\n      <td>364.687805</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>277.283142</td>\n      <td>99.461502</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>273.065125</td>\n      <td>144.322815</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>271.274536</td>\n      <td>249.147537</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>270.891907</td>\n      <td>194.599762</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>266.072418</td>\n      <td>208.462708</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>261.071320</td>\n      <td>758.546387</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>261.994873</td>\n      <td>168.242889</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>257.701813</td>\n      <td>214.642792</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>257.289581</td>\n      <td>81.480438</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>261.309509</td>\n      <td>134.429382</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>260.935150</td>\n      <td>544.245789</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>256.838135</td>\n      <td>272.246155</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>253.105789</td>\n      <td>840.436768</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>251.711990</td>\n      <td>499.939789</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>248.509201</td>\n      <td>211.211960</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>89</td>\n      <td>245.184769</td>\n      <td>140.332977</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>242.482574</td>\n      <td>91.930222</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>91</td>\n      <td>240.934937</td>\n      <td>152.729797</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>92</td>\n      <td>239.538437</td>\n      <td>192.729614</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>93</td>\n      <td>246.322586</td>\n      <td>180.759979</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>243.495926</td>\n      <td>71.894165</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>239.464249</td>\n      <td>77.181435</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>259.620209</td>\n      <td>246.196671</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>254.504349</td>\n      <td>91.373711</td>\n      <td>00:07</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>249.868835</td>\n      <td>165.651871</td>\n      <td>00:08</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>251.206848</td>\n      <td>161.101593</td>\n      <td>00:08</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"cell_type":"code","source":"#Get the dataloader and set batch size \nts=512 #training set size\nbs=256 \ndevice='cpu'\npath = untar_data(URLs.MNIST)\nitems = get_image_files(path/'training') #i.e. NOT testing!!!\nitems=items.shuffle()\n\nitems1 = items[0:ts]\nsplit = RandomSplitter(valid_pct=0.5) #randomly split training set into training and validation\n#tds = Datasets(items,splits=split(items)) #Do we want this?\ntds = Datasets(items1, [PILImageBW.create, [parent_label, Categorize()]], splits=split(items1)) #Or do we want this?\ndls = tds.dataloaders(bs=bs, after_item=[ToTensor(), IntToFloatTensor()], device=device)\n\n#Evaluate linear classifier on this guy\nitems2 = items[ts:]\nsplit = RandomSplitter(valid_pct=0.99) #randomly split training set into training and validation\ntds_new = Datasets(items2, [PILImageBW.create, [parent_label, Categorize()]], splits=split(items2)) #Or do we want this?\ndls_new = tds_new.dataloaders(bs=bs, after_item=[ToTensor(), IntToFloatTensor()], device=device)","metadata":{"execution":{"iopub.status.busy":"2022-09-14T13:11:46.534691Z","iopub.execute_input":"2022-09-14T13:11:46.535523Z","iopub.status.idle":"2022-09-14T13:11:47.813075Z","shell.execute_reply.started":"2022-09-14T13:11:46.535484Z","shell.execute_reply":"2022-09-14T13:11:47.811767Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#A \"reasonable\" composite augmentation: initially copy pasted BT. We run this cell a few times to check it makes sense\n#Also define encoder and model\nfastai_encoder = create_encoder('xresnet18', n_in=1, pretrained=False)\nmodel = create_barlow_twins_model(fastai_encoder, hidden_size=10,projection_size=10)# projection_size=1024)\n#So aside from size, randomresizedcrop takes in two args: resize_scale and resize_ratio. So we want to put in \n#values for these which is tantamount to doing nothing\n#So if we choose resize_scale=(1,1) then the images look the same.\n#IMPORTANT: So this aug pipelines, insofar as I can tell at the moment, is tantamount to \"do nothing\"\naug_pipelines = get_barlow_twins_aug_pipelines(size=28, rotate=True,flip_p=0,resize_scale=(0.7,1), jitter=False, bw=False,blur=True,blur_p=0.5,blur_s=8, stats=None, cuda=False)\n#learn = Learner(dls, model,ShortEpochCallback(0.001), cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\nlearn = Learner(dls, model, cbs=[BarlowTwins(aug_pipelines, print_augs=True)])\n\n#dls.valid.bs = len(dls.valid_ds) #Set the validation dataloader batch size to be the length of the validation dataset\n\nb = dls.one_batch()\nlearn._split(b)\nlearn('before_batch')\naxes = learn.barlow_twins.show(n=2)","metadata":{"execution":{"iopub.status.busy":"2022-09-14T11:50:09.946607Z","iopub.execute_input":"2022-09-14T11:50:09.947151Z","iopub.status.idle":"2022-09-14T11:50:10.947170Z","shell.execute_reply.started":"2022-09-14T11:50:09.947104Z","shell.execute_reply":"2022-09-14T11:50:10.946279Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Pipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\nPipeline: RandomResizedCrop -> RandomHorizontalFlip -> RandomGaussianBlur -- {'p': 0.5, 's': 8, 'same_on_batch': False} -> Rotate -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 1.0}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x432 with 4 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":"#Simple linear classifier\nclass LinearClassifier(nn.Module):\n    \n    def __init__(self,zdim):\n        super().__init__()\n        self.fc1 = nn.Linear(zdim,10) #As 10 classes for mnist\n        \n    def forward(self,x):\n        x = cast(self.fc1(x),Tensor) #so we have to use cross entropy loss. cast is because using old version fastai \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-09-14T12:19:16.662933Z","iopub.execute_input":"2022-09-14T12:19:16.663547Z","iopub.status.idle":"2022-09-14T12:19:16.671601Z","shell.execute_reply.started":"2022-09-14T12:19:16.663491Z","shell.execute_reply":"2022-09-14T12:19:16.670530Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\n#Train Classifier on encoder(mnist) for (at the moment) one epoch\n\nfastai_encoder.eval()\n\nzdim=1024 #see above\nhead = LinearClassifier(zdim=zdim)\ndevice='cpu'\nhead.to(device)\noptimizer = torch.optim.Adam(head.parameters())\ncriterion = nn.CrossEntropyLoss()\n#EPOCHS=100\n\nfor epoch in range(10):\n    for x,y in dls.valid:\n        #break \n        #b = dls.train.one_batch() #Seems need dls[0] or dls.train for training ... dls[1] is validation see here https://docs.fast.ai/data.core.html#DataLoaders.__getitem__\n        #x,y = b[0],b[1]\n\n        loss = criterion(head(fastai_encoder(x)),y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        #print(loss)\nprint('done')\n        ","metadata":{"execution":{"iopub.status.busy":"2022-09-14T13:44:16.001952Z","iopub.execute_input":"2022-09-14T13:44:16.003538Z","iopub.status.idle":"2022-09-14T13:44:47.289075Z","shell.execute_reply.started":"2022-09-14T13:44:16.003466Z","shell.execute_reply":"2022-09-14T13:44:47.287693Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}]},{"cell_type":"code","source":"\n#Test result of above cell on the validation set - assumes that batch size of valid-dataloader is = number of valid samples                                        \n\n# print('The validation batch size is: {} '.format(dls.valid.bs))\n# input()\n\n#b = dls.valid.one_batch()\n\nfastai_encoder.eval()\n\nnum_correct=0\nfor x,y in dls_new.valid:\n    ypred = head(fastai_encoder(x))\n    correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n    num_correct += correct.sum()\nprint(num_correct/len(dls_new.valid_ds))\nprint('done')\n\n#0.9168 - 400 epochs, poly_sinusoid (need to uncomment random_quintic top part)\n#quite good results on only 400 epochs. ","metadata":{"execution":{"iopub.status.busy":"2022-09-14T13:44:47.292014Z","iopub.execute_input":"2022-09-14T13:44:47.293234Z","iopub.status.idle":"2022-09-14T13:46:44.364011Z","shell.execute_reply.started":"2022-09-14T13:44:47.293154Z","shell.execute_reply":"2022-09-14T13:46:44.362575Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"TensorCategory(0.6499)\ndone\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Note: We should at least try training linear classifier head on dls.valid (i.e. on different data to what BT was trained on - I think right?). When we do this we get:\n\n","metadata":{}},{"cell_type":"markdown","source":"Hacking since ran out of GPU\nBase HPs: ts=512,bs=256, ps=hs=500,pp=1, learn_epochs=**200**, tune_epochs=**10**.\nRun_1: BT=0.6795, MBT=0.7303","metadata":{}},{"cell_type":"markdown","source":"Base HPs as before: ts=512,bs=256, ps=hs=500,pp=1, learn_epochs=1000, tune_epochs=100.\nSee random_polynomial above for implementation\n\n**Note: Using same random_polynomial as Version 6 notebook - so we record those \n(3) runs here and continue on given that mean was \"best so far\" (aside: although really best so far is ~ 0.92-ish, averaged over 5 or 6 runs (see some earlier notebooks). So we want to see how this random function does once we have 5 or 6 runs)**  \n**Run_1: 0.9274, Run_2: 0.9309, Run_3: 0.9202, Run_4: 0.9302, Run_5: 0.9236**   \n**Mean = 0.92646, which is basically what it was after 3 runs. Cool**\n\nComment: So ~92.5 is best mean performance so far. 92.6 with current system, got ~92.3-5 on some others. \n\n","metadata":{}},{"cell_type":"code","source":"from statistics import mean\nmean([0.9274,0.9309,0.9202])#,0.9302,0.9236])","metadata":{"execution":{"iopub.status.busy":"2022-09-14T08:45:51.812073Z","iopub.execute_input":"2022-09-14T08:45:51.812577Z","iopub.status.idle":"2022-09-14T08:45:51.846053Z","shell.execute_reply.started":"2022-09-14T08:45:51.812465Z","shell.execute_reply":"2022-09-14T08:45:51.844736Z"},"trusted":true},"execution_count":1,"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"0.9261666666666667"},"metadata":{}}]},{"cell_type":"markdown","source":"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# #Just train a linear classifier (no encoder)\n# #Basically cell above but remove encoder and some re-shaping\nzdim=500 #see above\nhead = LinearClassifier(zdim=zdim)\nhead.to(device)\noptimizer = torch.optim.Adam(head.parameters())\ncriterion = nn.CrossEntropyLoss()\n\n\nfor x,y in dls.train:\n    #break\n    #b = dls.train.one_batch() #see here https://docs.fast.ai/data.core.html#DataLoaders.__getitem__\n    #x,y = b[0],b[1]\n\n    x=x.view(bs,zdim)\n    x=cast(x, Tensor) #Have to do this when using old version of fastai for some reason...\n    \n    out = head(x)\n    loss = criterion(out,y)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Test result of above cell, (i.e. just a linear classifier), on the validation set - assumes that batch size of valid-dataloader is = number of valid samples                                        \n# num_correct=0\n# for x,y in dls_new.valid:\n\n#     x=x.view(x.shape[0],zdim)\n#     x=cast(x, Tensor) #Have to do this when using old version of fastai for some reason...\n    \n#     ypred = head(x)\n#     correct = (torch.argmax(ypred,dim=1) == y).type(torch.FloatTensor)\n#     num_correct += correct.sum()\n    \n# print(num_correct/len(dls_new.valid_ds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}